{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Brute-Force Training and Evaluation Pipeline for Datasets and Models - by Selman Tabet @ https://selman.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import socket\n",
    "\n",
    "TEMP_DIR = \"tmp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostname:  Chaos\n"
     ]
    }
   ],
   "source": [
    "print(\"Hostname: \", socket.gethostname())\n",
    "try: # for CUDA enviroment\n",
    "    os.system(\"nvidia-smi\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Data processing libraries\n",
    "import numpy as np\n",
    "from itertools import combinations # For brute force combinatoric search\n",
    "import json # For saving and loading training results\n",
    "import argparse # For command line arguments\n",
    "\n",
    "# Tensorflow-Keras ML libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model # To plot model architecture\n",
    "\n",
    "from IPython import get_ipython # To check if code is running in Jupyter notebook\n",
    "import importlib.util # To import config module from str\n",
    "from pprint import pprint # To show config\n",
    "\n",
    "# Custom helper libraries\n",
    "from notebook_cfg import * # Default parameters\n",
    "from utils.img_processing import enforce_image_params\n",
    "from utils.dataset_processors import * # Dataset and generator processing functions\n",
    "from utils.plot_functions import * # Plotting functions\n",
    "from utils.evaluator import * # Complete evaluation program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: None\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES')\n",
    "print(f\"CUDA_VISIBLE_DEVICES: {cuda_visible_devices}\")\n",
    "print(tf.config.get_visible_devices())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse arguments from command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Python config file specified, using default (notebook) config.\n"
     ]
    }
   ],
   "source": [
    "# Detect if running in a Jupyter notebook\n",
    "# Generated using GPT-4o. Prompt: \"Detect if running in a Jupyter notebook\"\n",
    "def in_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        else:\n",
    "            return False  # Other type (terminal, etc.)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "    \n",
    "from_py = False\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Parse command line arguments\")\n",
    "parser.add_argument('--from-py-cfg', type=str,\n",
    "                    help='Path to the config Python file')\n",
    "if not in_notebook():\n",
    "    args = parser.parse_args()\n",
    "    config_file_path = args.from_py_cfg\n",
    "    print(f\"Python Config Path: {config_file_path}\")\n",
    "else:\n",
    "    config_file_path = False\n",
    "\n",
    "if config_file_path:\n",
    "    spec = importlib.util.spec_from_file_location(\"config_module\", config_file_path)\n",
    "    config_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(config_module)\n",
    "    config = config_module.cfg\n",
    "    print(\"Loaded config from Python file:\")\n",
    "    pprint(config)\n",
    "    # Datasets, models, and hyperparameters are mandatory and must be processed now.\n",
    "    training_datasets = config.get('datasets', {})\n",
    "    full_test_dir = config.get('test')\n",
    "    base_models = config.get('keras_models', [])\n",
    "    custom_models = config.get('custom_models', [])\n",
    "    hyperparameters = config.get('hyperparameters')\n",
    "    default_hyperparameters = default_cfg.get('hyperparameters', {})\n",
    "    if hyperparameters is None or len(hyperparameters) == 0:\n",
    "        print(\"No training hyperparameters defined in config, using defaults.\")\n",
    "        hyperparameters = default_hyperparameters\n",
    "    else:\n",
    "        for key, value in default_hyperparameters.items():\n",
    "            if key not in hyperparameters:\n",
    "                print(f\"Missing hyperparameter - falling back to default {key}:{default_hyperparameters[key]}\")\n",
    "                hyperparameters[key] = default_hyperparameters[key]\n",
    "    from_py = True # Successfully completed the import\n",
    "else:\n",
    "    print(\"No Python config file specified, using default (notebook) config.\")\n",
    "    config = default_cfg\n",
    "    training_datasets = config.get('datasets', {})\n",
    "    base_models = config.get('keras_models', [])\n",
    "    custom_models = config.get('custom_models', [])\n",
    "    hyperparameters = config.get('hyperparameters', {\"epochs\": 50, \"batch_size\": 32})\n",
    "    full_test_dir = config.get('test')\n",
    "\n",
    "if training_datasets is None or len(training_datasets) == 0:\n",
    "    raise ValueError(\"No train datasets defined in config.\")\n",
    "\n",
    "if base_models is None or len(base_models) == 0:\n",
    "    if custom_models is None or len(custom_models) == 0:\n",
    "        raise ValueError(\"No models defined in config.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dirs = [training_datasets[ds].get('train') for ds in training_datasets]\n",
    "test_dirs = [training_datasets[ds].get('test') for ds in training_datasets]\n",
    "val_dirs = [training_datasets[ds].get('val') for ds in training_datasets]\n",
    "\n",
    "all_dirs = train_dirs + test_dirs + val_dirs + [full_test_dir]\n",
    "all_dirs = [d for d in all_dirs if d is not None] # Remove None values\n",
    "\n",
    "# Combine base_models and custom_models\n",
    "all_models = base_models + custom_models\n",
    "# Create a list to keep track of which models are custom\n",
    "is_custom_model = [False] * len(base_models) + [True] * len(custom_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if from_py:\n",
    "    epochs = hyperparameters.get('epochs') # Guaranteed to be present\n",
    "    batch_size = hyperparameters.get('batch_size') # Guaranteed to be present\n",
    "    img_height = config.get('image_height', default_cfg.get('image_height'))\n",
    "    img_width = config.get('image_width', default_cfg.get('image_width'))\n",
    "    optimizer_fn = config.get('optimizer', default_cfg.get('optimizer'))\n",
    "    loss_fn = config.get('loss', default_cfg.get('loss'))\n",
    "    callbacks_list = config.get('callbacks', default_cfg.get('callbacks'))\n",
    "    metrics_list = config.get('metrics', default_cfg.get('metrics'))\n",
    "    enforce_image_size = config.get('enforce_image_settings', default_cfg.get('enforce_image_settings'))\n",
    "    val_size = config.get('val_size', default_cfg.get('val_size'))\n",
    "else:\n",
    "    epochs = hyperparameters.get('epochs', 50)\n",
    "    batch_size = hyperparameters.get('batch_size', 32)\n",
    "    img_height = default_cfg.get('image_height', 224)\n",
    "    img_width = default_cfg.get('image_width', 224)\n",
    "    optimizer_fn = default_cfg.get('optimizer', 'adam')\n",
    "    loss_fn = default_cfg.get('loss', 'binary_crossentropy')\n",
    "    callbacks_list = default_cfg.get('callbacks', [])\n",
    "    metrics_list = default_cfg.get('metrics', ['accuracy'])\n",
    "    enforce_image_size = default_cfg.get('enforce_image_settings', False)\n",
    "    val_size = default_cfg.get('val_size', 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enforce defined resolution and colour mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting image properties in datasets\\dataset_1\\train\n",
      "Adjusting image properties in datasets\\dataset_2\\Training\n",
      "Adjusting image properties in datasets\\dataset_3\n",
      "Adjusting image properties in datasets\\dataset_1\\test\n",
      "Adjusting image properties in datasets\\dataset_2\\Testing\n",
      "Adjusting image properties in datasets\\dataset_1\\val\n",
      "Adjusting image properties in datasets\\d4_test\n"
     ]
    }
   ],
   "source": [
    "if enforce_image_size:\n",
    "    for directory in all_dirs:\n",
    "        print(f\"Adjusting image properties in {directory}\")\n",
    "        enforce_image_params(directory, target_size=(img_width, img_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: The Wildfire Dataset\n",
      "Augmenting The Wildfire Dataset\n",
      "Creating generators for training\n",
      "Found 1887 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1887 images belonging to 2 classes.\n",
      "--------------------\n",
      "Number of samples in generator: 1887\n",
      "Number of classes: 2\n",
      "--------------------\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "Class names: ['fire', 'nofire']\n",
      "Dataset Class Counts:\n",
      "fire: 730\n",
      "nofire: 1157\n",
      "\n",
      "Augmented Dataset Class Counts:\n",
      "fire: 730\n",
      "nofire: 1157\n",
      "\n",
      "\n",
      "Combined Dataset Class Counts:\n",
      "fire: 1460\n",
      "nofire: 2314\n",
      "--------------------\n",
      "Creating generators for validation\n",
      "Found 402 images belonging to 2 classes.\n",
      "Found 402 images belonging to 2 classes.\n",
      "--------------------\n",
      "Number of samples in generator: 402\n",
      "Number of classes: 2\n",
      "--------------------\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "Class names: ['fire', 'nofire']\n",
      "Dataset Class Counts:\n",
      "fire: 156\n",
      "nofire: 246\n",
      "\n",
      "Augmented Dataset Class Counts:\n",
      "fire: 156\n",
      "nofire: 246\n",
      "\n",
      "\n",
      "Combined Dataset Class Counts:\n",
      "fire: 312\n",
      "nofire: 492\n",
      "--------------------\n",
      "Processing: DeepFire\n",
      "Augmenting DeepFire\n",
      "Creating generators for training\n",
      "Found 1520 images belonging to 2 classes.\n",
      "Found 1520 images belonging to 2 classes.\n",
      "--------------------\n",
      "Number of samples in generator: 1520\n",
      "Number of classes: 2\n",
      "--------------------\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "Class names: ['fire', 'nofire']\n",
      "Dataset Class Counts:\n",
      "fire: 760\n",
      "nofire: 760\n",
      "\n",
      "Augmented Dataset Class Counts:\n",
      "fire: 760\n",
      "nofire: 760\n",
      "\n",
      "\n",
      "Combined Dataset Class Counts:\n",
      "fire: 1520\n",
      "nofire: 1520\n",
      "--------------------\n",
      "No validation set, splitting training set.\n",
      "-------------------------\n",
      "--- Splitted dataset ---\n",
      "Training dataset size: 2432 samples\n",
      "Validation dataset size: 608 samples\n",
      "-------------------------\n",
      "Processing: FIRE\n",
      "Augmenting FIRE\n",
      "Creating generators for training\n",
      "Found 999 images belonging to 2 classes.\n",
      "Found 999 images belonging to 2 classes.\n",
      "--------------------\n",
      "Number of samples in generator: 999\n",
      "Number of classes: 2\n",
      "--------------------\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "Class names: ['fire', 'nofire']\n",
      "Dataset Class Counts:\n",
      "fire: 755\n",
      "nofire: 244\n",
      "\n",
      "Augmented Dataset Class Counts:\n",
      "fire: 755\n",
      "nofire: 244\n",
      "\n",
      "\n",
      "Combined Dataset Class Counts:\n",
      "fire: 1510\n",
      "nofire: 488\n",
      "--------------------\n",
      "No validation set, splitting training set.\n",
      "-------------------------\n",
      "--- Splitted dataset ---\n",
      "Training dataset size: 1599 samples\n",
      "Validation dataset size: 399 samples\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_names = []\n",
    "train_datasets = [] # [ (dataset_1_train, dataset_2_train), ... ]\n",
    "train_sizes = [] # [ (dataset_1_train_size, dataset_2_train_size), ... ]\n",
    "val_datasets = [] # [ (dataset_1_val, dataset_2_val), ... ]\n",
    "val_sizes = [] # [ (dataset_1_val_size, dataset_2_val_size), ... ]\n",
    "train_counts = [] # [ (dataset_1_train_counts, dataset_2_train_counts), ... ]\n",
    "val_counts = [] # [ (dataset_1_val_counts, dataset_2_val_counts), ... ]\n",
    "\n",
    "for d in training_datasets:\n",
    "    print(f\"Processing: {d}\")\n",
    "    train_dir = training_datasets[d].get('train')\n",
    "    augment = training_datasets[d].get('augment', True)\n",
    "    print(\"Augmenting\" if augment else \"Not augmenting\", d)\n",
    "    # Apply original and augmented data generators for training\n",
    "    print(\"Creating generators for training\")\n",
    "    if \"val\" in training_datasets[d]:\n",
    "        train_generator, augmented_train_generator = create_generators(train_dir, batch_size=batch_size, augment=augment, img_width=img_width, img_height=img_height)\n",
    "        val_generator, augmented_val_generator = create_generators(training_datasets[d]['val'], batch_size=batch_size, augment=augment, shuffle=False, img_width=img_width, img_height=img_height)\n",
    "    else:\n",
    "        train_generator, augmented_train_generator, val_generator, augmented_val_generator = create_split_generators(train_dir, val_size=val_size, batch_size=batch_size, augment=augment, img_width=img_width, img_height=img_height)\n",
    "\n",
    "    train_samples = samples_from_generators([train_generator, augmented_train_generator])\n",
    "    train_count_dict = class_counts_from_generators(train_generator, augmented_train_generator)\n",
    "    train_dataset = generators_to_dataset([train_generator, augmented_train_generator], batch_size=batch_size, img_height=img_height, img_width=img_width)\n",
    "    \n",
    "    val_samples = samples_from_generators([val_generator, augmented_val_generator])\n",
    "    val_count_dict = class_counts_from_generators(val_generator, augmented_val_generator)\n",
    "    val_dataset = generators_to_dataset([val_generator, augmented_val_generator], batch_size=batch_size, img_height=img_height, img_width=img_width)\n",
    "    \n",
    "    # Calculate the number of samples for training and validation\n",
    "    train_sizes.append(train_samples)\n",
    "    val_sizes.append(val_samples)\n",
    "    \n",
    "    train_counts.append(train_count_dict)\n",
    "    val_counts.append(val_count_dict)\n",
    "    train_datasets.append(train_dataset)\n",
    "    val_datasets.append(val_dataset)\n",
    "    dataset_names.append(d)\n",
    "    \n",
    "# Ensure that the lengths are consistent across the board before continuing\n",
    "assert len(train_sizes) == len(train_datasets) == len(val_sizes) == len(val_datasets) == len(val_counts) == len(train_counts) == len(dataset_names), \"Dataset lengths are inconsistent.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute Force Combinatorial Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_combos = [] # [(0,), (1,), (0, 1), ...] where 0, 1 are the indices of the datasets within their respective lists\n",
    "for r in range(1, len(dataset_names) + 1):\n",
    "    dataset_combos.extend(combinations(range(len(dataset_names)), r))\n",
    "    \n",
    "combined_training_datasets = []\n",
    "combined_val_datasets = []\n",
    "combined_dataset_names = []\n",
    "steps_per_epoch_list = []\n",
    "validation_steps_list = []\n",
    "train_counts_list = []\n",
    "val_counts_list = []\n",
    "\n",
    "for combo in dataset_combos:\n",
    "    training_dataset = None\n",
    "    val_dataset = None\n",
    "    train_size = None\n",
    "    val_size = None\n",
    "    train_count = None\n",
    "    val_count = None\n",
    "    for idx in combo:\n",
    "        if training_dataset is None:\n",
    "            training_dataset = train_datasets[idx]\n",
    "            val_dataset = val_datasets[idx]\n",
    "            train_size = train_sizes[idx]\n",
    "            val_size = val_sizes[idx]\n",
    "            train_count = train_counts[idx]\n",
    "            val_count = val_counts[idx]\n",
    "        else:\n",
    "            training_dataset = training_dataset.concatenate(train_datasets[idx])\n",
    "            val_dataset = val_dataset.concatenate(val_datasets[idx])\n",
    "            train_size += train_sizes[idx]\n",
    "            val_size += val_sizes[idx]\n",
    "            train_count = {k: train_count.get(k, 0) + train_counts[idx].get(k, 0) for k in set(train_count) | set(train_counts[idx])}\n",
    "            val_count = {k: val_count.get(k, 0) + val_counts[idx].get(k, 0) for k in set(val_count) | set(val_counts[idx])}\n",
    "        train_count = {k: int(v) for k, v in train_count.items()}\n",
    "        val_count = {k: int(v) for k, v in val_count.items()}\n",
    "\n",
    "    combined_dataset_names.append(\"_\".join([dataset_names[idx] for idx in combo]))\n",
    "    combined_training_datasets.append(training_dataset)\n",
    "    combined_val_datasets.append(val_dataset)\n",
    "    steps_per_epoch_list.append(train_size // batch_size)\n",
    "    validation_steps_list.append(val_size // batch_size)\n",
    "    train_counts_list.append(train_count)\n",
    "    val_counts_list.append(val_count)\n",
    "\n",
    "    training_params = zip(combined_dataset_names, combined_training_datasets, combined_val_datasets, steps_per_epoch_list, validation_steps_list, train_counts_list, val_counts_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes.\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "\n",
      "\n",
      "Test Dataset Class Counts:\n",
      "fire: 100\n",
      "nofire: 100\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if full_test_dir is None:\n",
    "    test_generators = []\n",
    "    print(\"No target test directory provided, merging all tests from provided datasets if available.\")\n",
    "    for d in test_dirs:\n",
    "        if d is not None:\n",
    "            test_generators.append(create_generators(d, batch_size=batch_size, augment=False, shuffle=False, img_height=img_height, img_width=img_width)[0]) # No augmentation/shuffle for testing\n",
    "    if len(test_generators) == 0:\n",
    "        raise ValueError(\"No tests found in the provided datasets.\")\n",
    "    true_labels = np.concatenate([gen.classes for gen in test_generators])\n",
    "    test_dataset = generators_to_dataset(test_generators, batch_size=batch_size)\n",
    "    test_steps = sum([gen.samples for gen in test_generators]) // batch_size\n",
    "    print(\"Test Dataset Class Counts:\")\n",
    "    for gen in test_generators:\n",
    "        print(\"Class indices:\", gen.class_indices)\n",
    "        for class_name, class_index in gen.class_indices.items():\n",
    "            print(f\"{class_name}: {sum(gen.classes == class_index)}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "else:\n",
    "    test_generator, augmented_test_generator = create_generators(full_test_dir, batch_size=batch_size, augment=False, shuffle=False, img_height=img_height, img_width=img_width) # No augmentation/shuffle for testing\n",
    "    test_dataset = create_dataset(test_generator, batch_size=batch_size, img_height=img_height, img_width=img_width)\n",
    "    test_steps = test_generator.samples // batch_size\n",
    "    true_labels = test_generator.classes\n",
    "    print(\"Class indices:\", test_generator.class_indices)\n",
    "    print(\"\\n\")\n",
    "    print(\"Test Dataset Class Counts:\")\n",
    "    for class_name, class_index in test_generator.class_indices.items():\n",
    "        print(f\"{class_name}: {sum(test_generator.classes == class_index)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "true_labels = true_labels[: (len(true_labels) // batch_size) * batch_size] # Ensure that the true labels are divisible by the batch size to avoid size mismatch with predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_model(bm, custom=False, to_dir=TEMP_DIR):\n",
    "    if custom:\n",
    "        model = bm\n",
    "        model.compile(optimizer=optimizer_fn, loss=loss_fn, metrics=metrics_list)\n",
    "        os.makedirs(os.path.join(to_dir, model.name), exist_ok=True)\n",
    "        model.save_weights(os.path.join(to_dir, model.name, f\"{model.name}_initial.weights.h5\"))\n",
    "        return model\n",
    "    \n",
    "    base_model = bm(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(img_height, img_width, 3)\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create the model\n",
    "    inputs = Input(shape=(img_height, img_width, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name=bm.__name__)\n",
    "    model.compile(optimizer=optimizer_fn, loss=loss_fn, metrics=metrics_list)\n",
    "    os.makedirs(os.path.join(to_dir, model.name), exist_ok=True)\n",
    "    model.save_weights(os.path.join(to_dir, model.name, f\"{model.name}_initial.weights.h5\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the models and combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "run_number = len([d for d in os.listdir(\"runs\") if os.path.isdir(os.path.join(\"runs\", d)) and d.startswith('run_')]) + 1\n",
    "run_dir = os.path.join(\"runs\", f\"run_{run_number}\")\n",
    "os.makedirs(run_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    \"datasets\": training_datasets,\n",
    "    \"val_size\": val_size,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"test_dirs\": test_dirs,\n",
    "    \"full_test\": full_test_dir,\n",
    "}\n",
    "\n",
    "with open(os.path.join(run_dir, \"run_config.json\"), \"w\") as f:\n",
    "    json.dump(run_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MobileNetV3Small\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"MobileNetV3Small\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ MobileNetV3Small (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ MobileNetV3Small (\u001b[38;5;33mFunctional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      │       \u001b[38;5;34m939,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │         \u001b[38;5;34m2,304\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m147,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,090,417</span> (4.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,090,417\u001b[0m (4.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">149,633</span> (584.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m149,633\u001b[0m (584.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">940,784</span> (3.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m940,784\u001b[0m (3.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: MobileNetV3Small on dataset: The Wildfire Dataset\n",
      "Epoch 1/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 81ms/step - accuracy: 0.6399 - auc: 0.6976 - f1_score: 0.6713 - loss: 0.7389 - precision: 0.7480 - recall: 0.6179 - val_accuracy: 0.6150 - val_auc: 0.7919 - val_f1_score: 0.7580 - val_loss: 0.6736 - val_precision: 0.6150 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 71ms/step - accuracy: 0.7284 - auc: 0.7932 - f1_score: 0.7797 - loss: 0.5627 - precision: 0.7773 - recall: 0.7877 - val_accuracy: 0.6075 - val_auc: 0.8265 - val_f1_score: 0.7521 - val_loss: 0.6399 - val_precision: 0.6075 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 74ms/step - accuracy: 0.7251 - auc: 0.7958 - f1_score: 0.7788 - loss: 0.5596 - precision: 0.7807 - recall: 0.7813 - val_accuracy: 0.6413 - val_auc: 0.8648 - val_f1_score: 0.7743 - val_loss: 0.5824 - val_precision: 0.6381 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 71ms/step - accuracy: 0.7210 - auc: 0.7924 - f1_score: 0.7731 - loss: 0.5590 - precision: 0.7561 - recall: 0.7944 - val_accuracy: 0.7175 - val_auc: 0.8752 - val_f1_score: 0.8000 - val_loss: 0.5477 - val_precision: 0.6904 - val_recall: 0.9648 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 81ms/step - accuracy: 0.7501 - auc: 0.8201 - f1_score: 0.8000 - loss: 0.5067 - precision: 0.7889 - recall: 0.8136 - val_accuracy: 0.8163 - val_auc: 0.8788 - val_f1_score: 0.8495 - val_loss: 0.4782 - val_precision: 0.8532 - val_recall: 0.8549 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 155ms/step - accuracy: 0.7542 - auc: 0.8259 - f1_score: 0.7987 - loss: 0.5020 - precision: 0.7931 - recall: 0.8102 - val_accuracy: 0.8275 - val_auc: 0.9014 - val_f1_score: 0.8542 - val_loss: 0.4279 - val_precision: 0.8810 - val_recall: 0.8390 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 141ms/step - accuracy: 0.7543 - auc: 0.8214 - f1_score: 0.8010 - loss: 0.5067 - precision: 0.7867 - recall: 0.8190 - val_accuracy: 0.7862 - val_auc: 0.8707 - val_f1_score: 0.8115 - val_loss: 0.4593 - val_precision: 0.8390 - val_recall: 0.7872 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 149ms/step - accuracy: 0.7746 - auc: 0.8485 - f1_score: 0.8164 - loss: 0.4669 - precision: 0.8122 - recall: 0.8268 - val_accuracy: 0.8087 - val_auc: 0.8834 - val_f1_score: 0.8393 - val_loss: 0.4232 - val_precision: 0.8446 - val_recall: 0.8429 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 152ms/step - accuracy: 0.7548 - auc: 0.8256 - f1_score: 0.8038 - loss: 0.4985 - precision: 0.7910 - recall: 0.8212 - val_accuracy: 0.8413 - val_auc: 0.9103 - val_f1_score: 0.8686 - val_loss: 0.3861 - val_precision: 0.8677 - val_recall: 0.8765 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 170ms/step - accuracy: 0.7699 - auc: 0.8474 - f1_score: 0.8099 - loss: 0.4748 - precision: 0.7877 - recall: 0.8402 - val_accuracy: 0.8112 - val_auc: 0.8970 - val_f1_score: 0.8394 - val_loss: 0.4128 - val_precision: 0.8503 - val_recall: 0.8381 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 132ms/step - accuracy: 0.7745 - auc: 0.8432 - f1_score: 0.8176 - loss: 0.4751 - precision: 0.8006 - recall: 0.8403 - val_accuracy: 0.8375 - val_auc: 0.9135 - val_f1_score: 0.8602 - val_loss: 0.3893 - val_precision: 0.8782 - val_recall: 0.8492 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 146ms/step - accuracy: 0.7645 - auc: 0.8507 - f1_score: 0.8097 - loss: 0.4608 - precision: 0.8097 - recall: 0.8126 - val_accuracy: 0.8313 - val_auc: 0.9004 - val_f1_score: 0.8604 - val_loss: 0.4018 - val_precision: 0.8442 - val_recall: 0.8905 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 152ms/step - accuracy: 0.7861 - auc: 0.8540 - f1_score: 0.8263 - loss: 0.4608 - precision: 0.8157 - recall: 0.8419 - val_accuracy: 0.8175 - val_auc: 0.8967 - val_f1_score: 0.8504 - val_loss: 0.4030 - val_precision: 0.8540 - val_recall: 0.8505 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.7748 - auc: 0.8564 - f1_score: 0.8154 - loss: 0.4575 - precision: 0.8056 - recall: 0.8302\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 152ms/step - accuracy: 0.7748 - auc: 0.8564 - f1_score: 0.8155 - loss: 0.4575 - precision: 0.8056 - recall: 0.8303 - val_accuracy: 0.8138 - val_auc: 0.8932 - val_f1_score: 0.8438 - val_loss: 0.4064 - val_precision: 0.8648 - val_recall: 0.8356 - learning_rate: 0.0010\n",
      "Training time: 205.92 seconds\n",
      "Evaluating MobileNetV3Small on The Wildfire Dataset...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - accuracy: 0.6786 - auc: 0.5717 - f1_score: 0.3252 - loss: 0.7074 - precision: 0.4645 - recall: 0.5753        \n",
      "Training results:\n",
      "{'evaluation': {'accuracy': 0.6875,\n",
      "                'auc': 0.7554348111152649,\n",
      "                'f1_score': 0.4995758533477783,\n",
      "                'loss': 0.6712819933891296,\n",
      "                'precision': 0.6754385828971863,\n",
      "                'recall': 0.7699999809265137},\n",
      " 'history': {'accuracy': [0.6738781929016113,\n",
      "                          0.7342414259910583,\n",
      "                          0.7318376302719116,\n",
      "                          0.7318376302719116,\n",
      "                          0.754807710647583,\n",
      "                          0.75,\n",
      "                          0.7641559839248657,\n",
      "                          0.7748397588729858,\n",
      "                          0.7676281929016113,\n",
      "                          0.7676281929016113,\n",
      "                          0.773771345615387,\n",
      "                          0.7700320482254028,\n",
      "                          0.7881944179534912,\n",
      "                          0.7820512652397156],\n",
      "             'auc': [0.7309072613716125,\n",
      "                     0.7983800172805786,\n",
      "                     0.8013395667076111,\n",
      "                     0.7974125146865845,\n",
      "                     0.8230277299880981,\n",
      "                     0.823114812374115,\n",
      "                     0.8327071666717529,\n",
      "                     0.8492932319641113,\n",
      "                     0.8374620676040649,\n",
      "                     0.8409428000450134,\n",
      "                     0.8422045707702637,\n",
      "                     0.8512322306632996,\n",
      "                     0.8575919270515442,\n",
      "                     0.8573895692825317],\n",
      "             'f1_score': [0.717695951461792,\n",
      "                          0.7833606600761414,\n",
      "                          0.7828742861747742,\n",
      "                          0.7841277122497559,\n",
      "                          0.8019723892211914,\n",
      "                          0.7952434420585632,\n",
      "                          0.8084138631820679,\n",
      "                          0.8188406825065613,\n",
      "                          0.812821626663208,\n",
      "                          0.8098578453063965,\n",
      "                          0.8167234659194946,\n",
      "                          0.8139601349830627,\n",
      "                          0.827944278717041,\n",
      "                          0.8214987516403198],\n",
      "             'learning_rate': [0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513],\n",
      "             'loss': [0.6854590177536011,\n",
      "                      0.5582860708236694,\n",
      "                      0.5471735000610352,\n",
      "                      0.5467731952667236,\n",
      "                      0.5094841122627258,\n",
      "                      0.5064075589179993,\n",
      "                      0.48966801166534424,\n",
      "                      0.46540600061416626,\n",
      "                      0.4838930368423462,\n",
      "                      0.4783496558666229,\n",
      "                      0.47652578353881836,\n",
      "                      0.46286284923553467,\n",
      "                      0.4562964141368866,\n",
      "                      0.4570200741291046],\n",
      "             'precision': [0.7554814219474792,\n",
      "                           0.7730526328086853,\n",
      "                           0.7776360511779785,\n",
      "                           0.7683639526367188,\n",
      "                           0.7917726635932922,\n",
      "                           0.7865359783172607,\n",
      "                           0.7985672354698181,\n",
      "                           0.8120522499084473,\n",
      "                           0.7993311285972595,\n",
      "                           0.7983939051628113,\n",
      "                           0.8040370345115662,\n",
      "                           0.8066017627716064,\n",
      "                           0.8188375234603882,\n",
      "                           0.8078880310058594],\n",
      "             'recall': [0.6912341713905334,\n",
      "                        0.801047146320343,\n",
      "                        0.7917748689651489,\n",
      "                        0.8039301037788391,\n",
      "                        0.8138622641563416,\n",
      "                        0.8092941641807556,\n",
      "                        0.823913037776947,\n",
      "                        0.8291738629341125,\n",
      "                        0.8305820822715759,\n",
      "                        0.8277826309204102,\n",
      "                        0.8338420987129211,\n",
      "                        0.8251082301139832,\n",
      "                        0.8405923247337341,\n",
      "                        0.8399471044540405],\n",
      "             'val_accuracy': [0.6150000095367432,\n",
      "                              0.6075000166893005,\n",
      "                              0.6412500143051147,\n",
      "                              0.7174999713897705,\n",
      "                              0.8162500262260437,\n",
      "                              0.8274999856948853,\n",
      "                              0.7862499952316284,\n",
      "                              0.8087499737739563,\n",
      "                              0.8412500023841858,\n",
      "                              0.8112499713897705,\n",
      "                              0.8374999761581421,\n",
      "                              0.831250011920929,\n",
      "                              0.8174999952316284,\n",
      "                              0.8137500286102295],\n",
      "             'val_auc': [0.7918745279312134,\n",
      "                         0.8265346884727478,\n",
      "                         0.8647791147232056,\n",
      "                         0.8752375841140747,\n",
      "                         0.8788280487060547,\n",
      "                         0.9013661742210388,\n",
      "                         0.8706576228141785,\n",
      "                         0.8834002614021301,\n",
      "                         0.9103325009346008,\n",
      "                         0.8969761729240417,\n",
      "                         0.9134846925735474,\n",
      "                         0.9003838896751404,\n",
      "                         0.8966749906539917,\n",
      "                         0.8932170271873474],\n",
      "             'val_f1_score': [0.7579533457756042,\n",
      "                              0.7521413564682007,\n",
      "                              0.7743192315101624,\n",
      "                              0.7999722957611084,\n",
      "                              0.8495096564292908,\n",
      "                              0.8542439341545105,\n",
      "                              0.8114737868309021,\n",
      "                              0.839344322681427,\n",
      "                              0.8686063289642334,\n",
      "                              0.8394243121147156,\n",
      "                              0.8601836562156677,\n",
      "                              0.8603651523590088,\n",
      "                              0.8504273295402527,\n",
      "                              0.843808650970459],\n",
      "             'val_loss': [0.6735684275627136,\n",
      "                          0.6398539543151855,\n",
      "                          0.5824118256568909,\n",
      "                          0.5476614236831665,\n",
      "                          0.4782094657421112,\n",
      "                          0.4278889000415802,\n",
      "                          0.45925089716911316,\n",
      "                          0.42323169112205505,\n",
      "                          0.3860611021518707,\n",
      "                          0.41281741857528687,\n",
      "                          0.3892830014228821,\n",
      "                          0.4017592966556549,\n",
      "                          0.4030079245567322,\n",
      "                          0.40635886788368225],\n",
      "             'val_precision': [0.6150000095367432,\n",
      "                               0.6075000166893005,\n",
      "                               0.6380832195281982,\n",
      "                               0.6903703808784485,\n",
      "                               0.85317462682724,\n",
      "                               0.8810020685195923,\n",
      "                               0.839002251625061,\n",
      "                               0.8445807695388794,\n",
      "                               0.8677354454994202,\n",
      "                               0.8503118753433228,\n",
      "                               0.8782051205635071,\n",
      "                               0.8442307710647583,\n",
      "                               0.8539553880691528,\n",
      "                               0.8647540807723999],\n",
      "             'val_recall': [1.0,\n",
      "                            1.0,\n",
      "                            1.0,\n",
      "                            0.9648033380508423,\n",
      "                            0.8548707962036133,\n",
      "                            0.8389661908149719,\n",
      "                            0.7872340679168701,\n",
      "                            0.8428571224212646,\n",
      "                            0.876518189907074,\n",
      "                            0.8381147384643555,\n",
      "                            0.8491735458374023,\n",
      "                            0.8904665112495422,\n",
      "                            0.8505050539970398,\n",
      "                            0.8356435894966125]},\n",
      " 'optimal_threshold': 0.5847510695457458,\n",
      " 'train_counts': {'fire': 730, 'nofire': 1157},\n",
      " 'train_counts_total': 1887,\n",
      " 'train_dataset_size': 3744,\n",
      " 'training_time': 205.9229760169983,\n",
      " 'val_counts': {'fire': 156, 'nofire': 246},\n",
      " 'val_counts_total': 402,\n",
      " 'val_dataset_size': 800}\n",
      "Training model: MobileNetV3Small on dataset: DeepFire\n",
      "Epoch 1/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 189ms/step - accuracy: 0.6938 - auc: 0.7744 - f1_score: 0.6756 - loss: 0.6575 - precision: 0.7010 - recall: 0.6966 - val_accuracy: 0.5280 - val_auc: 0.8726 - val_f1_score: 0.6850 - val_loss: 0.7109 - val_precision: 0.5280 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 179ms/step - accuracy: 0.8046 - auc: 0.8905 - f1_score: 0.7957 - loss: 0.4554 - precision: 0.7926 - recall: 0.8065 - val_accuracy: 0.4951 - val_auc: 0.9241 - val_f1_score: 0.6591 - val_loss: 0.7099 - val_precision: 0.4951 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 175ms/step - accuracy: 0.8355 - auc: 0.9148 - f1_score: 0.8318 - loss: 0.3810 - precision: 0.8365 - recall: 0.8316 - val_accuracy: 0.4918 - val_auc: 0.9340 - val_f1_score: 0.6543 - val_loss: 0.7107 - val_precision: 0.4909 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 158ms/step - accuracy: 0.8490 - auc: 0.9257 - f1_score: 0.8433 - loss: 0.3558 - precision: 0.8396 - recall: 0.8504 - val_accuracy: 0.5641 - val_auc: 0.9389 - val_f1_score: 0.7009 - val_loss: 0.6123 - val_precision: 0.5415 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 149ms/step - accuracy: 0.8394 - auc: 0.9232 - f1_score: 0.8372 - loss: 0.3628 - precision: 0.8539 - recall: 0.8259 - val_accuracy: 0.6842 - val_auc: 0.9538 - val_f1_score: 0.7424 - val_loss: 0.5523 - val_precision: 0.6021 - val_recall: 0.9896 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 153ms/step - accuracy: 0.8624 - auc: 0.9254 - f1_score: 0.8604 - loss: 0.3490 - precision: 0.8562 - recall: 0.8664 - val_accuracy: 0.8536 - val_auc: 0.9732 - val_f1_score: 0.8666 - val_loss: 0.4437 - val_precision: 0.7743 - val_recall: 0.9899 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 165ms/step - accuracy: 0.8596 - auc: 0.9382 - f1_score: 0.8636 - loss: 0.3231 - precision: 0.8728 - recall: 0.8571 - val_accuracy: 0.8964 - val_auc: 0.9719 - val_f1_score: 0.8995 - val_loss: 0.3776 - val_precision: 0.8418 - val_recall: 0.9658 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 160ms/step - accuracy: 0.8704 - auc: 0.9378 - f1_score: 0.8702 - loss: 0.3299 - precision: 0.8602 - recall: 0.8852 - val_accuracy: 0.9046 - val_auc: 0.9627 - val_f1_score: 0.9042 - val_loss: 0.2965 - val_precision: 0.8997 - val_recall: 0.9115 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 159ms/step - accuracy: 0.8583 - auc: 0.9287 - f1_score: 0.8527 - loss: 0.3468 - precision: 0.8453 - recall: 0.8651 - val_accuracy: 0.9128 - val_auc: 0.9711 - val_f1_score: 0.9089 - val_loss: 0.2637 - val_precision: 0.9130 - val_recall: 0.9100 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 157ms/step - accuracy: 0.8614 - auc: 0.9359 - f1_score: 0.8613 - loss: 0.3271 - precision: 0.8654 - recall: 0.8594 - val_accuracy: 0.9095 - val_auc: 0.9728 - val_f1_score: 0.9103 - val_loss: 0.2554 - val_precision: 0.9148 - val_recall: 0.9119 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 151ms/step - accuracy: 0.8557 - auc: 0.9397 - f1_score: 0.8502 - loss: 0.3134 - precision: 0.8418 - recall: 0.8642 - val_accuracy: 0.9293 - val_auc: 0.9755 - val_f1_score: 0.9250 - val_loss: 0.2200 - val_precision: 0.9422 - val_recall: 0.9142 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 140ms/step - accuracy: 0.8493 - auc: 0.9311 - f1_score: 0.8480 - loss: 0.3367 - precision: 0.8529 - recall: 0.8462 - val_accuracy: 0.9128 - val_auc: 0.9742 - val_f1_score: 0.9132 - val_loss: 0.2203 - val_precision: 0.9042 - val_recall: 0.9248 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 152ms/step - accuracy: 0.8726 - auc: 0.9398 - f1_score: 0.8684 - loss: 0.3161 - precision: 0.8550 - recall: 0.8904 - val_accuracy: 0.9145 - val_auc: 0.9765 - val_f1_score: 0.9104 - val_loss: 0.2144 - val_precision: 0.9296 - val_recall: 0.8919 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 154ms/step - accuracy: 0.8814 - auc: 0.9495 - f1_score: 0.8807 - loss: 0.2883 - precision: 0.8694 - recall: 0.8986 - val_accuracy: 0.9309 - val_auc: 0.9784 - val_f1_score: 0.9272 - val_loss: 0.1976 - val_precision: 0.9203 - val_recall: 0.9390 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 137ms/step - accuracy: 0.8678 - auc: 0.9415 - f1_score: 0.8657 - loss: 0.3106 - precision: 0.8537 - recall: 0.8806 - val_accuracy: 0.9128 - val_auc: 0.9805 - val_f1_score: 0.9048 - val_loss: 0.2003 - val_precision: 0.8942 - val_recall: 0.9225 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 145ms/step - accuracy: 0.8728 - auc: 0.9464 - f1_score: 0.8752 - loss: 0.2954 - precision: 0.8710 - recall: 0.8834 - val_accuracy: 0.9178 - val_auc: 0.9721 - val_f1_score: 0.9203 - val_loss: 0.2255 - val_precision: 0.8994 - val_recall: 0.9408 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 136ms/step - accuracy: 0.8795 - auc: 0.9459 - f1_score: 0.8763 - loss: 0.2982 - precision: 0.8583 - recall: 0.9038 - val_accuracy: 0.9194 - val_auc: 0.9812 - val_f1_score: 0.9164 - val_loss: 0.2020 - val_precision: 0.9257 - val_recall: 0.9103 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 148ms/step - accuracy: 0.8586 - auc: 0.9347 - f1_score: 0.8580 - loss: 0.3309 - precision: 0.8468 - recall: 0.8740 - val_accuracy: 0.9359 - val_auc: 0.9845 - val_f1_score: 0.9266 - val_loss: 0.1924 - val_precision: 0.9329 - val_recall: 0.9296 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 151ms/step - accuracy: 0.8815 - auc: 0.9510 - f1_score: 0.8808 - loss: 0.2841 - precision: 0.8793 - recall: 0.8861 - val_accuracy: 0.9326 - val_auc: 0.9851 - val_f1_score: 0.9334 - val_loss: 0.1865 - val_precision: 0.9175 - val_recall: 0.9507 - learning_rate: 0.0010\n",
      "Epoch 20/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 136ms/step - accuracy: 0.8795 - auc: 0.9473 - f1_score: 0.8797 - loss: 0.2944 - precision: 0.8843 - recall: 0.8775 - val_accuracy: 0.9309 - val_auc: 0.9817 - val_f1_score: 0.9289 - val_loss: 0.1939 - val_precision: 0.9430 - val_recall: 0.9183 - learning_rate: 0.0010\n",
      "Epoch 21/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 137ms/step - accuracy: 0.8787 - auc: 0.9466 - f1_score: 0.8778 - loss: 0.2956 - precision: 0.8805 - recall: 0.8791 - val_accuracy: 0.9145 - val_auc: 0.9739 - val_f1_score: 0.9120 - val_loss: 0.2149 - val_precision: 0.9172 - val_recall: 0.9112 - learning_rate: 0.0010\n",
      "Epoch 22/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 158ms/step - accuracy: 0.8815 - auc: 0.9524 - f1_score: 0.8817 - loss: 0.2811 - precision: 0.8818 - recall: 0.8826 - val_accuracy: 0.9276 - val_auc: 0.9858 - val_f1_score: 0.9238 - val_loss: 0.1762 - val_precision: 0.9441 - val_recall: 0.9060 - learning_rate: 0.0010\n",
      "Epoch 23/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 266ms/step - accuracy: 0.8698 - auc: 0.9443 - f1_score: 0.8703 - loss: 0.3053 - precision: 0.8731 - recall: 0.8768 - val_accuracy: 0.9326 - val_auc: 0.9833 - val_f1_score: 0.9304 - val_loss: 0.1914 - val_precision: 0.9320 - val_recall: 0.9351 - learning_rate: 0.0010\n",
      "Epoch 24/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 276ms/step - accuracy: 0.8875 - auc: 0.9503 - f1_score: 0.8835 - loss: 0.2867 - precision: 0.8750 - recall: 0.8973 - val_accuracy: 0.9276 - val_auc: 0.9808 - val_f1_score: 0.9247 - val_loss: 0.1924 - val_precision: 0.9231 - val_recall: 0.9293 - learning_rate: 0.0010\n",
      "Epoch 25/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 141ms/step - accuracy: 0.8678 - auc: 0.9416 - f1_score: 0.8658 - loss: 0.3083 - precision: 0.8710 - recall: 0.8654 - val_accuracy: 0.9391 - val_auc: 0.9853 - val_f1_score: 0.9367 - val_loss: 0.1817 - val_precision: 0.9340 - val_recall: 0.9433 - learning_rate: 0.0010\n",
      "Epoch 26/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 152ms/step - accuracy: 0.8832 - auc: 0.9508 - f1_score: 0.8820 - loss: 0.2859 - precision: 0.8804 - recall: 0.8870 - val_accuracy: 0.9408 - val_auc: 0.9882 - val_f1_score: 0.9317 - val_loss: 0.1622 - val_precision: 0.9422 - val_recall: 0.9288 - learning_rate: 0.0010\n",
      "Epoch 27/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 143ms/step - accuracy: 0.8853 - auc: 0.9589 - f1_score: 0.8835 - loss: 0.2628 - precision: 0.8909 - recall: 0.8803 - val_accuracy: 0.9342 - val_auc: 0.9838 - val_f1_score: 0.9320 - val_loss: 0.1818 - val_precision: 0.9253 - val_recall: 0.9437 - learning_rate: 0.0010\n",
      "Epoch 28/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 158ms/step - accuracy: 0.8823 - auc: 0.9583 - f1_score: 0.8812 - loss: 0.2637 - precision: 0.8650 - recall: 0.9025 - val_accuracy: 0.9391 - val_auc: 0.9849 - val_f1_score: 0.9360 - val_loss: 0.1737 - val_precision: 0.9251 - val_recall: 0.9530 - learning_rate: 0.0010\n",
      "Epoch 29/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 149ms/step - accuracy: 0.9035 - auc: 0.9621 - f1_score: 0.9018 - loss: 0.2482 - precision: 0.9059 - recall: 0.9023 - val_accuracy: 0.9309 - val_auc: 0.9860 - val_f1_score: 0.9246 - val_loss: 0.1726 - val_precision: 0.9454 - val_recall: 0.9142 - learning_rate: 0.0010\n",
      "Epoch 30/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 159ms/step - accuracy: 0.8876 - auc: 0.9553 - f1_score: 0.8852 - loss: 0.2702 - precision: 0.8799 - recall: 0.8979 - val_accuracy: 0.9507 - val_auc: 0.9905 - val_f1_score: 0.9511 - val_loss: 0.1576 - val_precision: 0.9536 - val_recall: 0.9474 - learning_rate: 0.0010\n",
      "Epoch 31/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 147ms/step - accuracy: 0.8778 - auc: 0.9490 - f1_score: 0.8778 - loss: 0.2910 - precision: 0.8608 - recall: 0.8990 - val_accuracy: 0.9342 - val_auc: 0.9859 - val_f1_score: 0.9302 - val_loss: 0.1692 - val_precision: 0.9259 - val_recall: 0.9386 - learning_rate: 0.0010\n",
      "Epoch 32/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 149ms/step - accuracy: 0.8843 - auc: 0.9562 - f1_score: 0.8885 - loss: 0.2752 - precision: 0.8984 - recall: 0.8829 - val_accuracy: 0.9276 - val_auc: 0.9824 - val_f1_score: 0.9262 - val_loss: 0.1823 - val_precision: 0.9262 - val_recall: 0.9262 - learning_rate: 0.0010\n",
      "Epoch 33/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 140ms/step - accuracy: 0.8925 - auc: 0.9597 - f1_score: 0.8905 - loss: 0.2579 - precision: 0.8780 - recall: 0.9092 - val_accuracy: 0.9359 - val_auc: 0.9889 - val_f1_score: 0.9296 - val_loss: 0.1645 - val_precision: 0.9203 - val_recall: 0.9486 - learning_rate: 0.0010\n",
      "Epoch 34/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 146ms/step - accuracy: 0.8840 - auc: 0.9577 - f1_score: 0.8876 - loss: 0.2638 - precision: 0.8970 - recall: 0.8789 - val_accuracy: 0.9375 - val_auc: 0.9857 - val_f1_score: 0.9373 - val_loss: 0.1719 - val_precision: 0.9406 - val_recall: 0.9276 - learning_rate: 0.0010\n",
      "Epoch 35/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8938 - auc: 0.9611 - f1_score: 0.8916 - loss: 0.2526 - precision: 0.8933 - recall: 0.8955\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 140ms/step - accuracy: 0.8939 - auc: 0.9611 - f1_score: 0.8916 - loss: 0.2527 - precision: 0.8932 - recall: 0.8956 - val_accuracy: 0.9507 - val_auc: 0.9896 - val_f1_score: 0.9493 - val_loss: 0.1600 - val_precision: 0.9516 - val_recall: 0.9516 - learning_rate: 0.0010\n",
      "Training time: 461.00 seconds\n",
      "Evaluating MobileNetV3Small on DeepFire...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - accuracy: 0.8789 - auc: 0.6759 - f1_score: 0.3375 - loss: 0.2783 - precision: 0.6395 - recall: 0.5919        \n",
      "Training results:\n",
      "{'evaluation': {'accuracy': 0.8333333134651184,\n",
      "                'auc': 0.9297825694084167,\n",
      "                'f1_score': 0.5218563675880432,\n",
      "                'loss': 0.367122620344162,\n",
      "                'precision': 0.895348846912384,\n",
      "                'recall': 0.7699999809265137},\n",
      " 'history': {'accuracy': [0.7495887875556946,\n",
      "                          0.8108552694320679,\n",
      "                          0.8367598652839661,\n",
      "                          0.8375822305679321,\n",
      "                          0.8458059430122375,\n",
      "                          0.8597862124443054,\n",
      "                          0.8478618264198303,\n",
      "                          0.8647204041481018,\n",
      "                          0.8638980388641357,\n",
      "                          0.8618420958518982,\n",
      "                          0.859375,\n",
      "                          0.8601973652839661,\n",
      "                          0.8680098652839661,\n",
      "                          0.8787006735801697,\n",
      "                          0.8708881735801697,\n",
      "                          0.8758223652839661,\n",
      "                          0.8766447305679321,\n",
      "                          0.8721216917037964,\n",
      "                          0.8770559430122375,\n",
      "                          0.8774670958518982,\n",
      "                          0.8852795958518982,\n",
      "                          0.8828125,\n",
      "                          0.8696545958518982,\n",
      "                          0.8943256735801697,\n",
      "                          0.8754112124443054,\n",
      "                          0.8885690569877625,\n",
      "                          0.8922697305679321,\n",
      "                          0.8795230388641357,\n",
      "                          0.8980262875556946,\n",
      "                          0.8856908082962036,\n",
      "                          0.8782894611358643,\n",
      "                          0.8926809430122375,\n",
      "                          0.8898026347160339,\n",
      "                          0.8865131735801697,\n",
      "                          0.8963815569877625],\n",
      "             'auc': [0.8281760811805725,\n",
      "                     0.8948885798454285,\n",
      "                     0.9127747416496277,\n",
      "                     0.9171208739280701,\n",
      "                     0.9218835830688477,\n",
      "                     0.9271066784858704,\n",
      "                     0.9302915930747986,\n",
      "                     0.9329927563667297,\n",
      "                     0.93424391746521,\n",
      "                     0.9343428015708923,\n",
      "                     0.9392791986465454,\n",
      "                     0.9363353252410889,\n",
      "                     0.9383797645568848,\n",
      "                     0.9470635652542114,\n",
      "                     0.942628026008606,\n",
      "                     0.9479383230209351,\n",
      "                     0.9439589381217957,\n",
      "                     0.9436267018318176,\n",
      "                     0.948371171951294,\n",
      "                     0.9465442895889282,\n",
      "                     0.9503358006477356,\n",
      "                     0.9535976052284241,\n",
      "                     0.9422352313995361,\n",
      "                     0.9567611813545227,\n",
      "                     0.9454476237297058,\n",
      "                     0.9568146467208862,\n",
      "                     0.9601669907569885,\n",
      "                     0.9546262621879578,\n",
      "                     0.9598425030708313,\n",
      "                     0.9548740386962891,\n",
      "                     0.9502999186515808,\n",
      "                     0.9591700434684753,\n",
      "                     0.9546960592269897,\n",
      "                     0.9543326497077942,\n",
      "                     0.9587371349334717],\n",
      "             'f1_score': [0.7374008297920227,\n",
      "                          0.8057348728179932,\n",
      "                          0.8358839750289917,\n",
      "                          0.8330950140953064,\n",
      "                          0.8445279002189636,\n",
      "                          0.8578926920890808,\n",
      "                          0.8494858741760254,\n",
      "                          0.8648540377616882,\n",
      "                          0.8618823885917664,\n",
      "                          0.8625590801239014,\n",
      "                          0.8543729186058044,\n",
      "                          0.8592934012413025,\n",
      "                          0.8645510673522949,\n",
      "                          0.8784562945365906,\n",
      "                          0.8702362179756165,\n",
      "                          0.8759720325469971,\n",
      "                          0.8744227886199951,\n",
      "                          0.8701459765434265,\n",
      "                          0.8769578337669373,\n",
      "                          0.876450777053833,\n",
      "                          0.8844020962715149,\n",
      "                          0.8826870918273926,\n",
      "                          0.8679856657981873,\n",
      "                          0.8920729756355286,\n",
      "                          0.8744255900382996,\n",
      "                          0.886950671672821,\n",
      "                          0.894675076007843,\n",
      "                          0.8796876668930054,\n",
      "                          0.8959444761276245,\n",
      "                          0.8832700848579407,\n",
      "                          0.8783746957778931,\n",
      "                          0.8926512002944946,\n",
      "                          0.8865916728973389,\n",
      "                          0.8878905773162842,\n",
      "                          0.8937515616416931],\n",
      "             'learning_rate': [0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513],\n",
      "             'loss': [0.5534639954566956,\n",
      "                      0.4360051453113556,\n",
      "                      0.38893863558769226,\n",
      "                      0.37603452801704407,\n",
      "                      0.3657640516757965,\n",
      "                      0.34736886620521545,\n",
      "                      0.34192195534706116,\n",
      "                      0.3381807506084442,\n",
      "                      0.3271825909614563,\n",
      "                      0.33049580454826355,\n",
      "                      0.3153100609779358,\n",
      "                      0.32349491119384766,\n",
      "                      0.31854984164237976,\n",
      "                      0.29445815086364746,\n",
      "                      0.30577462911605835,\n",
      "                      0.2906306982040405,\n",
      "                      0.3033581078052521,\n",
      "                      0.3040390908718109,\n",
      "                      0.29087868332862854,\n",
      "                      0.29502955079078674,\n",
      "                      0.2837570011615753,\n",
      "                      0.2763691544532776,\n",
      "                      0.3092525005340576,\n",
      "                      0.26621899008750916,\n",
      "                      0.29769831895828247,\n",
      "                      0.26660290360450745,\n",
      "                      0.25531065464019775,\n",
      "                      0.2718905508518219,\n",
      "                      0.25600701570510864,\n",
      "                      0.27122142910957336,\n",
      "                      0.2857062816619873,\n",
      "                      0.257829487323761,\n",
      "                      0.27382123470306396,\n",
      "                      0.27354496717453003,\n",
      "                      0.2593822181224823],\n",
      "             'precision': [0.7457098364830017,\n",
      "                           0.8121869564056396,\n",
      "                           0.8344155550003052,\n",
      "                           0.8289036750793457,\n",
      "                           0.8475409746170044,\n",
      "                           0.8582028150558472,\n",
      "                           0.8449920415878296,\n",
      "                           0.8614020943641663,\n",
      "                           0.8607698678970337,\n",
      "                           0.8608414530754089,\n",
      "                           0.8522537350654602,\n",
      "                           0.8638228178024292,\n",
      "                           0.8564555048942566,\n",
      "                           0.8751999735832214,\n",
      "                           0.8674304485321045,\n",
      "                           0.8730031847953796,\n",
      "                           0.8681229948997498,\n",
      "                           0.8697788715362549,\n",
      "                           0.8720836639404297,\n",
      "                           0.8794093728065491,\n",
      "                           0.8801619410514832,\n",
      "                           0.8816326260566711,\n",
      "                           0.8642172813415527,\n",
      "                           0.8896210789680481,\n",
      "                           0.8759124279022217,\n",
      "                           0.8841413259506226,\n",
      "                           0.8955823183059692,\n",
      "                           0.8760129809379578,\n",
      "                           0.8946938514709473,\n",
      "                           0.8823052048683167,\n",
      "                           0.8655999898910522,\n",
      "                           0.8941841721534729,\n",
      "                           0.8776167631149292,\n",
      "                           0.8872668147087097,\n",
      "                           0.8914100527763367],\n",
      "             'recall': [0.7359507083892822,\n",
      "                        0.8054635524749756,\n",
      "                        0.8419328331947327,\n",
      "                        0.8407750725746155,\n",
      "                        0.8454619646072388,\n",
      "                        0.8603305816650391,\n",
      "                        0.8586429953575134,\n",
      "                        0.8719412684440613,\n",
      "                        0.8671616911888123,\n",
      "                        0.8664495348930359,\n",
      "                        0.8608769178390503,\n",
      "                        0.8581907153129578,\n",
      "                        0.8826446533203125,\n",
      "                        0.8872668147087097,\n",
      "                        0.8745874762535095,\n",
      "                        0.8843042254447937,\n",
      "                        0.886776864528656,\n",
      "                        0.874794065952301,\n",
      "                        0.8856208920478821,\n",
      "                        0.8765330910682678,\n",
      "                        0.8924466371536255,\n",
      "                        0.8852459192276001,\n",
      "                        0.8803905844688416,\n",
      "                        0.8977556228637695,\n",
      "                        0.8780487775802612,\n",
      "                        0.8922056555747986,\n",
      "                        0.8941459655761719,\n",
      "                        0.8853398561477661,\n",
      "                        0.9020575881004333,\n",
      "                        0.8909835815429688,\n",
      "                        0.8942148685455322,\n",
      "                        0.8949070572853088,\n",
      "                        0.9038142561912537,\n",
      "                        0.8887083530426025,\n",
      "                        0.9031198620796204],\n",
      "             'val_accuracy': [0.5279605388641357,\n",
      "                              0.4950657784938812,\n",
      "                              0.49177631735801697,\n",
      "                              0.5641447305679321,\n",
      "                              0.6842105388641357,\n",
      "                              0.8536184430122375,\n",
      "                              0.8963815569877625,\n",
      "                              0.9046052694320679,\n",
      "                              0.9128289222717285,\n",
      "                              0.9095394611358643,\n",
      "                              0.9292762875556946,\n",
      "                              0.9128289222717285,\n",
      "                              0.9144737124443054,\n",
      "                              0.9309210777282715,\n",
      "                              0.9128289222717285,\n",
      "                              0.9177631735801697,\n",
      "                              0.9194079041481018,\n",
      "                              0.9358552694320679,\n",
      "                              0.9325658082962036,\n",
      "                              0.9309210777282715,\n",
      "                              0.9144737124443054,\n",
      "                              0.9276315569877625,\n",
      "                              0.9325658082962036,\n",
      "                              0.9276315569877625,\n",
      "                              0.9391447305679321,\n",
      "                              0.9407894611358643,\n",
      "                              0.9342105388641357,\n",
      "                              0.9391447305679321,\n",
      "                              0.9309210777282715,\n",
      "                              0.9506579041481018,\n",
      "                              0.9342105388641357,\n",
      "                              0.9276315569877625,\n",
      "                              0.9358552694320679,\n",
      "                              0.9375,\n",
      "                              0.9506579041481018],\n",
      "             'val_auc': [0.8726486563682556,\n",
      "                         0.9241399765014648,\n",
      "                         0.9339629411697388,\n",
      "                         0.9389234781265259,\n",
      "                         0.953769862651825,\n",
      "                         0.9731760025024414,\n",
      "                         0.9719091653823853,\n",
      "                         0.9627441167831421,\n",
      "                         0.9710768461227417,\n",
      "                         0.9728204011917114,\n",
      "                         0.9755072593688965,\n",
      "                         0.9742133617401123,\n",
      "                         0.9764542579650879,\n",
      "                         0.978377640247345,\n",
      "                         0.9804871082305908,\n",
      "                         0.9720935821533203,\n",
      "                         0.9812189340591431,\n",
      "                         0.9844972491264343,\n",
      "                         0.9851378798484802,\n",
      "                         0.9817394018173218,\n",
      "                         0.9738735556602478,\n",
      "                         0.9857545495033264,\n",
      "                         0.9832792282104492,\n",
      "                         0.9808264970779419,\n",
      "                         0.9852759838104248,\n",
      "                         0.9882355332374573,\n",
      "                         0.9837899804115295,\n",
      "                         0.984861433506012,\n",
      "                         0.9860033392906189,\n",
      "                         0.9904724359512329,\n",
      "                         0.9858821630477905,\n",
      "                         0.9823934435844421,\n",
      "                         0.9889240264892578,\n",
      "                         0.9856593012809753,\n",
      "                         0.9895810484886169],\n",
      "             'val_f1_score': [0.6850221753120422,\n",
      "                              0.6591149568557739,\n",
      "                              0.6543385982513428,\n",
      "                              0.7009027600288391,\n",
      "                              0.7423732876777649,\n",
      "                              0.8666445016860962,\n",
      "                              0.8995041847229004,\n",
      "                              0.904183566570282,\n",
      "                              0.9089266061782837,\n",
      "                              0.9102649688720703,\n",
      "                              0.9249666333198547,\n",
      "                              0.9131951332092285,\n",
      "                              0.9103707671165466,\n",
      "                              0.9271836280822754,\n",
      "                              0.904774010181427,\n",
      "                              0.9203433990478516,\n",
      "                              0.9163637757301331,\n",
      "                              0.9265751838684082,\n",
      "                              0.9334185719490051,\n",
      "                              0.928936779499054,\n",
      "                              0.9120137095451355,\n",
      "                              0.9237725138664246,\n",
      "                              0.9304496645927429,\n",
      "                              0.9246628284454346,\n",
      "                              0.9367336630821228,\n",
      "                              0.9317358136177063,\n",
      "                              0.9319537281990051,\n",
      "                              0.9360477328300476,\n",
      "                              0.9246088266372681,\n",
      "                              0.9510901570320129,\n",
      "                              0.9302231669425964,\n",
      "                              0.9262173175811768,\n",
      "                              0.9296215176582336,\n",
      "                              0.93732088804245,\n",
      "                              0.9493245482444763],\n",
      "             'val_loss': [0.7109383344650269,\n",
      "                          0.7099002003669739,\n",
      "                          0.7107057571411133,\n",
      "                          0.6123239398002625,\n",
      "                          0.5522667169570923,\n",
      "                          0.44370123744010925,\n",
      "                          0.3776034712791443,\n",
      "                          0.29652637243270874,\n",
      "                          0.26373934745788574,\n",
      "                          0.25539976358413696,\n",
      "                          0.22002601623535156,\n",
      "                          0.22027979791164398,\n",
      "                          0.21441622078418732,\n",
      "                          0.19755597412586212,\n",
      "                          0.20030364394187927,\n",
      "                          0.22553707659244537,\n",
      "                          0.2019757330417633,\n",
      "                          0.19237017631530762,\n",
      "                          0.18649746477603912,\n",
      "                          0.19390475749969482,\n",
      "                          0.21493364870548248,\n",
      "                          0.17618010938167572,\n",
      "                          0.19141776859760284,\n",
      "                          0.19237089157104492,\n",
      "                          0.1816793978214264,\n",
      "                          0.16215868294239044,\n",
      "                          0.1817639172077179,\n",
      "                          0.17366556823253632,\n",
      "                          0.17255890369415283,\n",
      "                          0.15761364996433258,\n",
      "                          0.16921930015087128,\n",
      "                          0.18234893679618835,\n",
      "                          0.1644938439130783,\n",
      "                          0.17186804115772247,\n",
      "                          0.15997722744941711],\n",
      "             'val_precision': [0.5279605388641357,\n",
      "                               0.4950657784938812,\n",
      "                               0.49093905091285706,\n",
      "                               0.5415225028991699,\n",
      "                               0.6021052598953247,\n",
      "                               0.7742782235145569,\n",
      "                               0.841791033744812,\n",
      "                               0.8996763825416565,\n",
      "                               0.9130434989929199,\n",
      "                               0.914826512336731,\n",
      "                               0.942176878452301,\n",
      "                               0.9041533470153809,\n",
      "                               0.9295774698257446,\n",
      "                               0.920265793800354,\n",
      "                               0.894197940826416,\n",
      "                               0.8993710875511169,\n",
      "                               0.9256756901741028,\n",
      "                               0.9328621625900269,\n",
      "                               0.9174603223800659,\n",
      "                               0.9429529905319214,\n",
      "                               0.9172185659408569,\n",
      "                               0.9440559148788452,\n",
      "                               0.9320388436317444,\n",
      "                               0.9230769276618958,\n",
      "                               0.933993399143219,\n",
      "                               0.9422382712364197,\n",
      "                               0.9253246784210205,\n",
      "                               0.9250814318656921,\n",
      "                               0.9453924894332886,\n",
      "                               0.9536423683166504,\n",
      "                               0.9259259104728699,\n",
      "                               0.926174521446228,\n",
      "                               0.920265793800354,\n",
      "                               0.940559446811676,\n",
      "                               0.9516128897666931],\n",
      "             'val_recall': [1.0,\n",
      "                            1.0,\n",
      "                            1.0,\n",
      "                            1.0,\n",
      "                            0.9896193742752075,\n",
      "                            0.9899328947067261,\n",
      "                            0.965753436088562,\n",
      "                            0.911475419998169,\n",
      "                            0.9100000262260437,\n",
      "                            0.9119496941566467,\n",
      "                            0.9141914248466492,\n",
      "                            0.9248365759849548,\n",
      "                            0.8918918967247009,\n",
      "                            0.9389830231666565,\n",
      "                            0.922535240650177,\n",
      "                            0.9407894611358643,\n",
      "                            0.9102990031242371,\n",
      "                            0.9295774698257446,\n",
      "                            0.9506579041481018,\n",
      "                            0.9183006286621094,\n",
      "                            0.9111841917037964,\n",
      "                            0.9060402512550354,\n",
      "                            0.9350649118423462,\n",
      "                            0.9292929172515869,\n",
      "                            0.9433333277702332,\n",
      "                            0.9288256168365479,\n",
      "                            0.943708598613739,\n",
      "                            0.9530201554298401,\n",
      "                            0.9141914248466492,\n",
      "                            0.9473684430122375,\n",
      "                            0.9385665655136108,\n",
      "                            0.926174521446228,\n",
      "                            0.948630154132843,\n",
      "                            0.9275861978530884,\n",
      "                            0.9516128897666931]},\n",
      " 'optimal_threshold': 0.07212388515472412,\n",
      " 'train_counts': {'fire': 760, 'nofire': 760},\n",
      " 'train_counts_total': 1520,\n",
      " 'train_dataset_size': 2432,\n",
      " 'training_time': 460.9986879825592,\n",
      " 'val_counts': {'fire': 0, 'nofire': 0},\n",
      " 'val_counts_total': 0,\n",
      " 'val_dataset_size': 608}\n",
      "Training model: MobileNetV3Small on dataset: FIRE\n",
      "Epoch 1/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 285ms/step - accuracy: 0.7589 - auc: 0.8544 - f1_score: 0.6103 - loss: 0.6038 - precision: 0.6358 - recall: 0.7655 - val_accuracy: 0.5938 - val_auc: 0.9075 - val_f1_score: 0.5197 - val_loss: 0.6866 - val_precision: 0.3544 - val_recall: 0.9655 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 209ms/step - accuracy: 0.8286 - auc: 0.8759 - f1_score: 0.6665 - loss: 0.4290 - precision: 0.6385 - recall: 0.7076 - val_accuracy: 0.7422 - val_auc: 0.8907 - val_f1_score: 0.0000e+00 - val_loss: 0.5702 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 216ms/step - accuracy: 0.8518 - auc: 0.8963 - f1_score: 0.6750 - loss: 0.3590 - precision: 0.6513 - recall: 0.7133 - val_accuracy: 0.7656 - val_auc: 0.8341 - val_f1_score: 0.0000e+00 - val_loss: 0.5319 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 193ms/step - accuracy: 0.8626 - auc: 0.9006 - f1_score: 0.6873 - loss: 0.3329 - precision: 0.6886 - recall: 0.6914 - val_accuracy: 0.7526 - val_auc: 0.8804 - val_f1_score: 0.0000e+00 - val_loss: 0.5457 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 205ms/step - accuracy: 0.8934 - auc: 0.9274 - f1_score: 0.7740 - loss: 0.3094 - precision: 0.8148 - recall: 0.7337 - val_accuracy: 0.7552 - val_auc: 0.9556 - val_f1_score: 0.0000e+00 - val_loss: 0.4995 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 203ms/step - accuracy: 0.8353 - auc: 0.8712 - f1_score: 0.6272 - loss: 0.3927 - precision: 0.6826 - recall: 0.6081 - val_accuracy: 0.7604 - val_auc: 0.9592 - val_f1_score: 0.0000e+00 - val_loss: 0.4941 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 205ms/step - accuracy: 0.8762 - auc: 0.9072 - f1_score: 0.7261 - loss: 0.3184 - precision: 0.7767 - recall: 0.6917 - val_accuracy: 0.7578 - val_auc: 0.9588 - val_f1_score: 0.0000e+00 - val_loss: 0.4797 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 203ms/step - accuracy: 0.8938 - auc: 0.9261 - f1_score: 0.7713 - loss: 0.2883 - precision: 0.7900 - recall: 0.7640 - val_accuracy: 0.7552 - val_auc: 0.9680 - val_f1_score: 0.0238 - val_loss: 0.4339 - val_precision: 1.0000 - val_recall: 0.0105 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 198ms/step - accuracy: 0.8799 - auc: 0.9260 - f1_score: 0.7433 - loss: 0.2961 - precision: 0.7817 - recall: 0.7225 - val_accuracy: 0.7630 - val_auc: 0.9522 - val_f1_score: 0.2216 - val_loss: 0.4203 - val_precision: 0.9412 - val_recall: 0.1509 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 214ms/step - accuracy: 0.8806 - auc: 0.9243 - f1_score: 0.7160 - loss: 0.3042 - precision: 0.7669 - recall: 0.6961 - val_accuracy: 0.8177 - val_auc: 0.9700 - val_f1_score: 0.3291 - val_loss: 0.3486 - val_precision: 1.0000 - val_recall: 0.2473 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 184ms/step - accuracy: 0.8898 - auc: 0.9317 - f1_score: 0.7633 - loss: 0.2850 - precision: 0.7857 - recall: 0.7507 - val_accuracy: 0.8125 - val_auc: 0.9705 - val_f1_score: 0.4063 - val_loss: 0.3379 - val_precision: 0.9677 - val_recall: 0.2970 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 196ms/step - accuracy: 0.8794 - auc: 0.9315 - f1_score: 0.7347 - loss: 0.2822 - precision: 0.7971 - recall: 0.6835 - val_accuracy: 0.8906 - val_auc: 0.9621 - val_f1_score: 0.6608 - val_loss: 0.2404 - val_precision: 0.9750 - val_recall: 0.4875 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 193ms/step - accuracy: 0.8894 - auc: 0.9331 - f1_score: 0.7736 - loss: 0.2890 - precision: 0.8164 - recall: 0.7514 - val_accuracy: 0.8646 - val_auc: 0.9606 - val_f1_score: 0.6279 - val_loss: 0.2713 - val_precision: 0.9091 - val_recall: 0.5155 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 196ms/step - accuracy: 0.8635 - auc: 0.9259 - f1_score: 0.7021 - loss: 0.3020 - precision: 0.7255 - recall: 0.7007 - val_accuracy: 0.9010 - val_auc: 0.9716 - val_f1_score: 0.7316 - val_loss: 0.2232 - val_precision: 0.9492 - val_recall: 0.6154 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 190ms/step - accuracy: 0.8860 - auc: 0.9260 - f1_score: 0.7436 - loss: 0.2866 - precision: 0.7829 - recall: 0.7200 - val_accuracy: 0.8932 - val_auc: 0.9712 - val_f1_score: 0.7359 - val_loss: 0.2435 - val_precision: 0.9552 - val_recall: 0.6275 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 184ms/step - accuracy: 0.8840 - auc: 0.9421 - f1_score: 0.7631 - loss: 0.2695 - precision: 0.8042 - recall: 0.7378 - val_accuracy: 0.8958 - val_auc: 0.9696 - val_f1_score: 0.7699 - val_loss: 0.2223 - val_precision: 0.9079 - val_recall: 0.6765 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 210ms/step - accuracy: 0.8957 - auc: 0.9396 - f1_score: 0.7636 - loss: 0.2669 - precision: 0.8178 - recall: 0.7369 - val_accuracy: 0.9115 - val_auc: 0.9802 - val_f1_score: 0.8010 - val_loss: 0.2013 - val_precision: 0.9342 - val_recall: 0.7100 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 167ms/step - accuracy: 0.8861 - auc: 0.9280 - f1_score: 0.7478 - loss: 0.2891 - precision: 0.8390 - recall: 0.6841 - val_accuracy: 0.9141 - val_auc: 0.9737 - val_f1_score: 0.7473 - val_loss: 0.1982 - val_precision: 0.9153 - val_recall: 0.6585 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 187ms/step - accuracy: 0.8924 - auc: 0.9480 - f1_score: 0.7677 - loss: 0.2546 - precision: 0.8510 - recall: 0.7148 - val_accuracy: 0.9010 - val_auc: 0.9776 - val_f1_score: 0.7629 - val_loss: 0.2015 - val_precision: 0.9231 - val_recall: 0.6452 - learning_rate: 0.0010\n",
      "Epoch 20/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 184ms/step - accuracy: 0.8976 - auc: 0.9549 - f1_score: 0.7722 - loss: 0.2302 - precision: 0.8061 - recall: 0.7563 - val_accuracy: 0.9323 - val_auc: 0.9716 - val_f1_score: 0.8120 - val_loss: 0.1797 - val_precision: 0.9275 - val_recall: 0.7529 - learning_rate: 0.0010\n",
      "Epoch 21/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 183ms/step - accuracy: 0.9032 - auc: 0.9537 - f1_score: 0.7828 - loss: 0.2306 - precision: 0.8651 - recall: 0.7212 - val_accuracy: 0.9375 - val_auc: 0.9875 - val_f1_score: 0.8626 - val_loss: 0.1625 - val_precision: 0.9753 - val_recall: 0.7822 - learning_rate: 0.0010\n",
      "Epoch 22/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 188ms/step - accuracy: 0.8865 - auc: 0.9437 - f1_score: 0.7473 - loss: 0.2520 - precision: 0.7639 - recall: 0.7470 - val_accuracy: 0.9349 - val_auc: 0.9771 - val_f1_score: 0.8615 - val_loss: 0.1728 - val_precision: 0.8851 - val_recall: 0.8370 - learning_rate: 0.0010\n",
      "Epoch 23/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 187ms/step - accuracy: 0.8981 - auc: 0.9431 - f1_score: 0.7834 - loss: 0.2573 - precision: 0.8209 - recall: 0.7517 - val_accuracy: 0.9271 - val_auc: 0.9735 - val_f1_score: 0.8136 - val_loss: 0.1775 - val_precision: 0.8605 - val_recall: 0.8222 - learning_rate: 0.0010\n",
      "Epoch 24/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 212ms/step - accuracy: 0.8895 - auc: 0.9411 - f1_score: 0.7462 - loss: 0.2557 - precision: 0.8153 - recall: 0.6997 - val_accuracy: 0.9141 - val_auc: 0.9683 - val_f1_score: 0.8246 - val_loss: 0.2028 - val_precision: 0.8558 - val_recall: 0.8318 - learning_rate: 0.0010\n",
      "Epoch 25/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.9056 - auc: 0.9462 - f1_score: 0.7963 - loss: 0.2453 - precision: 0.8413 - recall: 0.7605 - val_accuracy: 0.9193 - val_auc: 0.9806 - val_f1_score: 0.8272 - val_loss: 0.1847 - val_precision: 0.9474 - val_recall: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 26/80\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9084 - auc: 0.9472 - f1_score: 0.7940 - loss: 0.2311 - precision: 0.8267 - recall: 0.7725\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 182ms/step - accuracy: 0.9084 - auc: 0.9471 - f1_score: 0.7939 - loss: 0.2314 - precision: 0.8268 - recall: 0.7722 - val_accuracy: 0.9323 - val_auc: 0.9844 - val_f1_score: 0.8439 - val_loss: 0.1656 - val_precision: 0.9114 - val_recall: 0.7912 - learning_rate: 0.0010\n",
      "Training time: 293.36 seconds\n",
      "Evaluating MobileNetV3Small on FIRE...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step - accuracy: 0.8196 - auc: 0.5827 - f1_score: 0.2596 - loss: 0.5418 - precision: 0.6396 - recall: 0.4432        \n",
      "Training results:\n",
      "{'evaluation': {'accuracy': 0.7135416865348816,\n",
      "                'auc': 0.7562500238418579,\n",
      "                'f1_score': 0.36105605959892273,\n",
      "                'loss': 0.8966307640075684,\n",
      "                'precision': 0.8947368264198303,\n",
      "                'recall': 0.5099999904632568},\n",
      " 'history': {'accuracy': [0.7818877696990967,\n",
      "                          0.8252550959587097,\n",
      "                          0.8533163070678711,\n",
      "                          0.858418345451355,\n",
      "                          0.8807398080825806,\n",
      "                          0.8488520383834839,\n",
      "                          0.8756377696990967,\n",
      "                          0.8813775777816772,\n",
      "                          0.8858418464660645,\n",
      "                          0.8781887888908386,\n",
      "                          0.8883928656578064,\n",
      "                          0.8839285969734192,\n",
      "                          0.8966836929321289,\n",
      "                          0.8762755393981934,\n",
      "                          0.8852040767669678,\n",
      "                          0.8839285969734192,\n",
      "                          0.8947703838348389,\n",
      "                          0.8839285969734192,\n",
      "                          0.8922193646430969,\n",
      "                          0.8947703838348389,\n",
      "                          0.8922193646430969,\n",
      "                          0.8960459232330322,\n",
      "                          0.8903061151504517,\n",
      "                          0.8954081535339355,\n",
      "                          0.8998724222183228,\n",
      "                          0.9068877696990967],\n",
      "             'auc': [0.852928638458252,\n",
      "                     0.8726705312728882,\n",
      "                     0.891511857509613,\n",
      "                     0.8936803936958313,\n",
      "                     0.9228273630142212,\n",
      "                     0.8862273097038269,\n",
      "                     0.9150472283363342,\n",
      "                     0.9194809198379517,\n",
      "                     0.9248309135437012,\n",
      "                     0.9246925115585327,\n",
      "                     0.9366868734359741,\n",
      "                     0.9292145371437073,\n",
      "                     0.9351096153259277,\n",
      "                     0.9313914179801941,\n",
      "                     0.9261141419410706,\n",
      "                     0.9359005689620972,\n",
      "                     0.9387264847755432,\n",
      "                     0.9283276796340942,\n",
      "                     0.9463168382644653,\n",
      "                     0.9478242993354797,\n",
      "                     0.9459044337272644,\n",
      "                     0.9502272009849548,\n",
      "                     0.9416794776916504,\n",
      "                     0.9443036913871765,\n",
      "                     0.947283148765564,\n",
      "                     0.9448419213294983],\n",
      "             'f1_score': [0.6264283061027527,\n",
      "                          0.6577020883560181,\n",
      "                          0.6837029457092285,\n",
      "                          0.6829020380973816,\n",
      "                          0.7430198788642883,\n",
      "                          0.6602796316146851,\n",
      "                          0.7304311990737915,\n",
      "                          0.7482880353927612,\n",
      "                          0.7529800534248352,\n",
      "                          0.7200539112091064,\n",
      "                          0.7605839967727661,\n",
      "                          0.7451820969581604,\n",
      "                          0.7834805250167847,\n",
      "                          0.7319493293762207,\n",
      "                          0.7265278697013855,\n",
      "                          0.7443230152130127,\n",
      "                          0.75551837682724,\n",
      "                          0.7387483716011047,\n",
      "                          0.7610003352165222,\n",
      "                          0.7696089744567871,\n",
      "                          0.7651979327201843,\n",
      "                          0.7621800899505615,\n",
      "                          0.7710631489753723,\n",
      "                          0.7591952085494995,\n",
      "                          0.7793310284614563,\n",
      "                          0.7871168255805969],\n",
      "             'learning_rate': [0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513],\n",
      "             'loss': [0.5600391030311584,\n",
      "                      0.43390804529190063,\n",
      "                      0.3679969608783722,\n",
      "                      0.35204634070396423,\n",
      "                      0.31742990016937256,\n",
      "                      0.3802967071533203,\n",
      "                      0.30753594636917114,\n",
      "                      0.3065201938152313,\n",
      "                      0.28734391927719116,\n",
      "                      0.2962253987789154,\n",
      "                      0.2772747576236725,\n",
      "                      0.2864772081375122,\n",
      "                      0.27614155411720276,\n",
      "                      0.2840796709060669,\n",
      "                      0.28840896487236023,\n",
      "                      0.2778831124305725,\n",
      "                      0.26110270619392395,\n",
      "                      0.2841344475746155,\n",
      "                      0.25477007031440735,\n",
      "                      0.24578191339969635,\n",
      "                      0.253412663936615,\n",
      "                      0.23880597949028015,\n",
      "                      0.2642019987106323,\n",
      "                      0.2481749802827835,\n",
      "                      0.246931254863739,\n",
      "                      0.24313515424728394],\n",
      "             'precision': [0.5946372151374817,\n",
      "                           0.6421800851821899,\n",
      "                           0.6797900199890137,\n",
      "                           0.6985507011413574,\n",
      "                           0.7857142686843872,\n",
      "                           0.7167630195617676,\n",
      "                           0.7710145115852356,\n",
      "                           0.7673797011375427,\n",
      "                           0.7827298045158386,\n",
      "                           0.7764350175857544,\n",
      "                           0.7954545617103577,\n",
      "                           0.8082596063613892,\n",
      "                           0.8140161633491516,\n",
      "                           0.761904776096344,\n",
      "                           0.7848485112190247,\n",
      "                           0.7947976589202881,\n",
      "                           0.7900874614715576,\n",
      "                           0.8029850721359253,\n",
      "                           0.8194842338562012,\n",
      "                           0.8067227005958557,\n",
      "                           0.823699414730072,\n",
      "                           0.805232584476471,\n",
      "                           0.821529746055603,\n",
      "                           0.8237082362174988,\n",
      "                           0.814084529876709,\n",
      "                           0.834319531917572],\n",
      "             'recall': [0.7631579041481018,\n",
      "                        0.6878172755241394,\n",
      "                        0.7057220935821533,\n",
      "                        0.6713091731071472,\n",
      "                        0.7105942964553833,\n",
      "                        0.6408268809318542,\n",
      "                        0.6963350772857666,\n",
      "                        0.7435232996940613,\n",
      "                        0.735602080821991,\n",
      "                        0.6871657967567444,\n",
      "                        0.7310705184936523,\n",
      "                        0.7007672786712646,\n",
      "                        0.7645569443702698,\n",
      "                        0.7139107584953308,\n",
      "                        0.7038043737411499,\n",
      "                        0.712435245513916,\n",
      "                        0.7445054650306702,\n",
      "                        0.6987013220787048,\n",
      "                        0.7295918464660645,\n",
      "                        0.75,\n",
      "                        0.7251908183097839,\n",
      "                        0.7426273226737976,\n",
      "                        0.7268170714378357,\n",
      "                        0.7188329100608826,\n",
      "                        0.7605262994766235,\n",
      "                        0.7580645084381104],\n",
      "             'val_accuracy': [0.59375,\n",
      "                              0.7421875,\n",
      "                              0.765625,\n",
      "                              0.7526041865348816,\n",
      "                              0.7552083134651184,\n",
      "                              0.7604166865348816,\n",
      "                              0.7578125,\n",
      "                              0.7552083134651184,\n",
      "                              0.7630208134651184,\n",
      "                              0.8177083134651184,\n",
      "                              0.8125,\n",
      "                              0.890625,\n",
      "                              0.8645833134651184,\n",
      "                              0.9010416865348816,\n",
      "                              0.8932291865348816,\n",
      "                              0.8958333134651184,\n",
      "                              0.9114583134651184,\n",
      "                              0.9140625,\n",
      "                              0.9010416865348816,\n",
      "                              0.9322916865348816,\n",
      "                              0.9375,\n",
      "                              0.9348958134651184,\n",
      "                              0.9270833134651184,\n",
      "                              0.9140625,\n",
      "                              0.9192708134651184,\n",
      "                              0.9322916865348816],\n",
      "             'val_auc': [0.907484769821167,\n",
      "                         0.8907318711280823,\n",
      "                         0.8340703248977661,\n",
      "                         0.8803860545158386,\n",
      "                         0.9556126594543457,\n",
      "                         0.9591833353042603,\n",
      "                         0.9587998390197754,\n",
      "                         0.9679839611053467,\n",
      "                         0.9521515369415283,\n",
      "                         0.969959020614624,\n",
      "                         0.9705419540405273,\n",
      "                         0.962109386920929,\n",
      "                         0.9606128931045532,\n",
      "                         0.9715898633003235,\n",
      "                         0.9711618423461914,\n",
      "                         0.9695799946784973,\n",
      "                         0.9801936745643616,\n",
      "                         0.9737118482589722,\n",
      "                         0.9776447415351868,\n",
      "                         0.9715522527694702,\n",
      "                         0.9875450730323792,\n",
      "                         0.9770883321762085,\n",
      "                         0.9734504818916321,\n",
      "                         0.968285083770752,\n",
      "                         0.9805776476860046,\n",
      "                         0.98436039686203],\n",
      "             'val_f1_score': [0.5196640491485596,\n",
      "                              0.0,\n",
      "                              0.0,\n",
      "                              0.0,\n",
      "                              0.0,\n",
      "                              0.0,\n",
      "                              0.0,\n",
      "                              0.0238095223903656,\n",
      "                              0.22163207828998566,\n",
      "                              0.329058438539505,\n",
      "                              0.4063401222229004,\n",
      "                              0.6607679128646851,\n",
      "                              0.6278741955757141,\n",
      "                              0.7316038012504578,\n",
      "                              0.7359215617179871,\n",
      "                              0.7699032425880432,\n",
      "                              0.8009850978851318,\n",
      "                              0.7472968697547913,\n",
      "                              0.7629094123840332,\n",
      "                              0.8120462894439697,\n",
      "                              0.8626061081886292,\n",
      "                              0.8614946007728577,\n",
      "                              0.8136069774627686,\n",
      "                              0.8245922923088074,\n",
      "                              0.8271892070770264,\n",
      "                              0.8438641428947449],\n",
      "             'val_loss': [0.6865615248680115,\n",
      "                          0.5702028870582581,\n",
      "                          0.5319081544876099,\n",
      "                          0.5456636548042297,\n",
      "                          0.4994598627090454,\n",
      "                          0.4940560758113861,\n",
      "                          0.47972723841667175,\n",
      "                          0.43394437432289124,\n",
      "                          0.42025992274284363,\n",
      "                          0.34858644008636475,\n",
      "                          0.33791717886924744,\n",
      "                          0.24041622877120972,\n",
      "                          0.2712915241718292,\n",
      "                          0.2232043594121933,\n",
      "                          0.24354130029678345,\n",
      "                          0.2222641259431839,\n",
      "                          0.20132236182689667,\n",
      "                          0.19821228086948395,\n",
      "                          0.20154505968093872,\n",
      "                          0.17968662083148956,\n",
      "                          0.1624981015920639,\n",
      "                          0.17276917397975922,\n",
      "                          0.17745108902454376,\n",
      "                          0.2027701586484909,\n",
      "                          0.18470187485218048,\n",
      "                          0.165571928024292],\n",
      "             'val_precision': [0.3544303774833679,\n",
      "                               0.0,\n",
      "                               0.0,\n",
      "                               0.0,\n",
      "                               0.0,\n",
      "                               0.0,\n",
      "                               0.0,\n",
      "                               1.0,\n",
      "                               0.9411764740943909,\n",
      "                               1.0,\n",
      "                               0.9677419066429138,\n",
      "                               0.9750000238418579,\n",
      "                               0.9090909361839294,\n",
      "                               0.9491525292396545,\n",
      "                               0.9552238583564758,\n",
      "                               0.9078947305679321,\n",
      "                               0.9342105388641357,\n",
      "                               0.9152542352676392,\n",
      "                               0.9230769276618958,\n",
      "                               0.9275362491607666,\n",
      "                               0.9753086566925049,\n",
      "                               0.8850574493408203,\n",
      "                               0.8604651093482971,\n",
      "                               0.8557692170143127,\n",
      "                               0.9473684430122375,\n",
      "                               0.9113923907279968],\n",
      "             'val_recall': [0.9655172228813171,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.0,\n",
      "                            0.010526316240429878,\n",
      "                            0.15094339847564697,\n",
      "                            0.24731183052062988,\n",
      "                            0.2970297038555145,\n",
      "                            0.48750001192092896,\n",
      "                            0.5154638886451721,\n",
      "                            0.6153846383094788,\n",
      "                            0.6274510025978088,\n",
      "                            0.6764705777168274,\n",
      "                            0.7099999785423279,\n",
      "                            0.6585366129875183,\n",
      "                            0.6451612710952759,\n",
      "                            0.7529411911964417,\n",
      "                            0.7821782231330872,\n",
      "                            0.8369565010070801,\n",
      "                            0.8222222328186035,\n",
      "                            0.8317757248878479,\n",
      "                            0.7272727489471436,\n",
      "                            0.791208803653717]},\n",
      " 'optimal_threshold': 0.10390546917915344,\n",
      " 'train_counts': {'fire': 755, 'nofire': 244},\n",
      " 'train_counts_total': 999,\n",
      " 'train_dataset_size': 1568,\n",
      " 'training_time': 293.3581964969635,\n",
      " 'val_counts': {'fire': 0, 'nofire': 0},\n",
      " 'val_counts_total': 0,\n",
      " 'val_dataset_size': 384}\n",
      "Training model: MobileNetV3Small on dataset: The Wildfire Dataset_DeepFire\n",
      "Epoch 1/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 174ms/step - accuracy: 0.6815 - auc: 0.7495 - f1_score: 0.7226 - loss: 0.6768 - precision: 0.7758 - recall: 0.6801 - val_accuracy: 0.6236 - val_auc: 0.8446 - val_f1_score: 0.7660 - val_loss: 0.6447 - val_precision: 0.6236 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 160ms/step - accuracy: 0.7481 - auc: 0.8111 - f1_score: 0.7924 - loss: 0.5398 - precision: 0.7849 - recall: 0.8047 - val_accuracy: 0.6371 - val_auc: 0.8510 - val_f1_score: 0.7689 - val_loss: 0.5916 - val_precision: 0.6307 - val_recall: 0.9954 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 161ms/step - accuracy: 0.7545 - auc: 0.8212 - f1_score: 0.8014 - loss: 0.5166 - precision: 0.7848 - recall: 0.8208 - val_accuracy: 0.7678 - val_auc: 0.8745 - val_f1_score: 0.8301 - val_loss: 0.4945 - val_precision: 0.7528 - val_recall: 0.9300 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 157ms/step - accuracy: 0.7603 - auc: 0.8353 - f1_score: 0.8052 - loss: 0.4934 - precision: 0.7969 - recall: 0.8170 - val_accuracy: 0.8047 - val_auc: 0.8886 - val_f1_score: 0.8270 - val_loss: 0.4372 - val_precision: 0.8591 - val_recall: 0.8073 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 150ms/step - accuracy: 0.7601 - auc: 0.8317 - f1_score: 0.8086 - loss: 0.4875 - precision: 0.7937 - recall: 0.8277 - val_accuracy: 0.7969 - val_auc: 0.8819 - val_f1_score: 0.8232 - val_loss: 0.4397 - val_precision: 0.8667 - val_recall: 0.7879 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 146ms/step - accuracy: 0.7879 - auc: 0.8574 - f1_score: 0.8267 - loss: 0.4571 - precision: 0.8160 - recall: 0.8424 - val_accuracy: 0.8210 - val_auc: 0.9028 - val_f1_score: 0.8513 - val_loss: 0.4011 - val_precision: 0.8468 - val_recall: 0.8607 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 147ms/step - accuracy: 0.7794 - auc: 0.8530 - f1_score: 0.8234 - loss: 0.4605 - precision: 0.8101 - recall: 0.8391 - val_accuracy: 0.8381 - val_auc: 0.9095 - val_f1_score: 0.8668 - val_loss: 0.3876 - val_precision: 0.8705 - val_recall: 0.8665 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 155ms/step - accuracy: 0.7813 - auc: 0.8551 - f1_score: 0.8225 - loss: 0.4614 - precision: 0.8143 - recall: 0.8336 - val_accuracy: 0.8274 - val_auc: 0.9121 - val_f1_score: 0.8507 - val_loss: 0.3927 - val_precision: 0.9110 - val_recall: 0.8087 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 216ms/step - accuracy: 0.7796 - auc: 0.8506 - f1_score: 0.8175 - loss: 0.4701 - precision: 0.8091 - recall: 0.8315 - val_accuracy: 0.8196 - val_auc: 0.9043 - val_f1_score: 0.8432 - val_loss: 0.3980 - val_precision: 0.8624 - val_recall: 0.8318 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 143ms/step - accuracy: 0.7854 - auc: 0.8588 - f1_score: 0.8277 - loss: 0.4527 - precision: 0.8154 - recall: 0.8436 - val_accuracy: 0.8438 - val_auc: 0.9219 - val_f1_score: 0.8651 - val_loss: 0.3761 - val_precision: 0.8926 - val_recall: 0.8470 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 149ms/step - accuracy: 0.7869 - auc: 0.8610 - f1_score: 0.8256 - loss: 0.4503 - precision: 0.8188 - recall: 0.8376 - val_accuracy: 0.8153 - val_auc: 0.9014 - val_f1_score: 0.8348 - val_loss: 0.4059 - val_precision: 0.8893 - val_recall: 0.7932 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 144ms/step - accuracy: 0.7956 - auc: 0.8652 - f1_score: 0.8336 - loss: 0.4448 - precision: 0.8257 - recall: 0.8463 - val_accuracy: 0.8438 - val_auc: 0.9274 - val_f1_score: 0.8732 - val_loss: 0.3559 - val_precision: 0.8749 - val_recall: 0.8749 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 116ms/step - accuracy: 0.7801 - auc: 0.8632 - f1_score: 0.8186 - loss: 0.4494 - precision: 0.8066 - recall: 0.8352 - val_accuracy: 0.8423 - val_auc: 0.9240 - val_f1_score: 0.8671 - val_loss: 0.3622 - val_precision: 0.8762 - val_recall: 0.8640 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 81ms/step - accuracy: 0.7920 - auc: 0.8755 - f1_score: 0.8343 - loss: 0.4267 - precision: 0.8211 - recall: 0.8495 - val_accuracy: 0.8509 - val_auc: 0.9236 - val_f1_score: 0.8694 - val_loss: 0.3623 - val_precision: 0.8864 - val_recall: 0.8565 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - accuracy: 0.7876 - auc: 0.8656 - f1_score: 0.8277 - loss: 0.4413 - precision: 0.8279 - recall: 0.8305 - val_accuracy: 0.8480 - val_auc: 0.9281 - val_f1_score: 0.8699 - val_loss: 0.3582 - val_precision: 0.8638 - val_recall: 0.8844 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.8076 - auc: 0.8835 - f1_score: 0.8451 - loss: 0.4171 - precision: 0.8345 - recall: 0.8577 - val_accuracy: 0.8558 - val_auc: 0.9283 - val_f1_score: 0.8788 - val_loss: 0.3478 - val_precision: 0.8807 - val_recall: 0.8858 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.7936 - auc: 0.8752 - f1_score: 0.8330 - loss: 0.4282 - precision: 0.8259 - recall: 0.8427 - val_accuracy: 0.8558 - val_auc: 0.9319 - val_f1_score: 0.8807 - val_loss: 0.3482 - val_precision: 0.8815 - val_recall: 0.8826 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - accuracy: 0.8012 - auc: 0.8804 - f1_score: 0.8360 - loss: 0.4241 - precision: 0.8293 - recall: 0.8475 - val_accuracy: 0.8558 - val_auc: 0.9365 - val_f1_score: 0.8761 - val_loss: 0.3411 - val_precision: 0.9051 - val_recall: 0.8535 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 74ms/step - accuracy: 0.8124 - auc: 0.8881 - f1_score: 0.8457 - loss: 0.4109 - precision: 0.8426 - recall: 0.8503 - val_accuracy: 0.8523 - val_auc: 0.9260 - val_f1_score: 0.8744 - val_loss: 0.3472 - val_precision: 0.8989 - val_recall: 0.8576 - learning_rate: 0.0010\n",
      "Epoch 20/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 74ms/step - accuracy: 0.8074 - auc: 0.8867 - f1_score: 0.8441 - loss: 0.4098 - precision: 0.8387 - recall: 0.8523 - val_accuracy: 0.8572 - val_auc: 0.9319 - val_f1_score: 0.8747 - val_loss: 0.3492 - val_precision: 0.8729 - val_recall: 0.8866 - learning_rate: 0.0010\n",
      "Epoch 21/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 77ms/step - accuracy: 0.8047 - auc: 0.8846 - f1_score: 0.8396 - loss: 0.4176 - precision: 0.8379 - recall: 0.8444 - val_accuracy: 0.8651 - val_auc: 0.9352 - val_f1_score: 0.8919 - val_loss: 0.3324 - val_precision: 0.8777 - val_recall: 0.9116 - learning_rate: 0.0010\n",
      "Epoch 22/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - accuracy: 0.8003 - auc: 0.8786 - f1_score: 0.8388 - loss: 0.4216 - precision: 0.8314 - recall: 0.8525 - val_accuracy: 0.8722 - val_auc: 0.9420 - val_f1_score: 0.8906 - val_loss: 0.3308 - val_precision: 0.9057 - val_recall: 0.8801 - learning_rate: 0.0010\n",
      "Epoch 23/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - accuracy: 0.8166 - auc: 0.8925 - f1_score: 0.8496 - loss: 0.4012 - precision: 0.8431 - recall: 0.8617 - val_accuracy: 0.8537 - val_auc: 0.9338 - val_f1_score: 0.8843 - val_loss: 0.3367 - val_precision: 0.8457 - val_recall: 0.9333 - learning_rate: 0.0010\n",
      "Epoch 24/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 81ms/step - accuracy: 0.8104 - auc: 0.8875 - f1_score: 0.8454 - loss: 0.4075 - precision: 0.8420 - recall: 0.8536 - val_accuracy: 0.8828 - val_auc: 0.9495 - val_f1_score: 0.9032 - val_loss: 0.3164 - val_precision: 0.8935 - val_recall: 0.9174 - learning_rate: 0.0010\n",
      "Epoch 25/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 81ms/step - accuracy: 0.8204 - auc: 0.8951 - f1_score: 0.8535 - loss: 0.3983 - precision: 0.8491 - recall: 0.8615 - val_accuracy: 0.8658 - val_auc: 0.9460 - val_f1_score: 0.8846 - val_loss: 0.3272 - val_precision: 0.9182 - val_recall: 0.8548 - learning_rate: 0.0010\n",
      "Epoch 26/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 80ms/step - accuracy: 0.8118 - auc: 0.8895 - f1_score: 0.8470 - loss: 0.4070 - precision: 0.8358 - recall: 0.8606 - val_accuracy: 0.8757 - val_auc: 0.9455 - val_f1_score: 0.8960 - val_loss: 0.3152 - val_precision: 0.8933 - val_recall: 0.9067 - learning_rate: 0.0010\n",
      "Epoch 27/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 81ms/step - accuracy: 0.8070 - auc: 0.8856 - f1_score: 0.8417 - loss: 0.4126 - precision: 0.8386 - recall: 0.8466 - val_accuracy: 0.8764 - val_auc: 0.9513 - val_f1_score: 0.8970 - val_loss: 0.3039 - val_precision: 0.9142 - val_recall: 0.8826 - learning_rate: 0.0010\n",
      "Epoch 28/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - accuracy: 0.8157 - auc: 0.8937 - f1_score: 0.8479 - loss: 0.4004 - precision: 0.8489 - recall: 0.8497 - val_accuracy: 0.8786 - val_auc: 0.9539 - val_f1_score: 0.8954 - val_loss: 0.3067 - val_precision: 0.9131 - val_recall: 0.8867 - learning_rate: 0.0010\n",
      "Epoch 29/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 80ms/step - accuracy: 0.8282 - auc: 0.8988 - f1_score: 0.8587 - loss: 0.3915 - precision: 0.8564 - recall: 0.8634 - val_accuracy: 0.8828 - val_auc: 0.9535 - val_f1_score: 0.8980 - val_loss: 0.3033 - val_precision: 0.9309 - val_recall: 0.8677 - learning_rate: 0.0010\n",
      "Epoch 30/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 82ms/step - accuracy: 0.8198 - auc: 0.8961 - f1_score: 0.8512 - loss: 0.3944 - precision: 0.8490 - recall: 0.8561 - val_accuracy: 0.8913 - val_auc: 0.9567 - val_f1_score: 0.9101 - val_loss: 0.2953 - val_precision: 0.9050 - val_recall: 0.9226 - learning_rate: 0.0010\n",
      "Epoch 31/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 86ms/step - accuracy: 0.8234 - auc: 0.9031 - f1_score: 0.8575 - loss: 0.3810 - precision: 0.8539 - recall: 0.8632 - val_accuracy: 0.8821 - val_auc: 0.9547 - val_f1_score: 0.8983 - val_loss: 0.2933 - val_precision: 0.9215 - val_recall: 0.8831 - learning_rate: 0.0010\n",
      "Epoch 32/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 80ms/step - accuracy: 0.8270 - auc: 0.9019 - f1_score: 0.8564 - loss: 0.3843 - precision: 0.8576 - recall: 0.8594 - val_accuracy: 0.8878 - val_auc: 0.9573 - val_f1_score: 0.9055 - val_loss: 0.3012 - val_precision: 0.9405 - val_recall: 0.8766 - learning_rate: 0.0010\n",
      "Epoch 33/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - accuracy: 0.8290 - auc: 0.9065 - f1_score: 0.8586 - loss: 0.3776 - precision: 0.8629 - recall: 0.8580 - val_accuracy: 0.8871 - val_auc: 0.9571 - val_f1_score: 0.9026 - val_loss: 0.2910 - val_precision: 0.9238 - val_recall: 0.8884 - learning_rate: 0.0010\n",
      "Epoch 34/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 87ms/step - accuracy: 0.8282 - auc: 0.9017 - f1_score: 0.8580 - loss: 0.3852 - precision: 0.8558 - recall: 0.8654 - val_accuracy: 0.8935 - val_auc: 0.9626 - val_f1_score: 0.9087 - val_loss: 0.2874 - val_precision: 0.9355 - val_recall: 0.8880 - learning_rate: 0.0010\n",
      "Epoch 35/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 85ms/step - accuracy: 0.8290 - auc: 0.9059 - f1_score: 0.8571 - loss: 0.3796 - precision: 0.8546 - recall: 0.8641 - val_accuracy: 0.8814 - val_auc: 0.9504 - val_f1_score: 0.9042 - val_loss: 0.2985 - val_precision: 0.9055 - val_recall: 0.9044 - learning_rate: 0.0010\n",
      "Epoch 36/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 82ms/step - accuracy: 0.8206 - auc: 0.8985 - f1_score: 0.8511 - loss: 0.3935 - precision: 0.8528 - recall: 0.8539 - val_accuracy: 0.8977 - val_auc: 0.9587 - val_f1_score: 0.9140 - val_loss: 0.2787 - val_precision: 0.9174 - val_recall: 0.9174 - learning_rate: 0.0010\n",
      "Epoch 37/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 97ms/step - accuracy: 0.8208 - auc: 0.9027 - f1_score: 0.8510 - loss: 0.3845 - precision: 0.8500 - recall: 0.8551 - val_accuracy: 0.8991 - val_auc: 0.9608 - val_f1_score: 0.9149 - val_loss: 0.2810 - val_precision: 0.9112 - val_recall: 0.9240 - learning_rate: 0.0010\n",
      "Epoch 38/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 84ms/step - accuracy: 0.8228 - auc: 0.9034 - f1_score: 0.8561 - loss: 0.3818 - precision: 0.8542 - recall: 0.8595 - val_accuracy: 0.8920 - val_auc: 0.9616 - val_f1_score: 0.9052 - val_loss: 0.2808 - val_precision: 0.9278 - val_recall: 0.8934 - learning_rate: 0.0010\n",
      "Epoch 39/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 82ms/step - accuracy: 0.8395 - auc: 0.9114 - f1_score: 0.8696 - loss: 0.3695 - precision: 0.8622 - recall: 0.8804 - val_accuracy: 0.9091 - val_auc: 0.9704 - val_f1_score: 0.9232 - val_loss: 0.2619 - val_precision: 0.9451 - val_recall: 0.9062 - learning_rate: 0.0010\n",
      "Epoch 40/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 85ms/step - accuracy: 0.8290 - auc: 0.9080 - f1_score: 0.8582 - loss: 0.3723 - precision: 0.8524 - recall: 0.8695 - val_accuracy: 0.9077 - val_auc: 0.9703 - val_f1_score: 0.9242 - val_loss: 0.2651 - val_precision: 0.9198 - val_recall: 0.9305 - learning_rate: 0.0010\n",
      "Epoch 41/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - accuracy: 0.8363 - auc: 0.9106 - f1_score: 0.8628 - loss: 0.3725 - precision: 0.8622 - recall: 0.8679 - val_accuracy: 0.9006 - val_auc: 0.9600 - val_f1_score: 0.9175 - val_loss: 0.2898 - val_precision: 0.9090 - val_recall: 0.9282 - learning_rate: 0.0010\n",
      "Epoch 42/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - accuracy: 0.8316 - auc: 0.9093 - f1_score: 0.8625 - loss: 0.3710 - precision: 0.8556 - recall: 0.8726 - val_accuracy: 0.9112 - val_auc: 0.9685 - val_f1_score: 0.9266 - val_loss: 0.2578 - val_precision: 0.9273 - val_recall: 0.9284 - learning_rate: 0.0010\n",
      "Epoch 43/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - accuracy: 0.8324 - auc: 0.9083 - f1_score: 0.8638 - loss: 0.3723 - precision: 0.8575 - recall: 0.8734 - val_accuracy: 0.9006 - val_auc: 0.9629 - val_f1_score: 0.9140 - val_loss: 0.2817 - val_precision: 0.9274 - val_recall: 0.9079 - learning_rate: 0.0010\n",
      "Epoch 44/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 77ms/step - accuracy: 0.8248 - auc: 0.9045 - f1_score: 0.8557 - loss: 0.3807 - precision: 0.8497 - recall: 0.8658 - val_accuracy: 0.9183 - val_auc: 0.9744 - val_f1_score: 0.9304 - val_loss: 0.2508 - val_precision: 0.9461 - val_recall: 0.9212 - learning_rate: 0.0010\n",
      "Epoch 45/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 74ms/step - accuracy: 0.8365 - auc: 0.9111 - f1_score: 0.8654 - loss: 0.3684 - precision: 0.8651 - recall: 0.8675 - val_accuracy: 0.9119 - val_auc: 0.9716 - val_f1_score: 0.9286 - val_loss: 0.2538 - val_precision: 0.9085 - val_recall: 0.9526 - learning_rate: 0.0010\n",
      "Epoch 46/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.8354 - auc: 0.9140 - f1_score: 0.8625 - loss: 0.3620 - precision: 0.8599 - recall: 0.8693 - val_accuracy: 0.9027 - val_auc: 0.9688 - val_f1_score: 0.9162 - val_loss: 0.2726 - val_precision: 0.9530 - val_recall: 0.8862 - learning_rate: 0.0010\n",
      "Epoch 47/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 82ms/step - accuracy: 0.8393 - auc: 0.9144 - f1_score: 0.8663 - loss: 0.3624 - precision: 0.8687 - recall: 0.8691 - val_accuracy: 0.9148 - val_auc: 0.9725 - val_f1_score: 0.9281 - val_loss: 0.2487 - val_precision: 0.9396 - val_recall: 0.9200 - learning_rate: 0.0010\n",
      "Epoch 48/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 87ms/step - accuracy: 0.8442 - auc: 0.9141 - f1_score: 0.8726 - loss: 0.3625 - precision: 0.8680 - recall: 0.8786 - val_accuracy: 0.9276 - val_auc: 0.9794 - val_f1_score: 0.9401 - val_loss: 0.2293 - val_precision: 0.9545 - val_recall: 0.9286 - learning_rate: 0.0010\n",
      "Epoch 49/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 81ms/step - accuracy: 0.8393 - auc: 0.9177 - f1_score: 0.8678 - loss: 0.3533 - precision: 0.8710 - recall: 0.8655 - val_accuracy: 0.9268 - val_auc: 0.9797 - val_f1_score: 0.9379 - val_loss: 0.2396 - val_precision: 0.9528 - val_recall: 0.9277 - learning_rate: 0.0010\n",
      "Epoch 50/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 83ms/step - accuracy: 0.8374 - auc: 0.9174 - f1_score: 0.8649 - loss: 0.3551 - precision: 0.8675 - recall: 0.8654 - val_accuracy: 0.9368 - val_auc: 0.9775 - val_f1_score: 0.9478 - val_loss: 0.2334 - val_precision: 0.9482 - val_recall: 0.9493 - learning_rate: 0.0010\n",
      "Epoch 51/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 81ms/step - accuracy: 0.8466 - auc: 0.9221 - f1_score: 0.8737 - loss: 0.3438 - precision: 0.8772 - recall: 0.8730 - val_accuracy: 0.9261 - val_auc: 0.9768 - val_f1_score: 0.9384 - val_loss: 0.2429 - val_precision: 0.9562 - val_recall: 0.9234 - learning_rate: 0.0010\n",
      "Epoch 52/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 93ms/step - accuracy: 0.8441 - auc: 0.9184 - f1_score: 0.8721 - loss: 0.3522 - precision: 0.8703 - recall: 0.8762 - val_accuracy: 0.9254 - val_auc: 0.9797 - val_f1_score: 0.9369 - val_loss: 0.2269 - val_precision: 0.9439 - val_recall: 0.9341 - learning_rate: 0.0010\n",
      "Epoch 53/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 91ms/step - accuracy: 0.8354 - auc: 0.9135 - f1_score: 0.8633 - loss: 0.3636 - precision: 0.8660 - recall: 0.8632 - val_accuracy: 0.9205 - val_auc: 0.9756 - val_f1_score: 0.9358 - val_loss: 0.2436 - val_precision: 0.9273 - val_recall: 0.9426 - learning_rate: 0.0010\n",
      "Epoch 54/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 95ms/step - accuracy: 0.8383 - auc: 0.9208 - f1_score: 0.8678 - loss: 0.3477 - precision: 0.8629 - recall: 0.8758 - val_accuracy: 0.9219 - val_auc: 0.9768 - val_f1_score: 0.9335 - val_loss: 0.2330 - val_precision: 0.9293 - val_recall: 0.9424 - learning_rate: 0.0010\n",
      "Epoch 55/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 96ms/step - accuracy: 0.8541 - auc: 0.9266 - f1_score: 0.8795 - loss: 0.3353 - precision: 0.8801 - recall: 0.8832 - val_accuracy: 0.9041 - val_auc: 0.9679 - val_f1_score: 0.9198 - val_loss: 0.2548 - val_precision: 0.9569 - val_recall: 0.8861 - learning_rate: 0.0010\n",
      "Epoch 56/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 92ms/step - accuracy: 0.8348 - auc: 0.9164 - f1_score: 0.8654 - loss: 0.3564 - precision: 0.8691 - recall: 0.8641 - val_accuracy: 0.9169 - val_auc: 0.9748 - val_f1_score: 0.9282 - val_loss: 0.2461 - val_precision: 0.9507 - val_recall: 0.9123 - learning_rate: 0.0010\n",
      "Epoch 57/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 97ms/step - accuracy: 0.8442 - auc: 0.9214 - f1_score: 0.8700 - loss: 0.3489 - precision: 0.8668 - recall: 0.8768 - val_accuracy: 0.9226 - val_auc: 0.9791 - val_f1_score: 0.9373 - val_loss: 0.2223 - val_precision: 0.9186 - val_recall: 0.9598 - learning_rate: 0.0010\n",
      "Epoch 58/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 110ms/step - accuracy: 0.8499 - auc: 0.9232 - f1_score: 0.8769 - loss: 0.3431 - precision: 0.8677 - recall: 0.8898 - val_accuracy: 0.9297 - val_auc: 0.9802 - val_f1_score: 0.9422 - val_loss: 0.2228 - val_precision: 0.9535 - val_recall: 0.9330 - learning_rate: 0.0010\n",
      "Epoch 59/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 137ms/step - accuracy: 0.8475 - auc: 0.9230 - f1_score: 0.8752 - loss: 0.3424 - precision: 0.8718 - recall: 0.8808 - val_accuracy: 0.9318 - val_auc: 0.9788 - val_f1_score: 0.9415 - val_loss: 0.2260 - val_precision: 0.9578 - val_recall: 0.9317 - learning_rate: 0.0010\n",
      "Epoch 60/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 91ms/step - accuracy: 0.8491 - auc: 0.9209 - f1_score: 0.8764 - loss: 0.3468 - precision: 0.8766 - recall: 0.8770 - val_accuracy: 0.9254 - val_auc: 0.9814 - val_f1_score: 0.9347 - val_loss: 0.2356 - val_precision: 0.9673 - val_recall: 0.9068 - learning_rate: 0.0010\n",
      "Epoch 61/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 84ms/step - accuracy: 0.8508 - auc: 0.9251 - f1_score: 0.8775 - loss: 0.3376 - precision: 0.8770 - recall: 0.8799 - val_accuracy: 0.9332 - val_auc: 0.9818 - val_f1_score: 0.9436 - val_loss: 0.2221 - val_precision: 0.9555 - val_recall: 0.9358 - learning_rate: 0.0010\n",
      "Epoch 62/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 85ms/step - accuracy: 0.8428 - auc: 0.9229 - f1_score: 0.8707 - loss: 0.3413 - precision: 0.8735 - recall: 0.8723 - val_accuracy: 0.9283 - val_auc: 0.9825 - val_f1_score: 0.9417 - val_loss: 0.2112 - val_precision: 0.9553 - val_recall: 0.9280 - learning_rate: 0.0010\n",
      "Epoch 63/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 70ms/step - accuracy: 0.8415 - auc: 0.9245 - f1_score: 0.8692 - loss: 0.3401 - precision: 0.8741 - recall: 0.8661 - val_accuracy: 0.9212 - val_auc: 0.9775 - val_f1_score: 0.9347 - val_loss: 0.2264 - val_precision: 0.9618 - val_recall: 0.9107 - learning_rate: 0.0010\n",
      "Epoch 64/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 96ms/step - accuracy: 0.8577 - auc: 0.9267 - f1_score: 0.8818 - loss: 0.3340 - precision: 0.8836 - recall: 0.8837 - val_accuracy: 0.9297 - val_auc: 0.9795 - val_f1_score: 0.9401 - val_loss: 0.2284 - val_precision: 0.9495 - val_recall: 0.9352 - learning_rate: 0.0010\n",
      "Epoch 65/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 122ms/step - accuracy: 0.8751 - auc: 0.9372 - f1_score: 0.8965 - loss: 0.3126 - precision: 0.8895 - recall: 0.9062 - val_accuracy: 0.9347 - val_auc: 0.9813 - val_f1_score: 0.9410 - val_loss: 0.2174 - val_precision: 0.9473 - val_recall: 0.9428 - learning_rate: 0.0010\n",
      "Epoch 66/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 125ms/step - accuracy: 0.8622 - auc: 0.9298 - f1_score: 0.8881 - loss: 0.3281 - precision: 0.8810 - recall: 0.8978 - val_accuracy: 0.9382 - val_auc: 0.9848 - val_f1_score: 0.9462 - val_loss: 0.2103 - val_precision: 0.9565 - val_recall: 0.9395 - learning_rate: 0.0010\n",
      "Epoch 67/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 95ms/step - accuracy: 0.8653 - auc: 0.9337 - f1_score: 0.8914 - loss: 0.3172 - precision: 0.8956 - recall: 0.8905 - val_accuracy: 0.9290 - val_auc: 0.9790 - val_f1_score: 0.9399 - val_loss: 0.2197 - val_precision: 0.9523 - val_recall: 0.9302 - learning_rate: 0.0010\n",
      "Epoch 68/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 81ms/step - accuracy: 0.8643 - auc: 0.9296 - f1_score: 0.8885 - loss: 0.3278 - precision: 0.8846 - recall: 0.8963 - val_accuracy: 0.9318 - val_auc: 0.9788 - val_f1_score: 0.9432 - val_loss: 0.2193 - val_precision: 0.9476 - val_recall: 0.9410 - learning_rate: 0.0010\n",
      "Epoch 69/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 80ms/step - accuracy: 0.8554 - auc: 0.9296 - f1_score: 0.8812 - loss: 0.3306 - precision: 0.8740 - recall: 0.8900 - val_accuracy: 0.9325 - val_auc: 0.9831 - val_f1_score: 0.9444 - val_loss: 0.2233 - val_precision: 0.9468 - val_recall: 0.9435 - learning_rate: 0.0010\n",
      "Epoch 70/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 80ms/step - accuracy: 0.8585 - auc: 0.9316 - f1_score: 0.8836 - loss: 0.3244 - precision: 0.8855 - recall: 0.8838 - val_accuracy: 0.9411 - val_auc: 0.9869 - val_f1_score: 0.9505 - val_loss: 0.2066 - val_precision: 0.9603 - val_recall: 0.9411 - learning_rate: 0.0010\n",
      "Epoch 71/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - accuracy: 0.8598 - auc: 0.9307 - f1_score: 0.8855 - loss: 0.3261 - precision: 0.8861 - recall: 0.8868 - val_accuracy: 0.9418 - val_auc: 0.9863 - val_f1_score: 0.9503 - val_loss: 0.2022 - val_precision: 0.9591 - val_recall: 0.9432 - learning_rate: 0.0010\n",
      "Epoch 72/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - accuracy: 0.8555 - auc: 0.9292 - f1_score: 0.8810 - loss: 0.3299 - precision: 0.8795 - recall: 0.8865 - val_accuracy: 0.9425 - val_auc: 0.9877 - val_f1_score: 0.9511 - val_loss: 0.2032 - val_precision: 0.9503 - val_recall: 0.9559 - learning_rate: 0.0010\n",
      "Epoch 73/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 86ms/step - accuracy: 0.8514 - auc: 0.9273 - f1_score: 0.8772 - loss: 0.3328 - precision: 0.8767 - recall: 0.8809 - val_accuracy: 0.9510 - val_auc: 0.9893 - val_f1_score: 0.9594 - val_loss: 0.1875 - val_precision: 0.9553 - val_recall: 0.9672 - learning_rate: 0.0010\n",
      "Epoch 74/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 85ms/step - accuracy: 0.8537 - auc: 0.9265 - f1_score: 0.8779 - loss: 0.3380 - precision: 0.8767 - recall: 0.8822 - val_accuracy: 0.9418 - val_auc: 0.9855 - val_f1_score: 0.9498 - val_loss: 0.1954 - val_precision: 0.9589 - val_recall: 0.9455 - learning_rate: 0.0010\n",
      "Epoch 75/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 83ms/step - accuracy: 0.8573 - auc: 0.9321 - f1_score: 0.8805 - loss: 0.3241 - precision: 0.8860 - recall: 0.8788 - val_accuracy: 0.9439 - val_auc: 0.9846 - val_f1_score: 0.9547 - val_loss: 0.1971 - val_precision: 0.9615 - val_recall: 0.9472 - learning_rate: 0.0010\n",
      "Epoch 76/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 88ms/step - accuracy: 0.8601 - auc: 0.9301 - f1_score: 0.8844 - loss: 0.3282 - precision: 0.8826 - recall: 0.8888 - val_accuracy: 0.9524 - val_auc: 0.9889 - val_f1_score: 0.9594 - val_loss: 0.1899 - val_precision: 0.9670 - val_recall: 0.9546 - learning_rate: 0.0010\n",
      "Epoch 77/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 85ms/step - accuracy: 0.8653 - auc: 0.9335 - f1_score: 0.8893 - loss: 0.3211 - precision: 0.8927 - recall: 0.8867 - val_accuracy: 0.9432 - val_auc: 0.9871 - val_f1_score: 0.9517 - val_loss: 0.1931 - val_precision: 0.9669 - val_recall: 0.9402 - learning_rate: 0.0010\n",
      "Epoch 78/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8601 - auc: 0.9342 - f1_score: 0.8829 - loss: 0.3187 - precision: 0.8866 - recall: 0.8831\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 96ms/step - accuracy: 0.8601 - auc: 0.9342 - f1_score: 0.8829 - loss: 0.3187 - precision: 0.8866 - recall: 0.8831 - val_accuracy: 0.9460 - val_auc: 0.9889 - val_f1_score: 0.9556 - val_loss: 0.1895 - val_precision: 0.9659 - val_recall: 0.9459 - learning_rate: 0.0010\n",
      "Training time: 1466.18 seconds\n",
      "Evaluating MobileNetV3Small on The Wildfire Dataset_DeepFire...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - accuracy: 0.6702 - auc: 0.5742 - f1_score: 0.3328 - loss: 0.8040 - precision: 0.4516 - recall: 0.6079        \n",
      "Training results:\n",
      "{'evaluation': {'accuracy': 0.671875,\n",
      "                'auc': 0.758423924446106,\n",
      "                'f1_score': 0.505158007144928,\n",
      "                'loss': 0.7791340947151184,\n",
      "                'precision': 0.6480000019073486,\n",
      "                'recall': 0.8100000023841858},\n",
      " 'history': {'accuracy': [0.7074158191680908,\n",
      "                          0.7414184212684631,\n",
      "                          0.7582577466964722,\n",
      "                          0.7546955943107605,\n",
      "                          0.7619818449020386,\n",
      "                          0.7802785038948059,\n",
      "                          0.7833549380302429,\n",
      "                          0.7773640155792236,\n",
      "                          0.7867552042007446,\n",
      "                          0.7793070077896118,\n",
      "                          0.7895077466964722,\n",
      "                          0.796955943107605,\n",
      "                          0.7857837080955505,\n",
      "                          0.796794056892395,\n",
      "                          0.7963082790374756,\n",
      "                          0.8042422533035278,\n",
      "                          0.7940414547920227,\n",
      "                          0.8044041395187378,\n",
      "                          0.8084520697593689,\n",
      "                          0.8037564754486084,\n",
      "                          0.8084520697593689,\n",
      "                          0.8045660853385925,\n",
      "                          0.8147668242454529,\n",
      "                          0.8116903901100159,\n",
      "                          0.8168717622756958,\n",
      "                          0.8144429922103882,\n",
      "                          0.809423565864563,\n",
      "                          0.8131476640701294,\n",
      "                          0.8225388526916504,\n",
      "                          0.8196243643760681,\n",
      "                          0.8236722946166992,\n",
      "                          0.818814754486084,\n",
      "                          0.8314443230628967,\n",
      "                          0.8277202248573303,\n",
      "                          0.825129508972168,\n",
      "                          0.8246437907218933,\n",
      "                          0.8194624185562134,\n",
      "                          0.824967622756958,\n",
      "                          0.8335492014884949,\n",
      "                          0.8272344470024109,\n",
      "                          0.8358160853385925,\n",
      "                          0.8317681550979614,\n",
      "                          0.834358811378479,\n",
      "                          0.8327396512031555,\n",
      "                          0.8403497338294983,\n",
      "                          0.8319300413131714,\n",
      "                          0.8395401835441589,\n",
      "                          0.8387305736541748,\n",
      "                          0.8338730335235596,\n",
      "                          0.8419688940048218,\n",
      "                          0.8408355116844177,\n",
      "                          0.8435880541801453,\n",
      "                          0.8409973978996277,\n",
      "                          0.8390544056892395,\n",
      "                          0.853141188621521,\n",
      "                          0.8350064754486084,\n",
      "                          0.8429403901100159,\n",
      "                          0.8511981964111328,\n",
      "                          0.8452072739601135,\n",
      "                          0.8431023359298706,\n",
      "                          0.8515220284461975,\n",
      "                          0.8456929922103882,\n",
      "                          0.8434261679649353,\n",
      "                          0.8489313721656799,\n",
      "                          0.8604274392127991,\n",
      "                          0.8579987287521362,\n",
      "                          0.862370491027832,\n",
      "                          0.8639896512031555,\n",
      "                          0.8528173565864563,\n",
      "                          0.856379508972168,\n",
      "                          0.8579987287521362,\n",
      "                          0.8533031344413757,\n",
      "                          0.8542746305465698,\n",
      "                          0.853141188621521,\n",
      "                          0.853141188621521,\n",
      "                          0.8578367829322815,\n",
      "                          0.8601036071777344,\n",
      "                          0.8589702248573303],\n",
      "             'auc': [0.7653453350067139,\n",
      "                     0.8038990497589111,\n",
      "                     0.8247467279434204,\n",
      "                     0.8277963995933533,\n",
      "                     0.8336671590805054,\n",
      "                     0.8541408181190491,\n",
      "                     0.8565613031387329,\n",
      "                     0.8555470108985901,\n",
      "                     0.8582749366760254,\n",
      "                     0.856164813041687,\n",
      "                     0.8630307912826538,\n",
      "                     0.8685316443443298,\n",
      "                     0.8686040043830872,\n",
      "                     0.8771171569824219,\n",
      "                     0.8699941635131836,\n",
      "                     0.8809420466423035,\n",
      "                     0.8746581077575684,\n",
      "                     0.8819913864135742,\n",
      "                     0.8857273459434509,\n",
      "                     0.8851615190505981,\n",
      "                     0.8896880149841309,\n",
      "                     0.88311368227005,\n",
      "                     0.890608012676239,\n",
      "                     0.8874642252922058,\n",
      "                     0.8910191059112549,\n",
      "                     0.8896681070327759,\n",
      "                     0.8899751305580139,\n",
      "                     0.8919808268547058,\n",
      "                     0.8965076804161072,\n",
      "                     0.8977101445198059,\n",
      "                     0.8991062045097351,\n",
      "                     0.8949769139289856,\n",
      "                     0.9071088433265686,\n",
      "                     0.9009681344032288,\n",
      "                     0.9018253684043884,\n",
      "                     0.9025171399116516,\n",
      "                     0.9023142457008362,\n",
      "                     0.9054781794548035,\n",
      "                     0.9087767004966736,\n",
      "                     0.9038657546043396,\n",
      "                     0.9096053838729858,\n",
      "                     0.9072250723838806,\n",
      "                     0.9083108901977539,\n",
      "                     0.9101864695549011,\n",
      "                     0.9127368927001953,\n",
      "                     0.9061558246612549,\n",
      "                     0.913560688495636,\n",
      "                     0.9133837223052979,\n",
      "                     0.9126567840576172,\n",
      "                     0.9185587763786316,\n",
      "                     0.917733371257782,\n",
      "                     0.9173871278762817,\n",
      "                     0.9187619090080261,\n",
      "                     0.9205335974693298,\n",
      "                     0.9250052571296692,\n",
      "                     0.9156070351600647,\n",
      "                     0.9200440645217896,\n",
      "                     0.9243177771568298,\n",
      "                     0.9223424196243286,\n",
      "                     0.9187104105949402,\n",
      "                     0.9242467284202576,\n",
      "                     0.9230958819389343,\n",
      "                     0.9250845909118652,\n",
      "                     0.9220589995384216,\n",
      "                     0.9290078282356262,\n",
      "                     0.927769660949707,\n",
      "                     0.9292894601821899,\n",
      "                     0.9328523874282837,\n",
      "                     0.9252223968505859,\n",
      "                     0.9300655126571655,\n",
      "                     0.9306473731994629,\n",
      "                     0.9266340136528015,\n",
      "                     0.9293053150177002,\n",
      "                     0.9290308952331543,\n",
      "                     0.9298115372657776,\n",
      "                     0.9298264384269714,\n",
      "                     0.9328879714012146,\n",
      "                     0.9332594871520996],\n",
      "             'f1_score': [0.7538596987724304,\n",
      "                          0.7893948554992676,\n",
      "                          0.8043860197067261,\n",
      "                          0.8008970022201538,\n",
      "                          0.8085823655128479,\n",
      "                          0.8210585713386536,\n",
      "                          0.825208306312561,\n",
      "                          0.818595826625824,\n",
      "                          0.824942946434021,\n",
      "                          0.821868896484375,\n",
      "                          0.8284432291984558,\n",
      "                          0.8355116844177246,\n",
      "                          0.8243650197982788,\n",
      "                          0.8356849551200867,\n",
      "                          0.8356891870498657,\n",
      "                          0.8424069285392761,\n",
      "                          0.8326065540313721,\n",
      "                          0.8392782807350159,\n",
      "                          0.8440021872520447,\n",
      "                          0.8400675058364868,\n",
      "                          0.8438137769699097,\n",
      "                          0.8416251540184021,\n",
      "                          0.8480440378189087,\n",
      "                          0.8460504412651062,\n",
      "                          0.8501919507980347,\n",
      "                          0.8493572473526001,\n",
      "                          0.8445482850074768,\n",
      "                          0.8464086651802063,\n",
      "                          0.855277419090271,\n",
      "                          0.8524114489555359,\n",
      "                          0.8555391430854797,\n",
      "                          0.8504531979560852,\n",
      "                          0.8604410886764526,\n",
      "                          0.8580026030540466,\n",
      "                          0.8555522561073303,\n",
      "                          0.8554130792617798,\n",
      "                          0.8510112762451172,\n",
      "                          0.8568771481513977,\n",
      "                          0.8643293380737305,\n",
      "                          0.856508195400238,\n",
      "                          0.8643113970756531,\n",
      "                          0.8628501296043396,\n",
      "                          0.8645373582839966,\n",
      "                          0.8624582290649414,\n",
      "                          0.8690124750137329,\n",
      "                          0.8608865737915039,\n",
      "                          0.8680834770202637,\n",
      "                          0.8674759864807129,\n",
      "                          0.86310875415802,\n",
      "                          0.8692070245742798,\n",
      "                          0.8690870404243469,\n",
      "                          0.8713929057121277,\n",
      "                          0.8684796690940857,\n",
      "                          0.8680683374404907,\n",
      "                          0.8788473606109619,\n",
      "                          0.8648344278335571,\n",
      "                          0.8703777194023132,\n",
      "                          0.8778368830680847,\n",
      "                          0.8726063966751099,\n",
      "                          0.8716520667076111,\n",
      "                          0.8781948089599609,\n",
      "                          0.8733448386192322,\n",
      "                          0.8707569241523743,\n",
      "                          0.8750482797622681,\n",
      "                          0.8847421407699585,\n",
      "                          0.8842651844024658,\n",
      "                          0.8878227472305298,\n",
      "                          0.8885168433189392,\n",
      "                          0.8790442943572998,\n",
      "                          0.8818901777267456,\n",
      "                          0.884326696395874,\n",
      "                          0.8795309662818909,\n",
      "                          0.8802739977836609,\n",
      "                          0.8780665397644043,\n",
      "                          0.8784885406494141,\n",
      "                          0.8833752274513245,\n",
      "                          0.885755717754364,\n",
      "                          0.8831439018249512],\n",
      "             'learning_rate': [0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513],\n",
      "             'loss': [0.6228840947151184,\n",
      "                      0.5451917052268982,\n",
      "                      0.5075544714927673,\n",
      "                      0.5017956495285034,\n",
      "                      0.4886639714241028,\n",
      "                      0.4614076614379883,\n",
      "                      0.45622411370277405,\n",
      "                      0.46070030331611633,\n",
      "                      0.45723992586135864,\n",
      "                      0.4555899202823639,\n",
      "                      0.44806087017059326,\n",
      "                      0.4393284022808075,\n",
      "                      0.4409143328666687,\n",
      "                      0.42462655901908875,\n",
      "                      0.4347968101501465,\n",
      "                      0.41955170035362244,\n",
      "                      0.4300972819328308,\n",
      "                      0.4191920757293701,\n",
      "                      0.4134368598461151,\n",
      "                      0.4125555455684662,\n",
      "                      0.4075043797492981,\n",
      "                      0.4162246584892273,\n",
      "                      0.40527287125587463,\n",
      "                      0.40916675329208374,\n",
      "                      0.4049271047115326,\n",
      "                      0.4062102437019348,\n",
      "                      0.4058404564857483,\n",
      "                      0.4038129150867462,\n",
      "                      0.39411917328834534,\n",
      "                      0.3904815912246704,\n",
      "                      0.3904602825641632,\n",
      "                      0.39679864048957825,\n",
      "                      0.37640488147735596,\n",
      "                      0.3871566653251648,\n",
      "                      0.3853842318058014,\n",
      "                      0.384676992893219,\n",
      "                      0.3836546540260315,\n",
      "                      0.3773985207080841,\n",
      "                      0.3715589940547943,\n",
      "                      0.380521297454834,\n",
      "                      0.37138739228248596,\n",
      "                      0.3743661642074585,\n",
      "                      0.37326425313949585,\n",
      "                      0.36932337284088135,\n",
      "                      0.36335042119026184,\n",
      "                      0.3775542378425598,\n",
      "                      0.3639364540576935,\n",
      "                      0.3627777099609375,\n",
      "                      0.36351731419563293,\n",
      "                      0.35297518968582153,\n",
      "                      0.3525565266609192,\n",
      "                      0.3551134765148163,\n",
      "                      0.35102710127830505,\n",
      "                      0.34750092029571533,\n",
      "                      0.3386111855506897,\n",
      "                      0.35824257135391235,\n",
      "                      0.35049450397491455,\n",
      "                      0.34068337082862854,\n",
      "                      0.343991756439209,\n",
      "                      0.3509729504585266,\n",
      "                      0.338692843914032,\n",
      "                      0.3428879976272583,\n",
      "                      0.3377203345298767,\n",
      "                      0.3448748290538788,\n",
      "                      0.32978585362434387,\n",
      "                      0.3315143585205078,\n",
      "                      0.3291425108909607,\n",
      "                      0.31981316208839417,\n",
      "                      0.3396027982234955,\n",
      "                      0.32781657576560974,\n",
      "                      0.3267167806625366,\n",
      "                      0.33401429653167725,\n",
      "                      0.3284452557563782,\n",
      "                      0.3311297297477722,\n",
      "                      0.3284294307231903,\n",
      "                      0.3289870321750641,\n",
      "                      0.3213951289653778,\n",
      "                      0.3199547827243805],\n",
      "             'precision': [0.771774172782898,\n",
      "                           0.7792607545852661,\n",
      "                           0.7921679019927979,\n",
      "                           0.7908998131752014,\n",
      "                           0.7947487831115723,\n",
      "                           0.8113110661506653,\n",
      "                           0.8150421977043152,\n",
      "                           0.8085161447525024,\n",
      "                           0.8177340030670166,\n",
      "                           0.8118457794189453,\n",
      "                           0.8204533457756042,\n",
      "                           0.8287477493286133,\n",
      "                           0.81565260887146,\n",
      "                           0.8247633576393127,\n",
      "                           0.8286006450653076,\n",
      "                           0.8329923152923584,\n",
      "                           0.8256024718284607,\n",
      "                           0.8368627429008484,\n",
      "                           0.8413361310958862,\n",
      "                           0.8333333134651184,\n",
      "                           0.8415015935897827,\n",
      "                           0.8329495787620544,\n",
      "                           0.8433923125267029,\n",
      "                           0.8408089280128479,\n",
      "                           0.8442198038101196,\n",
      "                           0.8401544690132141,\n",
      "                           0.8392810821533203,\n",
      "                           0.8420639038085938,\n",
      "                           0.8515135645866394,\n",
      "                           0.8509024381637573,\n",
      "                           0.8514748215675354,\n",
      "                           0.8483896255493164,\n",
      "                           0.8601895570755005,\n",
      "                           0.8558018207550049,\n",
      "                           0.851280689239502,\n",
      "                           0.8552215099334717,\n",
      "                           0.8515872955322266,\n",
      "                           0.8569550514221191,\n",
      "                           0.8601453304290771,\n",
      "                           0.8547794222831726,\n",
      "                           0.8657468557357788,\n",
      "                           0.8567364811897278,\n",
      "                           0.8591074347496033,\n",
      "                           0.8613445162773132,\n",
      "                           0.8677359819412231,\n",
      "                           0.8590956926345825,\n",
      "                           0.8653092980384827,\n",
      "                           0.8654602766036987,\n",
      "                           0.863010048866272,\n",
      "                           0.8697378635406494,\n",
      "                           0.8693361282348633,\n",
      "                           0.8689727187156677,\n",
      "                           0.8701298832893372,\n",
      "                           0.8634705543518066,\n",
      "                           0.8781192302703857,\n",
      "                           0.8630890250205994,\n",
      "                           0.8719915151596069,\n",
      "                           0.8733647465705872,\n",
      "                           0.8691883087158203,\n",
      "                           0.8682901263237,\n",
      "                           0.8736320734024048,\n",
      "                           0.8686816096305847,\n",
      "                           0.8740010857582092,\n",
      "                           0.8759239912033081,\n",
      "                           0.8835436105728149,\n",
      "                           0.8773877024650574,\n",
      "                           0.8867826461791992,\n",
      "                           0.8883647918701172,\n",
      "                           0.8750655055046082,\n",
      "                           0.8820269107818604,\n",
      "                           0.880124568939209,\n",
      "                           0.8780614733695984,\n",
      "                           0.8787640929222107,\n",
      "                           0.878360390663147,\n",
      "                           0.8792921304702759,\n",
      "                           0.8801987767219543,\n",
      "                           0.883818507194519,\n",
      "                           0.8839356303215027],\n",
      "             'recall': [0.7391864061355591,\n",
      "                        0.8046647310256958,\n",
      "                        0.819650411605835,\n",
      "                        0.8161435127258301,\n",
      "                        0.8273324370384216,\n",
      "                        0.8351415991783142,\n",
      "                        0.8382004499435425,\n",
      "                        0.8319171667098999,\n",
      "                        0.8370488286018372,\n",
      "                        0.8355228304862976,\n",
      "                        0.8408131003379822,\n",
      "                        0.8457098007202148,\n",
      "                        0.8363103270530701,\n",
      "                        0.8497627973556519,\n",
      "                        0.8463548421859741,\n",
      "                        0.8541830778121948,\n",
      "                        0.8417437076568604,\n",
      "                        0.8457067608833313,\n",
      "                        0.8486443758010864,\n",
      "                        0.8504746556282043,\n",
      "                        0.8488035798072815,\n",
      "                        0.8545931577682495,\n",
      "                        0.8567653298377991,\n",
      "                        0.8552215099334717,\n",
      "                        0.8600475192070007,\n",
      "                        0.8614410161972046,\n",
      "                        0.8519302010536194,\n",
      "                        0.8536909222602844,\n",
      "                        0.8609498739242554,\n",
      "                        0.8567289710044861,\n",
      "                        0.862506628036499,\n",
      "                        0.8571428656578064,\n",
      "                        0.8649721741676331,\n",
      "                        0.8652781248092651,\n",
      "                        0.8643842935562134,\n",
      "                        0.8586179614067078,\n",
      "                        0.8531672358512878,\n",
      "                        0.8585352897644043,\n",
      "                        0.8714510798454285,\n",
      "                        0.8636242747306824,\n",
      "                        0.865287721157074,\n",
      "                        0.8723012208938599,\n",
      "                        0.8733843564987183,\n",
      "                        0.8665785789489746,\n",
      "                        0.8723235726356506,\n",
      "                        0.8668435215950012,\n",
      "                        0.8746702075004578,\n",
      "                        0.8723235726356506,\n",
      "                        0.8648433089256287,\n",
      "                        0.8715839982032776,\n",
      "                        0.8714021444320679,\n",
      "                        0.8767847418785095,\n",
      "                        0.8696689009666443,\n",
      "                        0.8757928013801575,\n",
      "                        0.8829899430274963,\n",
      "                        0.8692327737808228,\n",
      "                        0.8715305328369141,\n",
      "                        0.8847071528434753,\n",
      "                        0.8784178495407104,\n",
      "                        0.8767847418785095,\n",
      "                        0.8858652710914612,\n",
      "                        0.8813111186027527,\n",
      "                        0.8691390752792358,\n",
      "                        0.8775455951690674,\n",
      "                        0.8891534209251404,\n",
      "                        0.8942383527755737,\n",
      "                        0.8918777704238892,\n",
      "                        0.891167163848877,\n",
      "                        0.885502278804779,\n",
      "                        0.8836594223976135,\n",
      "                        0.8909902572631836,\n",
      "                        0.8849790096282959,\n",
      "                        0.8847877383232117,\n",
      "                        0.8799999952316284,\n",
      "                        0.8809208869934082,\n",
      "                        0.8890356421470642,\n",
      "                        0.8889474868774414,\n",
      "                        0.886039137840271],\n",
      "             'val_accuracy': [0.6235795617103577,\n",
      "                              0.6370738744735718,\n",
      "                              0.7677556872367859,\n",
      "                              0.8046875,\n",
      "                              0.796875,\n",
      "                              0.8210227489471436,\n",
      "                              0.8380681872367859,\n",
      "                              0.8274147510528564,\n",
      "                              0.8196022510528564,\n",
      "                              0.84375,\n",
      "                              0.8153409361839294,\n",
      "                              0.84375,\n",
      "                              0.8423295617103577,\n",
      "                              0.8508522510528564,\n",
      "                              0.8480113744735718,\n",
      "                              0.8558238744735718,\n",
      "                              0.8558238744735718,\n",
      "                              0.8558238744735718,\n",
      "                              0.8522727489471436,\n",
      "                              0.8572443127632141,\n",
      "                              0.8650568127632141,\n",
      "                              0.8721590638160706,\n",
      "                              0.8536931872367859,\n",
      "                              0.8828125,\n",
      "                              0.8657670617103577,\n",
      "                              0.8757102489471436,\n",
      "                              0.8764204382896423,\n",
      "                              0.8785511255264282,\n",
      "                              0.8828125,\n",
      "                              0.8913352489471436,\n",
      "                              0.8821022510528564,\n",
      "                              0.8877840638160706,\n",
      "                              0.8870738744735718,\n",
      "                              0.8934659361839294,\n",
      "                              0.8813920617103577,\n",
      "                              0.8977272510528564,\n",
      "                              0.8991477489471436,\n",
      "                              0.8920454382896423,\n",
      "                              0.9090909361839294,\n",
      "                              0.9076704382896423,\n",
      "                              0.9005681872367859,\n",
      "                              0.9112215638160706,\n",
      "                              0.9005681872367859,\n",
      "                              0.9183238744735718,\n",
      "                              0.9119318127632141,\n",
      "                              0.9026988744735718,\n",
      "                              0.9147727489471436,\n",
      "                              0.9275568127632141,\n",
      "                              0.9268465638160706,\n",
      "                              0.9367897510528564,\n",
      "                              0.9261363744735718,\n",
      "                              0.9254261255264282,\n",
      "                              0.9204545617103577,\n",
      "                              0.921875,\n",
      "                              0.9041193127632141,\n",
      "                              0.9169034361839294,\n",
      "                              0.9225852489471436,\n",
      "                              0.9296875,\n",
      "                              0.9318181872367859,\n",
      "                              0.9254261255264282,\n",
      "                              0.9332386255264282,\n",
      "                              0.9282670617103577,\n",
      "                              0.9211647510528564,\n",
      "                              0.9296875,\n",
      "                              0.9346590638160706,\n",
      "                              0.9382102489471436,\n",
      "                              0.9289772510528564,\n",
      "                              0.9318181872367859,\n",
      "                              0.9325284361839294,\n",
      "                              0.9410511255264282,\n",
      "                              0.9417613744735718,\n",
      "                              0.9424715638160706,\n",
      "                              0.9509943127632141,\n",
      "                              0.9417613744735718,\n",
      "                              0.9438920617103577,\n",
      "                              0.9524147510528564,\n",
      "                              0.9431818127632141,\n",
      "                              0.9460227489471436],\n",
      "             'val_auc': [0.8446329236030579,\n",
      "                         0.8510372638702393,\n",
      "                         0.8745262026786804,\n",
      "                         0.8886070847511292,\n",
      "                         0.8819136023521423,\n",
      "                         0.9028028845787048,\n",
      "                         0.9094752073287964,\n",
      "                         0.9121464490890503,\n",
      "                         0.9043467044830322,\n",
      "                         0.9218556880950928,\n",
      "                         0.9014139175415039,\n",
      "                         0.927405059337616,\n",
      "                         0.924022912979126,\n",
      "                         0.9235569834709167,\n",
      "                         0.92812180519104,\n",
      "                         0.9282544851303101,\n",
      "                         0.9319013953208923,\n",
      "                         0.9365249872207642,\n",
      "                         0.9260124564170837,\n",
      "                         0.9319049715995789,\n",
      "                         0.9351876974105835,\n",
      "                         0.9420451521873474,\n",
      "                         0.9337754249572754,\n",
      "                         0.949461042881012,\n",
      "                         0.9460015892982483,\n",
      "                         0.9455378651618958,\n",
      "                         0.9512672424316406,\n",
      "                         0.9538754224777222,\n",
      "                         0.9535338282585144,\n",
      "                         0.9566510319709778,\n",
      "                         0.9547271132469177,\n",
      "                         0.9573197364807129,\n",
      "                         0.9570838212966919,\n",
      "                         0.962617039680481,\n",
      "                         0.9504367113113403,\n",
      "                         0.9587273597717285,\n",
      "                         0.9608324766159058,\n",
      "                         0.9615582823753357,\n",
      "                         0.9703942537307739,\n",
      "                         0.9702531099319458,\n",
      "                         0.9599767923355103,\n",
      "                         0.9684610366821289,\n",
      "                         0.9629084467887878,\n",
      "                         0.9744384288787842,\n",
      "                         0.9715666770935059,\n",
      "                         0.9688480496406555,\n",
      "                         0.9724584817886353,\n",
      "                         0.9793698191642761,\n",
      "                         0.9796686768531799,\n",
      "                         0.9775227904319763,\n",
      "                         0.9768050909042358,\n",
      "                         0.9797134399414062,\n",
      "                         0.9756259918212891,\n",
      "                         0.9768294095993042,\n",
      "                         0.9678997993469238,\n",
      "                         0.9747530221939087,\n",
      "                         0.9790592789649963,\n",
      "                         0.980190098285675,\n",
      "                         0.9787822365760803,\n",
      "                         0.9813963174819946,\n",
      "                         0.9818107485771179,\n",
      "                         0.982502281665802,\n",
      "                         0.977501630783081,\n",
      "                         0.9795262813568115,\n",
      "                         0.9813328981399536,\n",
      "                         0.9847898483276367,\n",
      "                         0.9790443181991577,\n",
      "                         0.9787575006484985,\n",
      "                         0.9830784797668457,\n",
      "                         0.9869319200515747,\n",
      "                         0.9862549901008606,\n",
      "                         0.9877092838287354,\n",
      "                         0.9892641305923462,\n",
      "                         0.9854997396469116,\n",
      "                         0.9845786094665527,\n",
      "                         0.9888854622840881,\n",
      "                         0.98708176612854,\n",
      "                         0.9889259338378906],\n",
      "             'val_f1_score': [0.765965461730957,\n",
      "                              0.7688799500465393,\n",
      "                              0.8301441073417664,\n",
      "                              0.8269811868667603,\n",
      "                              0.8231523036956787,\n",
      "                              0.8512950539588928,\n",
      "                              0.8667593598365784,\n",
      "                              0.8506966233253479,\n",
      "                              0.843191921710968,\n",
      "                              0.8650926351547241,\n",
      "                              0.8348196148872375,\n",
      "                              0.8731801509857178,\n",
      "                              0.8670762777328491,\n",
      "                              0.8693675398826599,\n",
      "                              0.8698845505714417,\n",
      "                              0.8787972927093506,\n",
      "                              0.8807096481323242,\n",
      "                              0.8760892748832703,\n",
      "                              0.8743808269500732,\n",
      "                              0.8747463226318359,\n",
      "                              0.8919111490249634,\n",
      "                              0.8906183242797852,\n",
      "                              0.8843316435813904,\n",
      "                              0.9031524062156677,\n",
      "                              0.8846229910850525,\n",
      "                              0.8960185647010803,\n",
      "                              0.8969987630844116,\n",
      "                              0.8953744769096375,\n",
      "                              0.8980311751365662,\n",
      "                              0.9101293087005615,\n",
      "                              0.8983402252197266,\n",
      "                              0.9054763913154602,\n",
      "                              0.9025636911392212,\n",
      "                              0.9086924195289612,\n",
      "                              0.9042277932167053,\n",
      "                              0.9140329360961914,\n",
      "                              0.9149317145347595,\n",
      "                              0.9052475094795227,\n",
      "                              0.9232078790664673,\n",
      "                              0.9242490530014038,\n",
      "                              0.9175063967704773,\n",
      "                              0.9266135692596436,\n",
      "                              0.9140375256538391,\n",
      "                              0.9304375052452087,\n",
      "                              0.9286439418792725,\n",
      "                              0.9161891937255859,\n",
      "                              0.9280564785003662,\n",
      "                              0.9401033520698547,\n",
      "                              0.9378553032875061,\n",
      "                              0.947770893573761,\n",
      "                              0.9383537769317627,\n",
      "                              0.9368847012519836,\n",
      "                              0.9357901215553284,\n",
      "                              0.9334930777549744,\n",
      "                              0.9198011755943298,\n",
      "                              0.9281912446022034,\n",
      "                              0.937269926071167,\n",
      "                              0.9421654343605042,\n",
      "                              0.9414991736412048,\n",
      "                              0.9346654415130615,\n",
      "                              0.9436452984809875,\n",
      "                              0.9416658282279968,\n",
      "                              0.9347156882286072,\n",
      "                              0.9401085376739502,\n",
      "                              0.9410070180892944,\n",
      "                              0.9461507797241211,\n",
      "                              0.9398935437202454,\n",
      "                              0.9432281851768494,\n",
      "                              0.9444417357444763,\n",
      "                              0.9505319595336914,\n",
      "                              0.9502865076065063,\n",
      "                              0.9511216878890991,\n",
      "                              0.9593576192855835,\n",
      "                              0.9498159289360046,\n",
      "                              0.9547234177589417,\n",
      "                              0.9594361782073975,\n",
      "                              0.9517107009887695,\n",
      "                              0.9556129574775696],\n",
      "             'val_loss': [0.6446933150291443,\n",
      "                          0.5915746688842773,\n",
      "                          0.4945172369480133,\n",
      "                          0.43723413348197937,\n",
      "                          0.4396815001964569,\n",
      "                          0.40113210678100586,\n",
      "                          0.3876156806945801,\n",
      "                          0.39274653792381287,\n",
      "                          0.39795681834220886,\n",
      "                          0.37606093287467957,\n",
      "                          0.4058941900730133,\n",
      "                          0.3559283912181854,\n",
      "                          0.36219674348831177,\n",
      "                          0.36229994893074036,\n",
      "                          0.3581908047199249,\n",
      "                          0.34776046872138977,\n",
      "                          0.34818553924560547,\n",
      "                          0.3411465287208557,\n",
      "                          0.3472253382205963,\n",
      "                          0.34922197461128235,\n",
      "                          0.3324301838874817,\n",
      "                          0.3308427333831787,\n",
      "                          0.3367294669151306,\n",
      "                          0.31635865569114685,\n",
      "                          0.3272276818752289,\n",
      "                          0.3152012526988983,\n",
      "                          0.30389803647994995,\n",
      "                          0.3066772222518921,\n",
      "                          0.3032548725605011,\n",
      "                          0.295264333486557,\n",
      "                          0.2932981848716736,\n",
      "                          0.30115988850593567,\n",
      "                          0.29095813632011414,\n",
      "                          0.28743717074394226,\n",
      "                          0.29848530888557434,\n",
      "                          0.27873578667640686,\n",
      "                          0.2810177505016327,\n",
      "                          0.2807823419570923,\n",
      "                          0.2619381844997406,\n",
      "                          0.26506999135017395,\n",
      "                          0.28983333706855774,\n",
      "                          0.257750004529953,\n",
      "                          0.2817375957965851,\n",
      "                          0.250757098197937,\n",
      "                          0.2537795603275299,\n",
      "                          0.2725648880004883,\n",
      "                          0.248683363199234,\n",
      "                          0.22933463752269745,\n",
      "                          0.23959492146968842,\n",
      "                          0.23337869346141815,\n",
      "                          0.24293743073940277,\n",
      "                          0.22688423097133636,\n",
      "                          0.24363140761852264,\n",
      "                          0.2329997569322586,\n",
      "                          0.25482094287872314,\n",
      "                          0.2460576593875885,\n",
      "                          0.22228555381298065,\n",
      "                          0.22277630865573883,\n",
      "                          0.2259744554758072,\n",
      "                          0.23555536568164825,\n",
      "                          0.22212094068527222,\n",
      "                          0.2112138420343399,\n",
      "                          0.2264331877231598,\n",
      "                          0.22835935652256012,\n",
      "                          0.21743488311767578,\n",
      "                          0.21032637357711792,\n",
      "                          0.21973377466201782,\n",
      "                          0.2192569375038147,\n",
      "                          0.22326068580150604,\n",
      "                          0.20661388337612152,\n",
      "                          0.20223049819469452,\n",
      "                          0.20317625999450684,\n",
      "                          0.1874537318944931,\n",
      "                          0.19542814791202545,\n",
      "                          0.19705796241760254,\n",
      "                          0.1898983120918274,\n",
      "                          0.19312821328639984,\n",
      "                          0.18949340283870697],\n",
      "             'val_precision': [0.6235795617103577,\n",
      "                               0.6307356357574463,\n",
      "                               0.7527881264686584,\n",
      "                               0.8591194748878479,\n",
      "                               0.8666666746139526,\n",
      "                               0.8467742204666138,\n",
      "                               0.8705202341079712,\n",
      "                               0.9110275506973267,\n",
      "                               0.8624078631401062,\n",
      "                               0.8925518989562988,\n",
      "                               0.8893280625343323,\n",
      "                               0.8748577833175659,\n",
      "                               0.8761792182922363,\n",
      "                               0.8863919973373413,\n",
      "                               0.8637951016426086,\n",
      "                               0.8807339668273926,\n",
      "                               0.8815330862998962,\n",
      "                               0.9050554633140564,\n",
      "                               0.8989169597625732,\n",
      "                               0.8729215860366821,\n",
      "                               0.8777292370796204,\n",
      "                               0.9056832194328308,\n",
      "                               0.8456725478172302,\n",
      "                               0.8935447335243225,\n",
      "                               0.9182389974594116,\n",
      "                               0.8933030366897583,\n",
      "                               0.9141835570335388,\n",
      "                               0.913095235824585,\n",
      "                               0.9309462904930115,\n",
      "                               0.9050279259681702,\n",
      "                               0.9214975833892822,\n",
      "                               0.9404617547988892,\n",
      "                               0.9238210320472717,\n",
      "                               0.9355230927467346,\n",
      "                               0.9054669737815857,\n",
      "                               0.9174311757087708,\n",
      "                               0.911188006401062,\n",
      "                               0.9277978539466858,\n",
      "                               0.9451074004173279,\n",
      "                               0.9198167324066162,\n",
      "                               0.9089861512184143,\n",
      "                               0.9273356199264526,\n",
      "                               0.9273809790611267,\n",
      "                               0.9460726976394653,\n",
      "                               0.9084895253181458,\n",
      "                               0.9530284404754639,\n",
      "                               0.9395734667778015,\n",
      "                               0.9545454382896423,\n",
      "                               0.9528301954269409,\n",
      "                               0.9482163190841675,\n",
      "                               0.9562129974365234,\n",
      "                               0.9439252614974976,\n",
      "                               0.9273356199264526,\n",
      "                               0.9293163418769836,\n",
      "                               0.9569495916366577,\n",
      "                               0.9507211446762085,\n",
      "                               0.9185918569564819,\n",
      "                               0.9535424113273621,\n",
      "                               0.9578454494476318,\n",
      "                               0.9672955870628357,\n",
      "                               0.955503523349762,\n",
      "                               0.955294132232666,\n",
      "                               0.9618138670921326,\n",
      "                               0.9494712352752686,\n",
      "                               0.9473053812980652,\n",
      "                               0.95652174949646,\n",
      "                               0.9523242115974426,\n",
      "                               0.9476134777069092,\n",
      "                               0.9467592835426331,\n",
      "                               0.9603365659713745,\n",
      "                               0.9590854644775391,\n",
      "                               0.9503464102745056,\n",
      "                               0.9553072452545166,\n",
      "                               0.9588719010353088,\n",
      "                               0.9615384340286255,\n",
      "                               0.9669811129570007,\n",
      "                               0.9669030904769897,\n",
      "                               0.9659224152565002],\n",
      "             'val_recall': [1.0,\n",
      "                            0.9954022765159607,\n",
      "                            0.9299655556678772,\n",
      "                            0.8073285818099976,\n",
      "                            0.7878788113594055,\n",
      "                            0.8606557250022888,\n",
      "                            0.8665132522583008,\n",
      "                            0.8086763024330139,\n",
      "                            0.8317535519599915,\n",
      "                            0.8470451831817627,\n",
      "                            0.7931844592094421,\n",
      "                            0.8748577833175659,\n",
      "                            0.8639534711837769,\n",
      "                            0.856453537940979,\n",
      "                            0.8843861818313599,\n",
      "                            0.8858131766319275,\n",
      "                            0.8825581669807434,\n",
      "                            0.8534883856773376,\n",
      "                            0.8576349020004272,\n",
      "                            0.8866103887557983,\n",
      "                            0.9115646481513977,\n",
      "                            0.880141019821167,\n",
      "                            0.9332566261291504,\n",
      "                            0.9174418449401855,\n",
      "                            0.8548009395599365,\n",
      "                            0.906682014465332,\n",
      "                            0.8826237320899963,\n",
      "                            0.886705219745636,\n",
      "                            0.8676996231079102,\n",
      "                            0.9225512742996216,\n",
      "                            0.8831018805503845,\n",
      "                            0.8765571713447571,\n",
      "                            0.8883720636367798,\n",
      "                            0.8879907727241516,\n",
      "                            0.9044368863105774,\n",
      "                            0.9174311757087708,\n",
      "                            0.9239766001701355,\n",
      "                            0.8933951258659363,\n",
      "                            0.9061784744262695,\n",
      "                            0.930475115776062,\n",
      "                            0.9282352924346924,\n",
      "                            0.9284064769744873,\n",
      "                            0.9079254269599915,\n",
      "                            0.9212328791618347,\n",
      "                            0.9526011347770691,\n",
      "                            0.8862069249153137,\n",
      "                            0.9199535846710205,\n",
      "                            0.9285714030265808,\n",
      "                            0.92766934633255,\n",
      "                            0.9493087530136108,\n",
      "                            0.9234285950660706,\n",
      "                            0.9341040253639221,\n",
      "                            0.9425556659698486,\n",
      "                            0.9424206614494324,\n",
      "                            0.8861047625541687,\n",
      "                            0.9123414158821106,\n",
      "                            0.959770143032074,\n",
      "                            0.9329545497894287,\n",
      "                            0.9316628575325012,\n",
      "                            0.9068396091461182,\n",
      "                            0.9357798099517822,\n",
      "                            0.9279999732971191,\n",
      "                            0.9107344746589661,\n",
      "                            0.9351851940155029,\n",
      "                            0.9427890181541443,\n",
      "                            0.9395017623901367,\n",
      "                            0.9301513433456421,\n",
      "                            0.9410404562950134,\n",
      "                            0.943483293056488,\n",
      "                            0.9411072134971619,\n",
      "                            0.9431952834129333,\n",
      "                            0.9558652639389038,\n",
      "                            0.9671945571899414,\n",
      "                            0.9455388188362122,\n",
      "                            0.9471871256828308,\n",
      "                            0.9545983672142029,\n",
      "                            0.9402298927307129,\n",
      "                            0.9459148645401001]},\n",
      " 'optimal_threshold': 0.5200961232185364,\n",
      " 'train_counts': {'fire': 1490, 'nofire': 1917},\n",
      " 'train_counts_total': 3407,\n",
      " 'train_dataset_size': 6176,\n",
      " 'training_time': 1466.1756443977356,\n",
      " 'val_counts': {'fire': 156, 'nofire': 246},\n",
      " 'val_counts_total': 402,\n",
      " 'val_dataset_size': 1408}\n",
      "Training model: MobileNetV3Small on dataset: The Wildfire Dataset_FIRE\n",
      "Epoch 1/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 133ms/step - accuracy: 0.6485 - auc: 0.7168 - f1_score: 0.6842 - loss: 0.7129 - precision: 0.7278 - recall: 0.6769 - val_accuracy: 0.6056 - val_auc: 0.8070 - val_f1_score: 0.7517 - val_loss: 0.6626 - val_precision: 0.6056 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 90ms/step - accuracy: 0.7131 - auc: 0.7785 - f1_score: 0.7666 - loss: 0.5852 - precision: 0.7563 - recall: 0.7811 - val_accuracy: 0.6647 - val_auc: 0.8624 - val_f1_score: 0.7825 - val_loss: 0.6096 - val_precision: 0.6500 - val_recall: 0.9946 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.7425 - auc: 0.8092 - f1_score: 0.7900 - loss: 0.5286 - precision: 0.7817 - recall: 0.8042 - val_accuracy: 0.7897 - val_auc: 0.8708 - val_f1_score: 0.8367 - val_loss: 0.5393 - val_precision: 0.7723 - val_recall: 0.9178 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 84ms/step - accuracy: 0.7564 - auc: 0.8242 - f1_score: 0.8058 - loss: 0.5108 - precision: 0.7851 - recall: 0.8318 - val_accuracy: 0.7998 - val_auc: 0.8793 - val_f1_score: 0.8400 - val_loss: 0.4579 - val_precision: 0.8138 - val_recall: 0.8791 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.7638 - auc: 0.8340 - f1_score: 0.8038 - loss: 0.4969 - precision: 0.7861 - recall: 0.8286 - val_accuracy: 0.8311 - val_auc: 0.8976 - val_f1_score: 0.8572 - val_loss: 0.4128 - val_precision: 0.8737 - val_recall: 0.8447 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 91ms/step - accuracy: 0.7626 - auc: 0.8399 - f1_score: 0.8046 - loss: 0.4843 - precision: 0.7982 - recall: 0.8163 - val_accuracy: 0.8108 - val_auc: 0.8919 - val_f1_score: 0.8446 - val_loss: 0.4200 - val_precision: 0.8318 - val_recall: 0.8641 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 86ms/step - accuracy: 0.7781 - auc: 0.8527 - f1_score: 0.8199 - loss: 0.4630 - precision: 0.8030 - recall: 0.8433 - val_accuracy: 0.7965 - val_auc: 0.8909 - val_f1_score: 0.8179 - val_loss: 0.4213 - val_precision: 0.8827 - val_recall: 0.7693 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.7801 - auc: 0.8553 - f1_score: 0.8216 - loss: 0.4598 - precision: 0.8181 - recall: 0.8278 - val_accuracy: 0.8353 - val_auc: 0.9118 - val_f1_score: 0.8659 - val_loss: 0.3847 - val_precision: 0.8498 - val_recall: 0.8884 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.7822 - auc: 0.8539 - f1_score: 0.8229 - loss: 0.4641 - precision: 0.8115 - recall: 0.8387 - val_accuracy: 0.7981 - val_auc: 0.8999 - val_f1_score: 0.8192 - val_loss: 0.4134 - val_precision: 0.8966 - val_recall: 0.7603 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.7922 - auc: 0.8637 - f1_score: 0.8291 - loss: 0.4508 - precision: 0.8243 - recall: 0.8364 - val_accuracy: 0.8252 - val_auc: 0.9148 - val_f1_score: 0.8458 - val_loss: 0.3866 - val_precision: 0.8845 - val_recall: 0.8163 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.7754 - auc: 0.8579 - f1_score: 0.8194 - loss: 0.4559 - precision: 0.8056 - recall: 0.8376 - val_accuracy: 0.8404 - val_auc: 0.9147 - val_f1_score: 0.8678 - val_loss: 0.3861 - val_precision: 0.8605 - val_recall: 0.8785 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.7796 - auc: 0.8585 - f1_score: 0.8193 - loss: 0.4563 - precision: 0.8060 - recall: 0.8386 - val_accuracy: 0.8370 - val_auc: 0.9174 - val_f1_score: 0.8648 - val_loss: 0.3774 - val_precision: 0.8601 - val_recall: 0.8755 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - accuracy: 0.7862 - auc: 0.8597 - f1_score: 0.8247 - loss: 0.4552 - precision: 0.8161 - recall: 0.8392 - val_accuracy: 0.8564 - val_auc: 0.9250 - val_f1_score: 0.8806 - val_loss: 0.3658 - val_precision: 0.8846 - val_recall: 0.8822 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.7802 - auc: 0.8610 - f1_score: 0.8190 - loss: 0.4499 - precision: 0.8108 - recall: 0.8344 - val_accuracy: 0.8353 - val_auc: 0.9195 - val_f1_score: 0.8615 - val_loss: 0.3687 - val_precision: 0.8793 - val_recall: 0.8491 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 87ms/step - accuracy: 0.7986 - auc: 0.8732 - f1_score: 0.8352 - loss: 0.4375 - precision: 0.8186 - recall: 0.8576 - val_accuracy: 0.8480 - val_auc: 0.9270 - val_f1_score: 0.8662 - val_loss: 0.3655 - val_precision: 0.9074 - val_recall: 0.8340 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.8014 - auc: 0.8764 - f1_score: 0.8395 - loss: 0.4262 - precision: 0.8495 - recall: 0.8311 - val_accuracy: 0.8497 - val_auc: 0.9232 - val_f1_score: 0.8778 - val_loss: 0.3545 - val_precision: 0.8865 - val_recall: 0.8721 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 74ms/step - accuracy: 0.8075 - auc: 0.8776 - f1_score: 0.8413 - loss: 0.4288 - precision: 0.8385 - recall: 0.8465 - val_accuracy: 0.8404 - val_auc: 0.9246 - val_f1_score: 0.8618 - val_loss: 0.3628 - val_precision: 0.9013 - val_recall: 0.8306 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 81ms/step - accuracy: 0.7928 - auc: 0.8770 - f1_score: 0.8303 - loss: 0.4263 - precision: 0.8293 - recall: 0.8367 - val_accuracy: 0.8632 - val_auc: 0.9302 - val_f1_score: 0.8851 - val_loss: 0.3578 - val_precision: 0.8837 - val_recall: 0.8911 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 82ms/step - accuracy: 0.7980 - auc: 0.8817 - f1_score: 0.8322 - loss: 0.4233 - precision: 0.8224 - recall: 0.8473 - val_accuracy: 0.8573 - val_auc: 0.9276 - val_f1_score: 0.8801 - val_loss: 0.3525 - val_precision: 0.8931 - val_recall: 0.8748 - learning_rate: 0.0010\n",
      "Epoch 20/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.8105 - auc: 0.8872 - f1_score: 0.8444 - loss: 0.4112 - precision: 0.8359 - recall: 0.8564 - val_accuracy: 0.8539 - val_auc: 0.9354 - val_f1_score: 0.8748 - val_loss: 0.3320 - val_precision: 0.8925 - val_recall: 0.8668 - learning_rate: 0.0010\n",
      "Epoch 21/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 82ms/step - accuracy: 0.8029 - auc: 0.8773 - f1_score: 0.8379 - loss: 0.4268 - precision: 0.8335 - recall: 0.8461 - val_accuracy: 0.8454 - val_auc: 0.9334 - val_f1_score: 0.8688 - val_loss: 0.3506 - val_precision: 0.9010 - val_recall: 0.8433 - learning_rate: 0.0010\n",
      "Epoch 22/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.8085 - auc: 0.8879 - f1_score: 0.8422 - loss: 0.4088 - precision: 0.8457 - recall: 0.8430 - val_accuracy: 0.8666 - val_auc: 0.9385 - val_f1_score: 0.8879 - val_loss: 0.3355 - val_precision: 0.9077 - val_recall: 0.8730 - learning_rate: 0.0010\n",
      "Epoch 23/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.8015 - auc: 0.8801 - f1_score: 0.8375 - loss: 0.4215 - precision: 0.8340 - recall: 0.8431 - val_accuracy: 0.8632 - val_auc: 0.9378 - val_f1_score: 0.8847 - val_loss: 0.3331 - val_precision: 0.8817 - val_recall: 0.8986 - learning_rate: 0.0010\n",
      "Epoch 24/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step - accuracy: 0.8106 - auc: 0.8887 - f1_score: 0.8460 - loss: 0.4055 - precision: 0.8408 - recall: 0.8546 - val_accuracy: 0.8742 - val_auc: 0.9442 - val_f1_score: 0.8906 - val_loss: 0.3242 - val_precision: 0.9095 - val_recall: 0.8804 - learning_rate: 0.0010\n",
      "Epoch 25/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 93ms/step - accuracy: 0.8035 - auc: 0.8807 - f1_score: 0.8385 - loss: 0.4253 - precision: 0.8273 - recall: 0.8530 - val_accuracy: 0.8666 - val_auc: 0.9399 - val_f1_score: 0.8908 - val_loss: 0.3268 - val_precision: 0.8932 - val_recall: 0.8932 - learning_rate: 0.0010\n",
      "Epoch 26/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.8097 - auc: 0.8880 - f1_score: 0.8428 - loss: 0.4088 - precision: 0.8352 - recall: 0.8560 - val_accuracy: 0.8632 - val_auc: 0.9386 - val_f1_score: 0.8823 - val_loss: 0.3309 - val_precision: 0.8828 - val_recall: 0.8926 - learning_rate: 0.0010\n",
      "Epoch 27/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.8120 - auc: 0.8855 - f1_score: 0.8472 - loss: 0.4115 - precision: 0.8406 - recall: 0.8572 - val_accuracy: 0.8826 - val_auc: 0.9513 - val_f1_score: 0.9026 - val_loss: 0.3115 - val_precision: 0.9399 - val_recall: 0.8714 - learning_rate: 0.0010\n",
      "Epoch 28/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.7981 - auc: 0.8836 - f1_score: 0.8314 - loss: 0.4161 - precision: 0.8312 - recall: 0.8386 - val_accuracy: 0.8539 - val_auc: 0.9353 - val_f1_score: 0.8756 - val_loss: 0.3337 - val_precision: 0.8961 - val_recall: 0.8601 - learning_rate: 0.0010\n",
      "Epoch 29/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.8077 - auc: 0.8887 - f1_score: 0.8410 - loss: 0.4088 - precision: 0.8445 - recall: 0.8427 - val_accuracy: 0.8682 - val_auc: 0.9411 - val_f1_score: 0.8885 - val_loss: 0.3226 - val_precision: 0.8799 - val_recall: 0.9000 - learning_rate: 0.0010\n",
      "Epoch 30/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.8154 - auc: 0.8906 - f1_score: 0.8504 - loss: 0.4009 - precision: 0.8576 - recall: 0.8485 - val_accuracy: 0.8691 - val_auc: 0.9454 - val_f1_score: 0.8898 - val_loss: 0.3156 - val_precision: 0.9021 - val_recall: 0.8836 - learning_rate: 0.0010\n",
      "Epoch 31/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.8217 - auc: 0.8979 - f1_score: 0.8536 - loss: 0.3925 - precision: 0.8473 - recall: 0.8617 - val_accuracy: 0.8750 - val_auc: 0.9458 - val_f1_score: 0.8952 - val_loss: 0.3126 - val_precision: 0.9299 - val_recall: 0.8678 - learning_rate: 0.0010\n",
      "Epoch 32/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.8216 - auc: 0.8936 - f1_score: 0.8522 - loss: 0.4011 - precision: 0.8463 - recall: 0.8626 - val_accuracy: 0.8784 - val_auc: 0.9491 - val_f1_score: 0.8981 - val_loss: 0.3070 - val_precision: 0.9033 - val_recall: 0.8984 - learning_rate: 0.0010\n",
      "Epoch 33/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.8141 - auc: 0.8929 - f1_score: 0.8436 - loss: 0.4014 - precision: 0.8415 - recall: 0.8544 - val_accuracy: 0.8674 - val_auc: 0.9423 - val_f1_score: 0.8894 - val_loss: 0.3196 - val_precision: 0.8740 - val_recall: 0.9053 - learning_rate: 0.0010\n",
      "Epoch 34/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.8160 - auc: 0.8904 - f1_score: 0.8504 - loss: 0.4081 - precision: 0.8503 - recall: 0.8536 - val_accuracy: 0.8674 - val_auc: 0.9489 - val_f1_score: 0.8862 - val_loss: 0.3136 - val_precision: 0.9290 - val_recall: 0.8521 - learning_rate: 0.0010\n",
      "Epoch 35/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 73ms/step - accuracy: 0.8198 - auc: 0.9007 - f1_score: 0.8526 - loss: 0.3878 - precision: 0.8527 - recall: 0.8542 - val_accuracy: 0.8818 - val_auc: 0.9506 - val_f1_score: 0.8992 - val_loss: 0.3158 - val_precision: 0.8920 - val_recall: 0.9122 - learning_rate: 0.0010\n",
      "Epoch 36/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 73ms/step - accuracy: 0.8198 - auc: 0.8990 - f1_score: 0.8543 - loss: 0.3904 - precision: 0.8458 - recall: 0.8639 - val_accuracy: 0.8792 - val_auc: 0.9499 - val_f1_score: 0.8992 - val_loss: 0.3044 - val_precision: 0.9182 - val_recall: 0.8815 - learning_rate: 0.0010\n",
      "Epoch 37/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.8253 - auc: 0.9012 - f1_score: 0.8558 - loss: 0.3869 - precision: 0.8567 - recall: 0.8577 - val_accuracy: 0.8818 - val_auc: 0.9552 - val_f1_score: 0.9029 - val_loss: 0.2901 - val_precision: 0.9075 - val_recall: 0.9026 - learning_rate: 0.0010\n",
      "Epoch 38/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 74ms/step - accuracy: 0.8126 - auc: 0.8965 - f1_score: 0.8468 - loss: 0.3971 - precision: 0.8393 - recall: 0.8563 - val_accuracy: 0.8953 - val_auc: 0.9537 - val_f1_score: 0.9109 - val_loss: 0.3009 - val_precision: 0.9209 - val_recall: 0.9027 - learning_rate: 0.0010\n",
      "Epoch 39/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.8249 - auc: 0.9018 - f1_score: 0.8600 - loss: 0.3821 - precision: 0.8531 - recall: 0.8686 - val_accuracy: 0.8834 - val_auc: 0.9580 - val_f1_score: 0.9032 - val_loss: 0.2931 - val_precision: 0.8898 - val_recall: 0.9198 - learning_rate: 0.0010\n",
      "Epoch 40/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.8333 - auc: 0.9002 - f1_score: 0.8655 - loss: 0.3901 - precision: 0.8585 - recall: 0.8749 - val_accuracy: 0.8868 - val_auc: 0.9531 - val_f1_score: 0.9055 - val_loss: 0.2909 - val_precision: 0.9062 - val_recall: 0.9112 - learning_rate: 0.0010\n",
      "Epoch 41/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 74ms/step - accuracy: 0.8344 - auc: 0.9073 - f1_score: 0.8640 - loss: 0.3745 - precision: 0.8654 - recall: 0.8634 - val_accuracy: 0.9046 - val_auc: 0.9640 - val_f1_score: 0.9222 - val_loss: 0.2725 - val_precision: 0.9183 - val_recall: 0.9295 - learning_rate: 0.0010\n",
      "Epoch 42/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.8265 - auc: 0.9063 - f1_score: 0.8594 - loss: 0.3759 - precision: 0.8516 - recall: 0.8699 - val_accuracy: 0.9037 - val_auc: 0.9616 - val_f1_score: 0.9177 - val_loss: 0.2796 - val_precision: 0.9197 - val_recall: 0.9222 - learning_rate: 0.0010\n",
      "Epoch 43/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 81ms/step - accuracy: 0.8309 - auc: 0.9091 - f1_score: 0.8593 - loss: 0.3713 - precision: 0.8570 - recall: 0.8660 - val_accuracy: 0.9096 - val_auc: 0.9668 - val_f1_score: 0.9248 - val_loss: 0.2676 - val_precision: 0.9330 - val_recall: 0.9216 - learning_rate: 0.0010\n",
      "Epoch 44/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.8268 - auc: 0.9021 - f1_score: 0.8582 - loss: 0.3858 - precision: 0.8579 - recall: 0.8604 - val_accuracy: 0.9079 - val_auc: 0.9633 - val_f1_score: 0.9219 - val_loss: 0.2730 - val_precision: 0.9363 - val_recall: 0.9147 - learning_rate: 0.0010\n",
      "Epoch 45/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 76ms/step - accuracy: 0.8278 - auc: 0.8980 - f1_score: 0.8593 - loss: 0.3927 - precision: 0.8544 - recall: 0.8660 - val_accuracy: 0.9054 - val_auc: 0.9695 - val_f1_score: 0.9188 - val_loss: 0.2658 - val_precision: 0.9413 - val_recall: 0.8990 - learning_rate: 0.0010\n",
      "Epoch 46/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.8354 - auc: 0.9129 - f1_score: 0.8650 - loss: 0.3628 - precision: 0.8606 - recall: 0.8725 - val_accuracy: 0.8986 - val_auc: 0.9595 - val_f1_score: 0.9156 - val_loss: 0.2756 - val_precision: 0.9303 - val_recall: 0.9050 - learning_rate: 0.0010\n",
      "Epoch 47/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.8274 - auc: 0.9134 - f1_score: 0.8579 - loss: 0.3612 - precision: 0.8636 - recall: 0.8553 - val_accuracy: 0.9189 - val_auc: 0.9712 - val_f1_score: 0.9316 - val_loss: 0.2554 - val_precision: 0.9357 - val_recall: 0.9305 - learning_rate: 0.0010\n",
      "Epoch 48/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.8270 - auc: 0.9073 - f1_score: 0.8590 - loss: 0.3720 - precision: 0.8594 - recall: 0.8613 - val_accuracy: 0.8995 - val_auc: 0.9604 - val_f1_score: 0.9176 - val_loss: 0.2780 - val_precision: 0.9223 - val_recall: 0.9135 - learning_rate: 0.0010\n",
      "Epoch 49/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.8389 - auc: 0.9117 - f1_score: 0.8685 - loss: 0.3678 - precision: 0.8705 - recall: 0.8699 - val_accuracy: 0.9079 - val_auc: 0.9724 - val_f1_score: 0.9190 - val_loss: 0.2570 - val_precision: 0.9408 - val_recall: 0.9021 - learning_rate: 0.0010\n",
      "Epoch 50/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.8341 - auc: 0.9079 - f1_score: 0.8671 - loss: 0.3694 - precision: 0.8701 - recall: 0.8681 - val_accuracy: 0.9122 - val_auc: 0.9699 - val_f1_score: 0.9245 - val_loss: 0.2590 - val_precision: 0.9351 - val_recall: 0.9196 - learning_rate: 0.0010\n",
      "Epoch 51/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.8364 - auc: 0.9108 - f1_score: 0.8678 - loss: 0.3688 - precision: 0.8594 - recall: 0.8776 - val_accuracy: 0.9071 - val_auc: 0.9695 - val_f1_score: 0.9201 - val_loss: 0.2613 - val_precision: 0.9403 - val_recall: 0.9068 - learning_rate: 0.0010\n",
      "Epoch 52/80\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8384 - auc: 0.9120 - f1_score: 0.8668 - loss: 0.3676 - precision: 0.8679 - recall: 0.8699\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 82ms/step - accuracy: 0.8383 - auc: 0.9120 - f1_score: 0.8668 - loss: 0.3676 - precision: 0.8678 - recall: 0.8699 - val_accuracy: 0.9155 - val_auc: 0.9712 - val_f1_score: 0.9283 - val_loss: 0.2691 - val_precision: 0.9229 - val_recall: 0.9384 - learning_rate: 0.0010\n",
      "Training time: 721.00 seconds\n",
      "Evaluating MobileNetV3Small on The Wildfire Dataset_FIRE...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6209 - auc: 0.5794 - f1_score: 0.3316 - loss: 0.7876 - precision: 0.4292 - recall: 0.6059            \n",
      "Training results:\n",
      "{'evaluation': {'accuracy': 0.6458333134651184,\n",
      "                'auc': 0.7620652318000793,\n",
      "                'f1_score': 0.5020489692687988,\n",
      "                'loss': 0.7347317337989807,\n",
      "                'precision': 0.625,\n",
      "                'recall': 0.800000011920929},\n",
      " 'history': {'accuracy': [0.6880613565444946,\n",
      "                          0.722305417060852,\n",
      "                          0.7514970302581787,\n",
      "                          0.7591691613197327,\n",
      "                          0.7640344500541687,\n",
      "                          0.763660192489624,\n",
      "                          0.777694582939148,\n",
      "                          0.7806886434555054,\n",
      "                          0.785179615020752,\n",
      "                          0.783682644367218,\n",
      "                          0.7791916131973267,\n",
      "                          0.7846182584762573,\n",
      "                          0.7840569019317627,\n",
      "                          0.788735032081604,\n",
      "                          0.7994012236595154,\n",
      "                          0.8005239367485046,\n",
      "                          0.802582323551178,\n",
      "                          0.7928518056869507,\n",
      "                          0.8003368377685547,\n",
      "                          0.808570384979248,\n",
      "                          0.8040793538093567,\n",
      "                          0.8040793538093567,\n",
      "                          0.8072604537010193,\n",
      "                          0.8070733547210693,\n",
      "                          0.8115643858909607,\n",
      "                          0.8130613565444946,\n",
      "                          0.81268709897995,\n",
      "                          0.8031437397003174,\n",
      "                          0.816055417060852,\n",
      "                          0.815119743347168,\n",
      "                          0.8227919340133667,\n",
      "                          0.8216691613197327,\n",
      "                          0.8235403895378113,\n",
      "                          0.816429615020752,\n",
      "                          0.8181137442588806,\n",
      "                          0.8220434188842773,\n",
      "                          0.82485032081604,\n",
      "                          0.8244760632514954,\n",
      "                          0.82485032081604,\n",
      "                          0.8310254216194153,\n",
      "                          0.8291541934013367,\n",
      "                          0.8265344500541687,\n",
      "                          0.8278443217277527,\n",
      "                          0.826347291469574,\n",
      "                          0.83121258020401,\n",
      "                          0.8334580659866333,\n",
      "                          0.828779935836792,\n",
      "                          0.8385104537010193,\n",
      "                          0.8370134830474854,\n",
      "                          0.832522451877594,\n",
      "                          0.8347679376602173,\n",
      "                          0.8340194821357727],\n",
      "             'auc': [0.7473361492156982,\n",
      "                     0.7914280891418457,\n",
      "                     0.81758052110672,\n",
      "                     0.8315495848655701,\n",
      "                     0.8360700011253357,\n",
      "                     0.8402972221374512,\n",
      "                     0.8533835411071777,\n",
      "                     0.853629469871521,\n",
      "                     0.8555005788803101,\n",
      "                     0.8555285930633545,\n",
      "                     0.8571921586990356,\n",
      "                     0.85813307762146,\n",
      "                     0.8632196187973022,\n",
      "                     0.866189181804657,\n",
      "                     0.8738018274307251,\n",
      "                     0.8766921758651733,\n",
      "                     0.876295268535614,\n",
      "                     0.8759414553642273,\n",
      "                     0.8810322880744934,\n",
      "                     0.8861664533615112,\n",
      "                     0.8779541850090027,\n",
      "                     0.8847809433937073,\n",
      "                     0.8852694630622864,\n",
      "                     0.8844856023788452,\n",
      "                     0.884842038154602,\n",
      "                     0.8898855447769165,\n",
      "                     0.8875065445899963,\n",
      "                     0.8864296078681946,\n",
      "                     0.893345832824707,\n",
      "                     0.8915931582450867,\n",
      "                     0.8973062634468079,\n",
      "                     0.8967524170875549,\n",
      "                     0.8968077898025513,\n",
      "                     0.8940297961235046,\n",
      "                     0.8970941305160522,\n",
      "                     0.902515172958374,\n",
      "                     0.9020123481750488,\n",
      "                     0.9022488594055176,\n",
      "                     0.9045655131340027,\n",
      "                     0.9032050371170044,\n",
      "                     0.9026221632957458,\n",
      "                     0.9058454632759094,\n",
      "                     0.9050700664520264,\n",
      "                     0.9035801291465759,\n",
      "                     0.9057730436325073,\n",
      "                     0.9128168225288391,\n",
      "                     0.9104976654052734,\n",
      "                     0.9142770767211914,\n",
      "                     0.9102770090103149,\n",
      "                     0.9064529538154602,\n",
      "                     0.9122331142425537,\n",
      "                     0.9110753536224365],\n",
      "             'f1_score': [0.7331551909446716,\n",
      "                          0.7748351693153381,\n",
      "                          0.7972943186759949,\n",
      "                          0.8059308528900146,\n",
      "                          0.8073559403419495,\n",
      "                          0.8070376515388489,\n",
      "                          0.8194228410720825,\n",
      "                          0.8231432437896729,\n",
      "                          0.8252807259559631,\n",
      "                          0.8239057064056396,\n",
      "                          0.8213403224945068,\n",
      "                          0.824600875377655,\n",
      "                          0.8238409161567688,\n",
      "                          0.8276917338371277,\n",
      "                          0.8358231782913208,\n",
      "                          0.8372440934181213,\n",
      "                          0.8384339809417725,\n",
      "                          0.830774188041687,\n",
      "                          0.836061418056488,\n",
      "                          0.8437491655349731,\n",
      "                          0.838974118232727,\n",
      "                          0.8393244743347168,\n",
      "                          0.8419783115386963,\n",
      "                          0.842812180519104,\n",
      "                          0.8458935618400574,\n",
      "                          0.8461917042732239,\n",
      "                          0.8469628691673279,\n",
      "                          0.8372634649276733,\n",
      "                          0.8486554026603699,\n",
      "                          0.8487963676452637,\n",
      "                          0.8547694087028503,\n",
      "                          0.8536366820335388,\n",
      "                          0.8543099761009216,\n",
      "                          0.8508813381195068,\n",
      "                          0.8515186309814453,\n",
      "                          0.8550993204116821,\n",
      "                          0.856088399887085,\n",
      "                          0.8565876483917236,\n",
      "                          0.8576757907867432,\n",
      "                          0.862401008605957,\n",
      "                          0.859420895576477,\n",
      "                          0.857864499092102,\n",
      "                          0.8573147058486938,\n",
      "                          0.8576542139053345,\n",
      "                          0.861737072467804,\n",
      "                          0.8636210560798645,\n",
      "                          0.8591159582138062,\n",
      "                          0.8680296540260315,\n",
      "                          0.8667703866958618,\n",
      "                          0.8640961647033691,\n",
      "                          0.8655253648757935,\n",
      "                          0.8637866377830505],\n",
      "             'learning_rate': [0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513],\n",
      "             'loss': [0.6491774916648865,\n",
      "                      0.5597213506698608,\n",
      "                      0.5163880586624146,\n",
      "                      0.4987625181674957,\n",
      "                      0.4886665642261505,\n",
      "                      0.4808005094528198,\n",
      "                      0.46281084418296814,\n",
      "                      0.4616369903087616,\n",
      "                      0.46144333481788635,\n",
      "                      0.4600681960582733,\n",
      "                      0.45699164271354675,\n",
      "                      0.4550997018814087,\n",
      "                      0.44730499386787415,\n",
      "                      0.44085267186164856,\n",
      "                      0.4338146448135376,\n",
      "                      0.4269106686115265,\n",
      "                      0.42972642183303833,\n",
      "                      0.42935019731521606,\n",
      "                      0.4215982258319855,\n",
      "                      0.4108421504497528,\n",
      "                      0.42658963799476624,\n",
      "                      0.41409388184547424,\n",
      "                      0.413237601518631,\n",
      "                      0.4133017659187317,\n",
      "                      0.41553789377212524,\n",
      "                      0.40602976083755493,\n",
      "                      0.41002270579338074,\n",
      "                      0.4114264249801636,\n",
      "                      0.40171438455581665,\n",
      "                      0.4009720981121063,\n",
      "                      0.39398902654647827,\n",
      "                      0.39460504055023193,\n",
      "                      0.394136905670166,\n",
      "                      0.39992332458496094,\n",
      "                      0.3943503499031067,\n",
      "                      0.38362637162208557,\n",
      "                      0.38429608941078186,\n",
      "                      0.3854944705963135,\n",
      "                      0.377961665391922,\n",
      "                      0.38240697979927063,\n",
      "                      0.38387683033943176,\n",
      "                      0.3769868314266205,\n",
      "                      0.3788643479347229,\n",
      "                      0.38230985403060913,\n",
      "                      0.37761005759239197,\n",
      "                      0.36285698413848877,\n",
      "                      0.3674435317516327,\n",
      "                      0.3608507513999939,\n",
      "                      0.3706585466861725,\n",
      "                      0.37295159697532654,\n",
      "                      0.36463746428489685,\n",
      "                      0.36798834800720215],\n",
      "             'precision': [0.7557346820831299,\n",
      "                           0.7640949487686157,\n",
      "                           0.7927523255348206,\n",
      "                           0.792640209197998,\n",
      "                           0.7950088977813721,\n",
      "                           0.801433265209198,\n",
      "                           0.8063754439353943,\n",
      "                           0.8116883039474487,\n",
      "                           0.8178016543388367,\n",
      "                           0.8173938989639282,\n",
      "                           0.8100706934928894,\n",
      "                           0.815617561340332,\n",
      "                           0.8158833980560303,\n",
      "                           0.8203450441360474,\n",
      "                           0.8251414895057678,\n",
      "                           0.8362095355987549,\n",
      "                           0.8322288990020752,\n",
      "                           0.8273898959159851,\n",
      "                           0.8315250277519226,\n",
      "                           0.8402295112609863,\n",
      "                           0.8327316641807556,\n",
      "                           0.8361496925354004,\n",
      "                           0.8406193256378174,\n",
      "                           0.8386227488517761,\n",
      "                           0.8396027684211731,\n",
      "                           0.8422322869300842,\n",
      "                           0.8394728899002075,\n",
      "                           0.8332830667495728,\n",
      "                           0.8453421592712402,\n",
      "                           0.850210964679718,\n",
      "                           0.8504701256752014,\n",
      "                           0.8492326140403748,\n",
      "                           0.8512820601463318,\n",
      "                           0.8470553159713745,\n",
      "                           0.8491079807281494,\n",
      "                           0.8488581776618958,\n",
      "                           0.8558832406997681,\n",
      "                           0.8545949459075928,\n",
      "                           0.8530207276344299,\n",
      "                           0.8550029993057251,\n",
      "                           0.861425518989563,\n",
      "                           0.8551161885261536,\n",
      "                           0.8547112345695496,\n",
      "                           0.85523521900177,\n",
      "                           0.8577443361282349,\n",
      "                           0.8609933257102966,\n",
      "                           0.8604580163955688,\n",
      "                           0.8635542392730713,\n",
      "                           0.8634586334228516,\n",
      "                           0.8588445782661438,\n",
      "                           0.8582535982131958,\n",
      "                           0.8631961345672607],\n",
      "             'recall': [0.721301794052124,\n",
      "                        0.7889093160629272,\n",
      "                        0.8062747716903687,\n",
      "                        0.8246733546257019,\n",
      "                        0.8241453766822815,\n",
      "                        0.8177940249443054,\n",
      "                        0.8370097875595093,\n",
      "                        0.8373934030532837,\n",
      "                        0.8357753157615662,\n",
      "                        0.8338414430618286,\n",
      "                        0.8371880650520325,\n",
      "                        0.8382667303085327,\n",
      "                        0.8367907404899597,\n",
      "                        0.8400852680206299,\n",
      "                        0.8509984612464905,\n",
      "                        0.8402798771858215,\n",
      "                        0.8472861051559448,\n",
      "                        0.8386998772621155,\n",
      "                        0.8445056676864624,\n",
      "                        0.8492063283920288,\n",
      "                        0.8493402600288391,\n",
      "                        0.8461068868637085,\n",
      "                        0.8457544445991516,\n",
      "                        0.8505921363830566,\n",
      "                        0.8547794222831726,\n",
      "                        0.8543451428413391,\n",
      "                        0.857711136341095,\n",
      "                        0.8470732569694519,\n",
      "                        0.8564447164535522,\n",
      "                        0.8517512083053589,\n",
      "                        0.8606507182121277,\n",
      "                        0.8619425892829895,\n",
      "                        0.8624694347381592,\n",
      "                        0.856578528881073,\n",
      "                        0.8558366298675537,\n",
      "                        0.8631225228309631,\n",
      "                        0.8590173721313477,\n",
      "                        0.8608404397964478,\n",
      "                        0.8639269471168518,\n",
      "                        0.8719828724861145,\n",
      "                        0.8595848679542542,\n",
      "                        0.8637195229530334,\n",
      "                        0.8641671538352966,\n",
      "                        0.8620373010635376,\n",
      "                        0.8692471981048584,\n",
      "                        0.8683567643165588,\n",
      "                        0.8601953387260437,\n",
      "                        0.8748855590820312,\n",
      "                        0.8731752038002014,\n",
      "                        0.872617244720459,\n",
      "                        0.8752668499946594,\n",
      "                        0.8676604628562927],\n",
      "             'val_accuracy': [0.6055743098258972,\n",
      "                              0.6646959185600281,\n",
      "                              0.7896959185600281,\n",
      "                              0.7998310923576355,\n",
      "                              0.8310810923576355,\n",
      "                              0.8108108043670654,\n",
      "                              0.7964527010917664,\n",
      "                              0.8353040814399719,\n",
      "                              0.7981418967247009,\n",
      "                              0.8251689076423645,\n",
      "                              0.8403716087341309,\n",
      "                              0.8369932174682617,\n",
      "                              0.8564189076423645,\n",
      "                              0.8353040814399719,\n",
      "                              0.8479729890823364,\n",
      "                              0.849662184715271,\n",
      "                              0.8403716087341309,\n",
      "                              0.8631756901741028,\n",
      "                              0.8572635054588318,\n",
      "                              0.8538851141929626,\n",
      "                              0.8454391956329346,\n",
      "                              0.8665540814399719,\n",
      "                              0.8631756901741028,\n",
      "                              0.8741554021835327,\n",
      "                              0.8665540814399719,\n",
      "                              0.8631756901741028,\n",
      "                              0.8826013803482056,\n",
      "                              0.8538851141929626,\n",
      "                              0.8682432174682617,\n",
      "                              0.869087815284729,\n",
      "                              0.875,\n",
      "                              0.8783783912658691,\n",
      "                              0.8673986196517944,\n",
      "                              0.8673986196517944,\n",
      "                              0.8817567825317383,\n",
      "                              0.8792229890823364,\n",
      "                              0.8817567825317383,\n",
      "                              0.8952702879905701,\n",
      "                              0.8834459185600281,\n",
      "                              0.8868243098258972,\n",
      "                              0.9045608043670654,\n",
      "                              0.9037162065505981,\n",
      "                              0.9096283912658691,\n",
      "                              0.9079391956329346,\n",
      "                              0.9054054021835327,\n",
      "                              0.8986486196517944,\n",
      "                              0.9189189076423645,\n",
      "                              0.8994932174682617,\n",
      "                              0.9079391956329346,\n",
      "                              0.912162184715271,\n",
      "                              0.9070945978164673,\n",
      "                              0.9155405163764954],\n",
      "             'val_auc': [0.8070162534713745,\n",
      "                         0.8623600006103516,\n",
      "                         0.8708381056785583,\n",
      "                         0.8793414235115051,\n",
      "                         0.8975774049758911,\n",
      "                         0.8919411897659302,\n",
      "                         0.8908929824829102,\n",
      "                         0.9118171334266663,\n",
      "                         0.899861216545105,\n",
      "                         0.9148122668266296,\n",
      "                         0.9146955609321594,\n",
      "                         0.9174174666404724,\n",
      "                         0.9249954223632812,\n",
      "                         0.9194892644882202,\n",
      "                         0.9269559383392334,\n",
      "                         0.923207700252533,\n",
      "                         0.9246228933334351,\n",
      "                         0.9301512241363525,\n",
      "                         0.9276335835456848,\n",
      "                         0.9353672862052917,\n",
      "                         0.9334166049957275,\n",
      "                         0.938513994216919,\n",
      "                         0.9377798438072205,\n",
      "                         0.944202721118927,\n",
      "                         0.9399226903915405,\n",
      "                         0.9386197328567505,\n",
      "                         0.9512923359870911,\n",
      "                         0.9352598190307617,\n",
      "                         0.9410950541496277,\n",
      "                         0.9454212188720703,\n",
      "                         0.9458481669425964,\n",
      "                         0.9491336941719055,\n",
      "                         0.9422827363014221,\n",
      "                         0.9488828182220459,\n",
      "                         0.9505656957626343,\n",
      "                         0.949853777885437,\n",
      "                         0.955232560634613,\n",
      "                         0.9536886215209961,\n",
      "                         0.9579887986183167,\n",
      "                         0.9531438946723938,\n",
      "                         0.963952362537384,\n",
      "                         0.9616274237632751,\n",
      "                         0.9667578935623169,\n",
      "                         0.9632786512374878,\n",
      "                         0.9694973230361938,\n",
      "                         0.9594568610191345,\n",
      "                         0.9712429046630859,\n",
      "                         0.960440456867218,\n",
      "                         0.9723566770553589,\n",
      "                         0.9699181914329529,\n",
      "                         0.969470739364624,\n",
      "                         0.9712095260620117],\n",
      "             'val_f1_score': [0.7517128586769104,\n",
      "                              0.7824839353561401,\n",
      "                              0.8366975784301758,\n",
      "                              0.8399966359138489,\n",
      "                              0.8571714162826538,\n",
      "                              0.8446245789527893,\n",
      "                              0.8179364204406738,\n",
      "                              0.8658554553985596,\n",
      "                              0.8191808462142944,\n",
      "                              0.8457819223403931,\n",
      "                              0.8678488731384277,\n",
      "                              0.8648401498794556,\n",
      "                              0.8805657625198364,\n",
      "                              0.8614551424980164,\n",
      "                              0.8661547303199768,\n",
      "                              0.8777633905410767,\n",
      "                              0.8618072867393494,\n",
      "                              0.8850740790367126,\n",
      "                              0.8800763487815857,\n",
      "                              0.8748191595077515,\n",
      "                              0.8687859773635864,\n",
      "                              0.8878542184829712,\n",
      "                              0.8847158551216125,\n",
      "                              0.8906152248382568,\n",
      "                              0.8908364772796631,\n",
      "                              0.882296085357666,\n",
      "                              0.902587890625,\n",
      "                              0.8755589127540588,\n",
      "                              0.888460636138916,\n",
      "                              0.8897981643676758,\n",
      "                              0.8952054977416992,\n",
      "                              0.8981088995933533,\n",
      "                              0.889393150806427,\n",
      "                              0.886195182800293,\n",
      "                              0.8991899490356445,\n",
      "                              0.8991783857345581,\n",
      "                              0.9028794765472412,\n",
      "                              0.910932183265686,\n",
      "                              0.9032124876976013,\n",
      "                              0.905450165271759,\n",
      "                              0.922201931476593,\n",
      "                              0.9177431464195251,\n",
      "                              0.9247857928276062,\n",
      "                              0.9219084978103638,\n",
      "                              0.9187536239624023,\n",
      "                              0.9156392812728882,\n",
      "                              0.9316391944885254,\n",
      "                              0.917558491230011,\n",
      "                              0.9189783334732056,\n",
      "                              0.9244540333747864,\n",
      "                              0.9200745820999146,\n",
      "                              0.9282609820365906],\n",
      "             'val_loss': [0.6626179218292236,\n",
      "                          0.6096369028091431,\n",
      "                          0.5392689108848572,\n",
      "                          0.4579419195652008,\n",
      "                          0.41282978653907776,\n",
      "                          0.42002686858177185,\n",
      "                          0.42126089334487915,\n",
      "                          0.3846505284309387,\n",
      "                          0.41335487365722656,\n",
      "                          0.3866289258003235,\n",
      "                          0.3861258625984192,\n",
      "                          0.3773881196975708,\n",
      "                          0.3658333718776703,\n",
      "                          0.3687340319156647,\n",
      "                          0.36549684405326843,\n",
      "                          0.35445287823677063,\n",
      "                          0.3628283143043518,\n",
      "                          0.3577992618083954,\n",
      "                          0.3525288701057434,\n",
      "                          0.3320038616657257,\n",
      "                          0.35056832432746887,\n",
      "                          0.3355320990085602,\n",
      "                          0.3330678939819336,\n",
      "                          0.32424497604370117,\n",
      "                          0.32678040862083435,\n",
      "                          0.33090338110923767,\n",
      "                          0.31145989894866943,\n",
      "                          0.3336937427520752,\n",
      "                          0.32262879610061646,\n",
      "                          0.3156208395957947,\n",
      "                          0.3125792145729065,\n",
      "                          0.3070051670074463,\n",
      "                          0.31957197189331055,\n",
      "                          0.31358203291893005,\n",
      "                          0.3158082067966461,\n",
      "                          0.3044150769710541,\n",
      "                          0.2901325821876526,\n",
      "                          0.30092915892601013,\n",
      "                          0.2931429445743561,\n",
      "                          0.2909145951271057,\n",
      "                          0.2724916636943817,\n",
      "                          0.27958670258522034,\n",
      "                          0.26762932538986206,\n",
      "                          0.2729882299900055,\n",
      "                          0.265787273645401,\n",
      "                          0.2755705714225769,\n",
      "                          0.2554006278514862,\n",
      "                          0.27802950143814087,\n",
      "                          0.2570156753063202,\n",
      "                          0.25897595286369324,\n",
      "                          0.2613420784473419,\n",
      "                          0.26912355422973633],\n",
      "             'val_precision': [0.6055743098258972,\n",
      "                               0.6500445008277893,\n",
      "                               0.7723480463027954,\n",
      "                               0.8138364553451538,\n",
      "                               0.8737446069717407,\n",
      "                               0.8317757248878479,\n",
      "                               0.8827258348464966,\n",
      "                               0.8498023748397827,\n",
      "                               0.8966074585914612,\n",
      "                               0.8844984769821167,\n",
      "                               0.8604651093482971,\n",
      "                               0.8600543737411499,\n",
      "                               0.8846153616905212,\n",
      "                               0.8792613744735718,\n",
      "                               0.9074355363845825,\n",
      "                               0.8864569067955017,\n",
      "                               0.9013453125953674,\n",
      "                               0.8836565017700195,\n",
      "                               0.8930555582046509,\n",
      "                               0.892503559589386,\n",
      "                               0.9010189175605774,\n",
      "                               0.9076704382896423,\n",
      "                               0.8817204236984253,\n",
      "                               0.9094827771186829,\n",
      "                               0.8932432532310486,\n",
      "                               0.882758617401123,\n",
      "                               0.9399141669273376,\n",
      "                               0.8961039185523987,\n",
      "                               0.8798882961273193,\n",
      "                               0.9020978808403015,\n",
      "                               0.9298998713493347,\n",
      "                               0.9033148884773254,\n",
      "                               0.8739612102508545,\n",
      "                               0.9289940595626831,\n",
      "                               0.8919667601585388,\n",
      "                               0.9182209372520447,\n",
      "                               0.9074829816818237,\n",
      "                               0.9208633303642273,\n",
      "                               0.8897958993911743,\n",
      "                               0.90625,\n",
      "                               0.9183400273323059,\n",
      "                               0.9196676015853882,\n",
      "                               0.9329685568809509,\n",
      "                               0.9362881183624268,\n",
      "                               0.9412628412246704,\n",
      "                               0.9302650094032288,\n",
      "                               0.9356643557548523,\n",
      "                               0.9223300814628601,\n",
      "                               0.9408283829689026,\n",
      "                               0.9351198673248291,\n",
      "                               0.9403409361839294,\n",
      "                               0.9228650331497192],\n",
      "             'val_recall': [1.0,\n",
      "                            0.9945504069328308,\n",
      "                            0.9178470373153687,\n",
      "                            0.8790760636329651,\n",
      "                            0.844660222530365,\n",
      "                            0.8640776872634888,\n",
      "                            0.769336998462677,\n",
      "                            0.8884297609329224,\n",
      "                            0.7602739930152893,\n",
      "                            0.8162692785263062,\n",
      "                            0.8784916400909424,\n",
      "                            0.8755186796188354,\n",
      "                            0.8821917772293091,\n",
      "                            0.8491083383560181,\n",
      "                            0.8340306878089905,\n",
      "                            0.8721399903297424,\n",
      "                            0.8305785059928894,\n",
      "                            0.8910614252090454,\n",
      "                            0.8748299479484558,\n",
      "                            0.8667582273483276,\n",
      "                            0.8433242440223694,\n",
      "                            0.8729507923126221,\n",
      "                            0.8986301422119141,\n",
      "                            0.8803894519805908,\n",
      "                            0.8932432532310486,\n",
      "                            0.892608106136322,\n",
      "                            0.8713527917861938,\n",
      "                            0.8601108193397522,\n",
      "                            0.8999999761581421,\n",
      "                            0.8835616707801819,\n",
      "                            0.8678237795829773,\n",
      "                            0.8983516693115234,\n",
      "                            0.9053084850311279,\n",
      "                            0.8521031141281128,\n",
      "                            0.9121813178062439,\n",
      "                            0.8815426826477051,\n",
      "                            0.9025710225105286,\n",
      "                            0.9026798009872437,\n",
      "                            0.9198312163352966,\n",
      "                            0.9112021923065186,\n",
      "                            0.9295393228530884,\n",
      "                            0.9222221970558167,\n",
      "                            0.9216216206550598,\n",
      "                            0.9147496819496155,\n",
      "                            0.8990182280540466,\n",
      "                            0.9050203561782837,\n",
      "                            0.9304589629173279,\n",
      "                            0.9134615659713745,\n",
      "                            0.9021276831626892,\n",
      "                            0.919556200504303,\n",
      "                            0.9068493247032166,\n",
      "                            0.9383753538131714]},\n",
      " 'optimal_threshold': 0.6758792400360107,\n",
      " 'train_counts': {'fire': 1485, 'nofire': 1401},\n",
      " 'train_counts_total': 2886,\n",
      " 'train_dataset_size': 5344,\n",
      " 'training_time': 721.0012540817261,\n",
      " 'val_counts': {'fire': 156, 'nofire': 246},\n",
      " 'val_counts_total': 402,\n",
      " 'val_dataset_size': 1184}\n",
      "Training model: MobileNetV3Small on dataset: DeepFire_FIRE\n",
      "Epoch 1/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 96ms/step - accuracy: 0.7411 - auc: 0.8098 - f1_score: 0.7324 - loss: 0.5866 - precision: 0.7327 - recall: 0.7418 - val_accuracy: 0.5071 - val_auc: 0.9298 - val_f1_score: 0.6693 - val_loss: 0.6811 - val_precision: 0.5071 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - accuracy: 0.8220 - auc: 0.8979 - f1_score: 0.8217 - loss: 0.4277 - precision: 0.8158 - recall: 0.8322 - val_accuracy: 0.5101 - val_auc: 0.9496 - val_f1_score: 0.6623 - val_loss: 0.6373 - val_precision: 0.5005 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 86ms/step - accuracy: 0.8433 - auc: 0.9230 - f1_score: 0.8450 - loss: 0.3636 - precision: 0.8593 - recall: 0.8337 - val_accuracy: 0.6442 - val_auc: 0.9502 - val_f1_score: 0.7268 - val_loss: 0.5636 - val_precision: 0.5791 - val_recall: 0.9959 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.8584 - auc: 0.9274 - f1_score: 0.8565 - loss: 0.3490 - precision: 0.8503 - recall: 0.8687 - val_accuracy: 0.8569 - val_auc: 0.9664 - val_f1_score: 0.8727 - val_loss: 0.4169 - val_precision: 0.7888 - val_recall: 0.9801 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 69ms/step - accuracy: 0.8647 - auc: 0.9377 - f1_score: 0.8642 - loss: 0.3229 - precision: 0.8629 - recall: 0.8672 - val_accuracy: 0.9073 - val_auc: 0.9699 - val_f1_score: 0.9070 - val_loss: 0.2985 - val_precision: 0.8762 - val_recall: 0.9473 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 72ms/step - accuracy: 0.8638 - auc: 0.9357 - f1_score: 0.8629 - loss: 0.3269 - precision: 0.8699 - recall: 0.8599 - val_accuracy: 0.9163 - val_auc: 0.9713 - val_f1_score: 0.9141 - val_loss: 0.2397 - val_precision: 0.9104 - val_recall: 0.9232 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 72ms/step - accuracy: 0.8733 - auc: 0.9417 - f1_score: 0.8718 - loss: 0.3102 - precision: 0.8605 - recall: 0.8872 - val_accuracy: 0.9153 - val_auc: 0.9740 - val_f1_score: 0.9121 - val_loss: 0.2206 - val_precision: 0.9229 - val_recall: 0.9041 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 75ms/step - accuracy: 0.8724 - auc: 0.9421 - f1_score: 0.8737 - loss: 0.3127 - precision: 0.8781 - recall: 0.8727 - val_accuracy: 0.9183 - val_auc: 0.9768 - val_f1_score: 0.9164 - val_loss: 0.2119 - val_precision: 0.9184 - val_recall: 0.9238 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - accuracy: 0.8795 - auc: 0.9465 - f1_score: 0.8784 - loss: 0.2952 - precision: 0.8694 - recall: 0.8881 - val_accuracy: 0.9264 - val_auc: 0.9788 - val_f1_score: 0.9250 - val_loss: 0.2018 - val_precision: 0.9217 - val_recall: 0.9310 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.8803 - auc: 0.9439 - f1_score: 0.8781 - loss: 0.3033 - precision: 0.8790 - recall: 0.8818 - val_accuracy: 0.9183 - val_auc: 0.9769 - val_f1_score: 0.9141 - val_loss: 0.2131 - val_precision: 0.9372 - val_recall: 0.8928 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - accuracy: 0.8645 - auc: 0.9369 - f1_score: 0.8650 - loss: 0.3231 - precision: 0.8670 - recall: 0.8669 - val_accuracy: 0.9274 - val_auc: 0.9810 - val_f1_score: 0.9278 - val_loss: 0.2051 - val_precision: 0.9248 - val_recall: 0.9286 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 78ms/step - accuracy: 0.8751 - auc: 0.9461 - f1_score: 0.8756 - loss: 0.3012 - precision: 0.8855 - recall: 0.8698 - val_accuracy: 0.9194 - val_auc: 0.9793 - val_f1_score: 0.9182 - val_loss: 0.2033 - val_precision: 0.9254 - val_recall: 0.9143 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.8923 - auc: 0.9556 - f1_score: 0.8932 - loss: 0.2701 - precision: 0.8925 - recall: 0.8969 - val_accuracy: 0.9385 - val_auc: 0.9850 - val_f1_score: 0.9362 - val_loss: 0.1837 - val_precision: 0.9443 - val_recall: 0.9309 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 79ms/step - accuracy: 0.8676 - auc: 0.9359 - f1_score: 0.8688 - loss: 0.3248 - precision: 0.8708 - recall: 0.8722 - val_accuracy: 0.9325 - val_auc: 0.9817 - val_f1_score: 0.9327 - val_loss: 0.1890 - val_precision: 0.9398 - val_recall: 0.9267 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8859 - auc: 0.9495 - f1_score: 0.8848 - loss: 0.2860 - precision: 0.8828 - recall: 0.8902 - val_accuracy: 0.9234 - val_auc: 0.9817 - val_f1_score: 0.9206 - val_loss: 0.1923 - val_precision: 0.9206 - val_recall: 0.9280 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 81ms/step - accuracy: 0.8884 - auc: 0.9548 - f1_score: 0.8869 - loss: 0.2722 - precision: 0.8801 - recall: 0.8980 - val_accuracy: 0.9425 - val_auc: 0.9851 - val_f1_score: 0.9412 - val_loss: 0.1769 - val_precision: 0.9414 - val_recall: 0.9433 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.8898 - auc: 0.9557 - f1_score: 0.8891 - loss: 0.2696 - precision: 0.8875 - recall: 0.8935 - val_accuracy: 0.9204 - val_auc: 0.9818 - val_f1_score: 0.9195 - val_loss: 0.1916 - val_precision: 0.9333 - val_recall: 0.9094 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 89ms/step - accuracy: 0.8893 - auc: 0.9552 - f1_score: 0.8872 - loss: 0.2704 - precision: 0.8898 - recall: 0.8886 - val_accuracy: 0.9375 - val_auc: 0.9846 - val_f1_score: 0.9343 - val_loss: 0.1764 - val_precision: 0.9329 - val_recall: 0.9441 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 91ms/step - accuracy: 0.8931 - auc: 0.9619 - f1_score: 0.8889 - loss: 0.2499 - precision: 0.8858 - recall: 0.8979 - val_accuracy: 0.9456 - val_auc: 0.9872 - val_f1_score: 0.9456 - val_loss: 0.1653 - val_precision: 0.9316 - val_recall: 0.9617 - learning_rate: 0.0010\n",
      "Epoch 20/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 90ms/step - accuracy: 0.8901 - auc: 0.9585 - f1_score: 0.8875 - loss: 0.2605 - precision: 0.8958 - recall: 0.8809 - val_accuracy: 0.9375 - val_auc: 0.9880 - val_f1_score: 0.9388 - val_loss: 0.1686 - val_precision: 0.9419 - val_recall: 0.9382 - learning_rate: 0.0010\n",
      "Epoch 21/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 90ms/step - accuracy: 0.8826 - auc: 0.9544 - f1_score: 0.8802 - loss: 0.2739 - precision: 0.8799 - recall: 0.8845 - val_accuracy: 0.9355 - val_auc: 0.9872 - val_f1_score: 0.9337 - val_loss: 0.1674 - val_precision: 0.9435 - val_recall: 0.9286 - learning_rate: 0.0010\n",
      "Epoch 22/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 93ms/step - accuracy: 0.8951 - auc: 0.9592 - f1_score: 0.8937 - loss: 0.2580 - precision: 0.8883 - recall: 0.9043 - val_accuracy: 0.9496 - val_auc: 0.9912 - val_f1_score: 0.9505 - val_loss: 0.1505 - val_precision: 0.9318 - val_recall: 0.9723 - learning_rate: 0.0010\n",
      "Epoch 23/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 98ms/step - accuracy: 0.8868 - auc: 0.9590 - f1_score: 0.8845 - loss: 0.2596 - precision: 0.8784 - recall: 0.8965 - val_accuracy: 0.9425 - val_auc: 0.9879 - val_f1_score: 0.9438 - val_loss: 0.1643 - val_precision: 0.9510 - val_recall: 0.9381 - learning_rate: 0.0010\n",
      "Epoch 24/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 91ms/step - accuracy: 0.8845 - auc: 0.9531 - f1_score: 0.8828 - loss: 0.2782 - precision: 0.8812 - recall: 0.8864 - val_accuracy: 0.9375 - val_auc: 0.9874 - val_f1_score: 0.9344 - val_loss: 0.1657 - val_precision: 0.9411 - val_recall: 0.9335 - learning_rate: 0.0010\n",
      "Epoch 25/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 90ms/step - accuracy: 0.8906 - auc: 0.9593 - f1_score: 0.8910 - loss: 0.2578 - precision: 0.8903 - recall: 0.8950 - val_accuracy: 0.9415 - val_auc: 0.9846 - val_f1_score: 0.9409 - val_loss: 0.1714 - val_precision: 0.9494 - val_recall: 0.9343 - learning_rate: 0.0010\n",
      "Epoch 26/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 95ms/step - accuracy: 0.8903 - auc: 0.9519 - f1_score: 0.8900 - loss: 0.2826 - precision: 0.8947 - recall: 0.8913 - val_accuracy: 0.9526 - val_auc: 0.9913 - val_f1_score: 0.9485 - val_loss: 0.1559 - val_precision: 0.9403 - val_recall: 0.9621 - learning_rate: 0.0010\n",
      "Epoch 27/80\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9050 - auc: 0.9651 - f1_score: 0.9046 - loss: 0.2385 - precision: 0.9032 - recall: 0.9103\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 93ms/step - accuracy: 0.9050 - auc: 0.9651 - f1_score: 0.9046 - loss: 0.2385 - precision: 0.9032 - recall: 0.9103 - val_accuracy: 0.9435 - val_auc: 0.9899 - val_f1_score: 0.9372 - val_loss: 0.1549 - val_precision: 0.9457 - val_recall: 0.9379 - learning_rate: 0.0010\n",
      "Training time: 310.04 seconds\n",
      "Evaluating MobileNetV3Small on DeepFire_FIRE...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 137ms/step - accuracy: 0.8594 - auc: 0.6678 - f1_score: 0.3366 - loss: 0.3145 - precision: 0.6155 - recall: 0.5912        \n",
      "Training results:\n",
      "{'evaluation': {'accuracy': 0.8229166865348816,\n",
      "                'auc': 0.9190760850906372,\n",
      "                'f1_score': 0.5204816460609436,\n",
      "                'loss': 0.38176214694976807,\n",
      "                'precision': 0.8666666746139526,\n",
      "                'recall': 0.7799999713897705},\n",
      " 'history': {'accuracy': [0.7897499799728394,\n",
      "                          0.8309999704360962,\n",
      "                          0.8422499895095825,\n",
      "                          0.859250009059906,\n",
      "                          0.8615000247955322,\n",
      "                          0.8577499985694885,\n",
      "                          0.8634999990463257,\n",
      "                          0.8744999766349792,\n",
      "                          0.878250002861023,\n",
      "                          0.8794999718666077,\n",
      "                          0.8694999814033508,\n",
      "                          0.8802499771118164,\n",
      "                          0.8805000185966492,\n",
      "                          0.8774999976158142,\n",
      "                          0.8852499723434448,\n",
      "                          0.8867499828338623,\n",
      "                          0.8889999985694885,\n",
      "                          0.8882499933242798,\n",
      "                          0.890999972820282,\n",
      "                          0.8889999985694885,\n",
      "                          0.8845000267028809,\n",
      "                          0.8957499861717224,\n",
      "                          0.8939999938011169,\n",
      "                          0.8834999799728394,\n",
      "                          0.8924999833106995,\n",
      "                          0.8899999856948853,\n",
      "                          0.9037500023841858],\n",
      "             'auc': [0.861463725566864,\n",
      "                     0.9073851704597473,\n",
      "                     0.9219383597373962,\n",
      "                     0.9269737601280212,\n",
      "                     0.935308575630188,\n",
      "                     0.9346216320991516,\n",
      "                     0.9358500242233276,\n",
      "                     0.9443288445472717,\n",
      "                     0.9473232626914978,\n",
      "                     0.949177622795105,\n",
      "                     0.9421097040176392,\n",
      "                     0.9500615000724792,\n",
      "                     0.9489395618438721,\n",
      "                     0.9442367553710938,\n",
      "                     0.9505137801170349,\n",
      "                     0.9527161717414856,\n",
      "                     0.9563244581222534,\n",
      "                     0.9540141224861145,\n",
      "                     0.9598191976547241,\n",
      "                     0.959036111831665,\n",
      "                     0.9559758305549622,\n",
      "                     0.9605576992034912,\n",
      "                     0.9594681262969971,\n",
      "                     0.9542034864425659,\n",
      "                     0.9604840874671936,\n",
      "                     0.9564926624298096,\n",
      "                     0.9647611379623413],\n",
      "             'f1_score': [0.7837592959403992,\n",
      "                          0.8306447267532349,\n",
      "                          0.8427923321723938,\n",
      "                          0.8573136925697327,\n",
      "                          0.860951840877533,\n",
      "                          0.8549768924713135,\n",
      "                          0.8627375364303589,\n",
      "                          0.8743287920951843,\n",
      "                          0.8779736161231995,\n",
      "                          0.8778347969055176,\n",
      "                          0.8688213229179382,\n",
      "                          0.8787117600440979,\n",
      "                          0.8791529536247253,\n",
      "                          0.8752421140670776,\n",
      "                          0.884112536907196,\n",
      "                          0.8853859901428223,\n",
      "                          0.8883002400398254,\n",
      "                          0.8870291709899902,\n",
      "                          0.8889283537864685,\n",
      "                          0.8867148160934448,\n",
      "                          0.8827873468399048,\n",
      "                          0.8942065238952637,\n",
      "                          0.8914113640785217,\n",
      "                          0.8819724917411804,\n",
      "                          0.8919869661331177,\n",
      "                          0.8882938623428345,\n",
      "                          0.9037604928016663],\n",
      "             'learning_rate': [0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513],\n",
      "             'loss': [0.49415498971939087,\n",
      "                      0.40266403555870056,\n",
      "                      0.3646647036075592,\n",
      "                      0.34940075874328613,\n",
      "                      0.3296186625957489,\n",
      "                      0.3283301889896393,\n",
      "                      0.326376736164093,\n",
      "                      0.30214646458625793,\n",
      "                      0.2925201952457428,\n",
      "                      0.28825122117996216,\n",
      "                      0.3076375722885132,\n",
      "                      0.28633660078048706,\n",
      "                      0.2891562283039093,\n",
      "                      0.3021632134914398,\n",
      "                      0.2837640345096588,\n",
      "                      0.27744725346565247,\n",
      "                      0.2675856351852417,\n",
      "                      0.2740315794944763,\n",
      "                      0.25624334812164307,\n",
      "                      0.2594631612300873,\n",
      "                      0.2687983810901642,\n",
      "                      0.25345781445503235,\n",
      "                      0.2574913203716278,\n",
      "                      0.2742714285850525,\n",
      "                      0.2542879581451416,\n",
      "                      0.26651403307914734,\n",
      "                      0.23958291113376617],\n",
      "             'precision': [0.7819404602050781,\n",
      "                           0.8238469362258911,\n",
      "                           0.8461918830871582,\n",
      "                           0.8543307185173035,\n",
      "                           0.8595617413520813,\n",
      "                           0.8589170575141907,\n",
      "                           0.8543307185173035,\n",
      "                           0.8678132891654968,\n",
      "                           0.8752484917640686,\n",
      "                           0.8740079402923584,\n",
      "                           0.8629191517829895,\n",
      "                           0.8768472671508789,\n",
      "                           0.8787878751754761,\n",
      "                           0.8728437423706055,\n",
      "                           0.8827037811279297,\n",
      "                           0.8800393342971802,\n",
      "                           0.8839505910873413,\n",
      "                           0.8856143951416016,\n",
      "                           0.888059675693512,\n",
      "                           0.8854271173477173,\n",
      "                           0.8801591396331787,\n",
      "                           0.8876900672912598,\n",
      "                           0.8876181244850159,\n",
      "                           0.8791812062263489,\n",
      "                           0.8881188035011292,\n",
      "                           0.8891649842262268,\n",
      "                           0.8987341523170471],\n",
      "             'recall': [0.7815650701522827,\n",
      "                        0.8411823511123657,\n",
      "                        0.8424421548843384,\n",
      "                        0.8666999340057373,\n",
      "                        0.8638638854026794,\n",
      "                        0.8584905862808228,\n",
      "                        0.8741188049316406,\n",
      "                        0.8834417462348938,\n",
      "                        0.8818227052688599,\n",
      "                        0.8854271173477173,\n",
      "                        0.8776329159736633,\n",
      "                        0.8860129714012146,\n",
      "                        0.883175253868103,\n",
      "                        0.8841737508773804,\n",
      "                        0.8884442448616028,\n",
      "                        0.8954477310180664,\n",
      "                        0.8954477310180664,\n",
      "                        0.8905072808265686,\n",
      "                        0.8942885994911194,\n",
      "                        0.890798807144165,\n",
      "                        0.8890004754066467,\n",
      "                        0.9059059023857117,\n",
      "                        0.9001513123512268,\n",
      "                        0.8871536254882812,\n",
      "                        0.8978978991508484,\n",
      "                        0.8918245434761047,\n",
      "                        0.9125061631202698],\n",
      "             'val_accuracy': [0.507056474685669,\n",
      "                              0.5100806355476379,\n",
      "                              0.6441532373428345,\n",
      "                              0.8568548560142517,\n",
      "                              0.9072580933570862,\n",
      "                              0.9163306355476379,\n",
      "                              0.9153226017951965,\n",
      "                              0.9183467626571655,\n",
      "                              0.9264112710952759,\n",
      "                              0.9183467626571655,\n",
      "                              0.9274193644523621,\n",
      "                              0.9193548560142517,\n",
      "                              0.9385080933570862,\n",
      "                              0.9324596524238586,\n",
      "                              0.9233871102333069,\n",
      "                              0.9425403475761414,\n",
      "                              0.9203628897666931,\n",
      "                              0.9375,\n",
      "                              0.9455645084381104,\n",
      "                              0.9375,\n",
      "                              0.9354838728904724,\n",
      "                              0.9495967626571655,\n",
      "                              0.9425403475761414,\n",
      "                              0.9375,\n",
      "                              0.9415322542190552,\n",
      "                              0.9526209831237793,\n",
      "                              0.9435483813285828],\n",
      "             'val_auc': [0.9297690391540527,\n",
      "                         0.9495842456817627,\n",
      "                         0.9501920938491821,\n",
      "                         0.9664202928543091,\n",
      "                         0.9699358344078064,\n",
      "                         0.9712516665458679,\n",
      "                         0.9739999175071716,\n",
      "                         0.976788341999054,\n",
      "                         0.978789210319519,\n",
      "                         0.9768661856651306,\n",
      "                         0.9810370206832886,\n",
      "                         0.9792503714561462,\n",
      "                         0.9850101470947266,\n",
      "                         0.981696367263794,\n",
      "                         0.9816972017288208,\n",
      "                         0.9851226806640625,\n",
      "                         0.9817812442779541,\n",
      "                         0.9845848083496094,\n",
      "                         0.9872183799743652,\n",
      "                         0.9879934191703796,\n",
      "                         0.9872373938560486,\n",
      "                         0.9912124872207642,\n",
      "                         0.9878713488578796,\n",
      "                         0.987389087677002,\n",
      "                         0.9846328496932983,\n",
      "                         0.9912531971931458,\n",
      "                         0.9899042844772339],\n",
      "             'val_f1_score': [0.669287919998169,\n",
      "                              0.66234290599823,\n",
      "                              0.7268174290657043,\n",
      "                              0.8726719617843628,\n",
      "                              0.9070197939872742,\n",
      "                              0.9140814542770386,\n",
      "                              0.9121325016021729,\n",
      "                              0.9164087772369385,\n",
      "                              0.9249825477600098,\n",
      "                              0.9141205549240112,\n",
      "                              0.9278128743171692,\n",
      "                              0.9182217717170715,\n",
      "                              0.9361668825149536,\n",
      "                              0.93268883228302,\n",
      "                              0.9206477999687195,\n",
      "                              0.9411813020706177,\n",
      "                              0.9195164442062378,\n",
      "                              0.9342941045761108,\n",
      "                              0.9456307888031006,\n",
      "                              0.9387897253036499,\n",
      "                              0.9336527585983276,\n",
      "                              0.9504825472831726,\n",
      "                              0.9438108801841736,\n",
      "                              0.9343780875205994,\n",
      "                              0.9408685564994812,\n",
      "                              0.9485496282577515,\n",
      "                              0.9371696710586548],\n",
      "             'val_loss': [0.681052029132843,\n",
      "                          0.6373403072357178,\n",
      "                          0.5635687708854675,\n",
      "                          0.4169093370437622,\n",
      "                          0.29846152663230896,\n",
      "                          0.2397337406873703,\n",
      "                          0.22064530849456787,\n",
      "                          0.2118677943944931,\n",
      "                          0.20179466903209686,\n",
      "                          0.21310541033744812,\n",
      "                          0.2051149159669876,\n",
      "                          0.20333850383758545,\n",
      "                          0.1836697906255722,\n",
      "                          0.18897004425525665,\n",
      "                          0.19233687222003937,\n",
      "                          0.1769322007894516,\n",
      "                          0.19158707559108734,\n",
      "                          0.17638522386550903,\n",
      "                          0.16530945897102356,\n",
      "                          0.1686006486415863,\n",
      "                          0.16737902164459229,\n",
      "                          0.15046902000904083,\n",
      "                          0.16433528065681458,\n",
      "                          0.16573284566402435,\n",
      "                          0.17139264941215515,\n",
      "                          0.155887633562088,\n",
      "                          0.15487320721149445],\n",
      "             'val_precision': [0.507056474685669,\n",
      "                               0.5005138516426086,\n",
      "                               0.5791366696357727,\n",
      "                               0.7888000011444092,\n",
      "                               0.8761726021766663,\n",
      "                               0.9103585481643677,\n",
      "                               0.9229166507720947,\n",
      "                               0.9184466004371643,\n",
      "                               0.9216867685317993,\n",
      "                               0.9372294545173645,\n",
      "                               0.9247967600822449,\n",
      "                               0.9254032373428345,\n",
      "                               0.9443299174308777,\n",
      "                               0.9397590160369873,\n",
      "                               0.920634925365448,\n",
      "                               0.9414141178131104,\n",
      "                               0.9333333373069763,\n",
      "                               0.9329388737678528,\n",
      "                               0.931640625,\n",
      "                               0.9418604373931885,\n",
      "                               0.9435483813285828,\n",
      "                               0.9318181872367859,\n",
      "                               0.9509803652763367,\n",
      "                               0.9410569071769714,\n",
      "                               0.9493927359580994,\n",
      "                               0.9403291940689087,\n",
      "                               0.9457202553749084],\n",
      "             'val_recall': [1.0,\n",
      "                            1.0,\n",
      "                            0.9958763122558594,\n",
      "                            0.9801192879676819,\n",
      "                            0.9472616910934448,\n",
      "                            0.9232323169708252,\n",
      "                            0.9040816426277161,\n",
      "                            0.923828125,\n",
      "                            0.931034505367279,\n",
      "                            0.892783522605896,\n",
      "                            0.9285714030265808,\n",
      "                            0.9143426418304443,\n",
      "                            0.9308943152427673,\n",
      "                            0.9267326593399048,\n",
      "                            0.9279999732971191,\n",
      "                            0.9433198571205139,\n",
      "                            0.9094488024711609,\n",
      "                            0.9441117644309998,\n",
      "                            0.961693525314331,\n",
      "                            0.9382239580154419,\n",
      "                            0.9285714030265808,\n",
      "                            0.9723320007324219,\n",
      "                            0.9381044507026672,\n",
      "                            0.9334677457809448,\n",
      "                            0.9342629313468933,\n",
      "                            0.9621052742004395,\n",
      "                            0.9378882050514221]},\n",
      " 'optimal_threshold': 0.20315159857273102,\n",
      " 'train_counts': {'fire': 1515, 'nofire': 1004},\n",
      " 'train_counts_total': 2519,\n",
      " 'train_dataset_size': 4000,\n",
      " 'training_time': 310.04219675064087,\n",
      " 'val_counts': {'fire': 0, 'nofire': 0},\n",
      " 'val_counts_total': 0,\n",
      " 'val_dataset_size': 992}\n",
      "Training model: MobileNetV3Small on dataset: The Wildfire Dataset_DeepFire_FIRE\n",
      "Epoch 1/80\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.6653 - auc: 0.7497 - f1_score: 0.7088 - loss: 0.6990 - precision: 0.7634 - recall: 0.6943"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initial training of the model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Record the end time\u001b[39;00m\n\u001b[0;32m     28\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:392\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    383\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m    384\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    391\u001b[0m     )\n\u001b[1;32m--> 392\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    404\u001b[0m }\n\u001b[0;32m    405\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:481\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    480\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m--> 481\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    482\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_evaluating:\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_results = {}\n",
    "results_file = os.path.join(run_dir, 'training_results.json')\n",
    "\n",
    "for base_model, custom_bool in zip(all_models, is_custom_model):\n",
    "    model = generate_model(base_model, custom=custom_bool, to_dir=run_dir) # To display the model summary\n",
    "    model.summary()\n",
    "    model_dir = os.path.join(run_dir, model.name)\n",
    "    training_results[model.name] = {}\n",
    "    plot_model(model, show_shapes=True, show_layer_names=True, to_file=os.path.join(model_dir, f\"{model.name}_architecture.png\"))\n",
    "    for dataset_id, train_dataset, val_dataset, steps_per_epoch, validation_steps, train_counts_dict, val_counts_dict in training_params:\n",
    "        model.load_weights(os.path.join(run_dir, model.name, f\"{model.name}_initial.weights.h5\"))\n",
    "        print(f\"Training model: {model.name} on dataset: {dataset_id}\")\n",
    "        \n",
    "        # Record the start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Initial training of the model\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=val_dataset,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks_list\n",
    "        )\n",
    "\n",
    "        # Record the end time\n",
    "        end_time = time.time()\n",
    "        # Calculate the training time\n",
    "        training_time = end_time - start_time\n",
    "        print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "        model_ds_dir = os.path.join(model_dir, dataset_id)\n",
    "        os.makedirs(model_ds_dir, exist_ok=True)\n",
    "        # Save the model\n",
    "        model.save(os.path.join(model_ds_dir, f\"{model.name}_{dataset_id}.keras\"))\n",
    "\n",
    "        ### Evaluation stage ###\n",
    "        optimal_threshold = full_eval(model_ds_dir, history, model, dataset_id, test_dataset, true_labels, test_steps)\n",
    "        \n",
    "        training_results[model.name][dataset_id] = {\n",
    "            'history': history.history,\n",
    "            'training_time': training_time,\n",
    "            'optimal_threshold': float(optimal_threshold),\n",
    "            'train_dataset_size': steps_per_epoch * batch_size, # Includes augmented data (2x)\n",
    "            'val_dataset_size': validation_steps * batch_size, # Includes augmented data (2x)\n",
    "            'train_counts': train_counts_dict,\n",
    "            'val_counts': val_counts_dict,\n",
    "            'train_counts_total': sum(train_counts_dict.values()),\n",
    "            'val_counts_total': sum(val_counts_dict.values()),\n",
    "            \"evaluation\": model.evaluate(test_dataset, return_dict=True, steps=test_steps)\n",
    "        }\n",
    "        print(\"Training results:\")\n",
    "        pprint(training_results[model.name][dataset_id])\n",
    "        # Save the training results to a file after each iteration\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(training_results, f, indent=4)\n",
    "        \n",
    "        model.compile(optimizer=optimizer_fn, loss=loss_fn, metrics=metrics_list) # Reset the model for the next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brute force loop completed!\n",
      "All models and evaluations are available at: runs\\run_11\n"
     ]
    }
   ],
   "source": [
    "print(\"Brute force loop completed!\")\n",
    "print(f\"All models are now available at: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = os.path.join(run_dir, \"evaluations\")\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "rows = extract_evaluation_data(training_results)\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(os.path.join(eval_dir, \"training_data.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_chart(df, \"Evaluation F1 Score\", eval_dir)\n",
    "plot_metric_chart(df, \"Evaluation Accuracy\", eval_dir)\n",
    "plot_metric_chart(df, \"Evaluation Precision\", eval_dir)\n",
    "plot_metric_chart(df, \"Evaluation Recall\", eval_dir)\n",
    "plot_metric_chart(df, \"Evaluation AUC\", eval_dir)\n",
    "plot_metric_chart(df, \"Training Time\", eval_dir)\n",
    "plot_metric_chart(df, \"Train Size\", eval_dir)\n",
    "plot_metric_chart(df, \"Val Size\", eval_dir)\n",
    "\n",
    "plot_time_extrapolation(df, eval_dir)\n",
    "\n",
    "print(\"All evaluations completed!\")\n",
    "print(f\"Results are available at: {eval_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1351460,
     "sourceId": 2247205,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
