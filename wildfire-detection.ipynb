{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Brute-Force Training and Evaluation Pipeline for Datasets and Models - by Selman Tabet @ https://selman.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import socket\n",
    "\n",
    "TEMP_DIR = \"tmp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostname:  Chaos\n"
     ]
    }
   ],
   "source": [
    "print(\"Hostname: \", socket.gethostname())\n",
    "try: # for CUDA enviroment\n",
    "    os.system(\"nvidia-smi\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Data processing libraries\n",
    "import numpy as np\n",
    "from itertools import combinations # For brute force combinatoric search\n",
    "import json # For saving and loading training results\n",
    "import argparse # For command line arguments\n",
    "\n",
    "# Tensorflow-Keras ML libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model # To plot model architecture\n",
    "\n",
    "from IPython import get_ipython # To check if code is running in Jupyter notebook\n",
    "import importlib.util # To import config module from str\n",
    "from pprint import pprint # To show config\n",
    "\n",
    "# Custom helper libraries\n",
    "from notebook_cfg import * # Default parameters\n",
    "from utils.img_processing import enforce_image_params\n",
    "from utils.dataset_processors import * # Dataset and generator processing functions\n",
    "from utils.plot_functions import * # Plotting functions\n",
    "from utils.evaluator import * # Complete evaluation program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: None\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES')\n",
    "print(f\"CUDA_VISIBLE_DEVICES: {cuda_visible_devices}\")\n",
    "print(tf.config.get_visible_devices())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse arguments from command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Python config file specified, using default (notebook) config.\n"
     ]
    }
   ],
   "source": [
    "# Detect if running in a Jupyter notebook\n",
    "# Generated using GPT-4o. Prompt: \"Detect if running in a Jupyter notebook\"\n",
    "def in_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        else:\n",
    "            return False  # Other type (terminal, etc.)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "    \n",
    "from_py = False\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Parse command line arguments\")\n",
    "parser.add_argument('--from-py-cfg', type=str,\n",
    "                    help='Path to the config Python file')\n",
    "if not in_notebook():\n",
    "    args = parser.parse_args()\n",
    "    config_file_path = args.from_py_cfg\n",
    "    print(f\"Python Config Path: {config_file_path}\")\n",
    "else:\n",
    "    config_file_path = False\n",
    "\n",
    "if config_file_path:\n",
    "    spec = importlib.util.spec_from_file_location(\"config_module\", config_file_path)\n",
    "    config_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(config_module)\n",
    "    config = config_module.cfg\n",
    "    print(\"Loaded config from Python file:\")\n",
    "    pprint(config)\n",
    "    # Datasets, models, and hyperparameters are mandatory and must be processed now.\n",
    "    training_datasets = config.get('datasets', {})\n",
    "    full_test_dir = config.get('test')\n",
    "    base_models = config.get('keras_models', [])\n",
    "    custom_models = config.get('custom_models', [])\n",
    "    hyperparameters = config.get('hyperparameters')\n",
    "    default_hyperparameters = default_cfg.get('hyperparameters', {})\n",
    "    if hyperparameters is None or len(hyperparameters) == 0:\n",
    "        print(\"No training hyperparameters defined in config, using defaults.\")\n",
    "        hyperparameters = default_hyperparameters\n",
    "    else:\n",
    "        for key, value in default_hyperparameters.items():\n",
    "            if key not in hyperparameters:\n",
    "                print(f\"Missing hyperparameter - falling back to default {key}:{default_hyperparameters[key]}\")\n",
    "                hyperparameters[key] = default_hyperparameters[key]\n",
    "    from_py = True # Successfully completed the import\n",
    "else:\n",
    "    print(\"No Python config file specified, using default (notebook) config.\")\n",
    "    config = default_cfg\n",
    "    training_datasets = config.get('datasets', {})\n",
    "    base_models = config.get('keras_models', [])\n",
    "    custom_models = config.get('custom_models', [])\n",
    "    hyperparameters = config.get('hyperparameters', {\"epochs\": 50, \"batch_size\": 32})\n",
    "    full_test_dir = config.get('test')\n",
    "\n",
    "if training_datasets is None or len(training_datasets) == 0:\n",
    "    raise ValueError(\"No train datasets defined in config.\")\n",
    "\n",
    "if base_models is None or len(base_models) == 0:\n",
    "    if custom_models is None or len(custom_models) == 0:\n",
    "        raise ValueError(\"No models defined in config.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dirs = [training_datasets[ds].get('train') for ds in training_datasets]\n",
    "test_dirs = [training_datasets[ds].get('test') for ds in training_datasets]\n",
    "val_dirs = [training_datasets[ds].get('val') for ds in training_datasets]\n",
    "\n",
    "all_dirs = train_dirs + test_dirs + val_dirs + [full_test_dir]\n",
    "all_dirs = [d for d in all_dirs if d is not None] # Remove None values\n",
    "\n",
    "# Combine base_models and custom_models\n",
    "all_models = base_models + custom_models\n",
    "# Create a list to keep track of which models are custom\n",
    "is_custom_model = [False] * len(base_models) + [True] * len(custom_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if from_py:\n",
    "    epochs = hyperparameters.get('epochs') # Guaranteed to be present\n",
    "    batch_size = hyperparameters.get('batch_size') # Guaranteed to be present\n",
    "    img_height = config.get('image_height', default_cfg.get('image_height'))\n",
    "    img_width = config.get('image_width', default_cfg.get('image_width'))\n",
    "    optimizer_fn = config.get('optimizer', default_cfg.get('optimizer'))\n",
    "    loss_fn = config.get('loss', default_cfg.get('loss'))\n",
    "    callbacks_list = config.get('callbacks', default_cfg.get('callbacks'))\n",
    "    metrics_list = config.get('metrics', default_cfg.get('metrics'))\n",
    "    enforce_image_size = config.get('enforce_image_settings', default_cfg.get('enforce_image_settings'))\n",
    "    val_size = config.get('val_size', default_cfg.get('val_size'))\n",
    "else:\n",
    "    epochs = hyperparameters.get('epochs', 50)\n",
    "    batch_size = hyperparameters.get('batch_size', 32)\n",
    "    img_height = default_cfg.get('image_height', 224)\n",
    "    img_width = default_cfg.get('image_width', 224)\n",
    "    optimizer_fn = default_cfg.get('optimizer', 'adam')\n",
    "    loss_fn = default_cfg.get('loss', 'binary_crossentropy')\n",
    "    callbacks_list = default_cfg.get('callbacks', [])\n",
    "    metrics_list = default_cfg.get('metrics', ['accuracy'])\n",
    "    enforce_image_size = default_cfg.get('enforce_image_settings', False)\n",
    "    val_size = default_cfg.get('val_size', 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enforce defined resolution and colour mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting image properties in datasets\\dataset_1\\train\n",
      "Adjusting image properties in datasets\\dataset_2\\Training\n",
      "Adjusting image properties in datasets\\dataset_1\\test\n",
      "Adjusting image properties in datasets\\dataset_2\\Testing\n",
      "Adjusting image properties in datasets\\dataset_1\\val\n",
      "Adjusting image properties in datasets\\dataset_3\n"
     ]
    }
   ],
   "source": [
    "if enforce_image_size:\n",
    "    for directory in all_dirs:\n",
    "        print(f\"Adjusting image properties in {directory}\")\n",
    "        enforce_image_params(directory, target_size=(img_width, img_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: The Wildfire Dataset\n",
      "Augmenting The Wildfire Dataset\n",
      "Creating generators for training\n",
      "Found 1887 images belonging to 2 classes.\n",
      "Found 1887 images belonging to 2 classes.\n",
      "--------------------\n",
      "Number of samples in generator: 1887\n",
      "Number of classes: 2\n",
      "--------------------\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "Class names: ['fire', 'nofire']\n",
      "Dataset Class Counts:\n",
      "fire: 730\n",
      "nofire: 1157\n",
      "\n",
      "Augmented Dataset Class Counts:\n",
      "fire: 730\n",
      "nofire: 1157\n",
      "\n",
      "\n",
      "Combined Dataset Class Counts:\n",
      "fire: 1460\n",
      "nofire: 2314\n",
      "--------------------\n",
      "Creating generators for validation\n",
      "Found 402 images belonging to 2 classes.\n",
      "Found 402 images belonging to 2 classes.\n",
      "--------------------\n",
      "Number of samples in generator: 402\n",
      "Number of classes: 2\n",
      "--------------------\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "Class names: ['fire', 'nofire']\n",
      "Dataset Class Counts:\n",
      "fire: 156\n",
      "nofire: 246\n",
      "\n",
      "Augmented Dataset Class Counts:\n",
      "fire: 156\n",
      "nofire: 246\n",
      "\n",
      "\n",
      "Combined Dataset Class Counts:\n",
      "fire: 312\n",
      "nofire: 492\n",
      "--------------------\n",
      "Processing: DeepFire\n",
      "Augmenting DeepFire\n",
      "Creating generators for training\n",
      "Found 1520 images belonging to 2 classes.\n",
      "Found 1520 images belonging to 2 classes.\n",
      "--------------------\n",
      "Number of samples in generator: 1520\n",
      "Number of classes: 2\n",
      "--------------------\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "Class names: ['fire', 'nofire']\n",
      "Dataset Class Counts:\n",
      "fire: 760\n",
      "nofire: 760\n",
      "\n",
      "Augmented Dataset Class Counts:\n",
      "fire: 760\n",
      "nofire: 760\n",
      "\n",
      "\n",
      "Combined Dataset Class Counts:\n",
      "fire: 1520\n",
      "nofire: 1520\n",
      "--------------------\n",
      "No validation set, splitting training set.\n",
      "-------------------------\n",
      "--- Splitted dataset ---\n",
      "Training dataset size: 2432 samples\n",
      "Validation dataset size: 608 samples\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_names = []\n",
    "train_datasets = [] # [ (dataset_1_train, dataset_2_train), ... ]\n",
    "train_sizes = [] # [ (dataset_1_train_size, dataset_2_train_size), ... ]\n",
    "val_datasets = [] # [ (dataset_1_val, dataset_2_val), ... ]\n",
    "val_sizes = [] # [ (dataset_1_val_size, dataset_2_val_size), ... ]\n",
    "train_counts = [] # [ (dataset_1_train_counts, dataset_2_train_counts), ... ]\n",
    "val_counts = [] # [ (dataset_1_val_counts, dataset_2_val_counts), ... ]\n",
    "\n",
    "for d in training_datasets:\n",
    "    print(f\"Processing: {d}\")\n",
    "    train_dir = training_datasets[d].get('train')\n",
    "    augment = training_datasets[d].get('augment', True)\n",
    "    print(\"Augmenting\" if augment else \"Not augmenting\", d)\n",
    "    # Apply original and augmented data generators for training\n",
    "    print(\"Creating generators for training\")\n",
    "    train_generator, augmented_train_generator = create_generators(train_dir, batch_size=batch_size, augment=augment, img_width=img_width, img_height=img_height)\n",
    "    train_samples = samples_from_generators([train_generator, augmented_train_generator])\n",
    "    train_count_dict = class_counts_from_generators(train_generator, augmented_train_generator)\n",
    "    train_dataset = generators_to_dataset([train_generator, augmented_train_generator], batch_size=batch_size, img_height=img_height, img_width=img_width)\n",
    "    # Apply original and augmented data generators for validation\n",
    "    if \"val\" in training_datasets[d]:\n",
    "        val_dir = training_datasets[d]['val']\n",
    "        print(\"Creating generators for validation\")\n",
    "        val_generator, augmented_val_generator = create_generators(val_dir, batch_size=batch_size, augment=augment, shuffle=False, img_width=img_width, img_height=img_height)\n",
    "        val_samples = samples_from_generators([val_generator, augmented_val_generator])\n",
    "        val_count_dict = class_counts_from_generators(val_generator, augmented_val_generator)\n",
    "        val_dataset = generators_to_dataset([train_generator, augmented_train_generator], batch_size=batch_size, img_height=img_height, img_width=img_width)\n",
    "    else:\n",
    "        print(\"No validation set, splitting training set.\")\n",
    "        train_dataset, val_dataset, train_samples, val_samples = val_split(train_dataset, train_samples, val_size=val_size)\n",
    "        val_generator, augmented_val_generator = None, None\n",
    "        val_count_dict = {k: 0 for k in train_count_dict.keys()}\n",
    "    \n",
    "    # Calculate the number of samples for training and validation\n",
    "    train_sizes.append(train_samples)\n",
    "    val_sizes.append(val_samples)\n",
    "    \n",
    "    train_counts.append(train_count_dict)\n",
    "    val_counts.append(val_count_dict)\n",
    "    train_datasets.append(train_dataset)\n",
    "    val_datasets.append(val_dataset)\n",
    "    dataset_names.append(d)\n",
    "    \n",
    "# Ensure that the lengths are consistent across the board before continuing\n",
    "assert len(train_sizes) == len(train_datasets) == len(val_sizes) == len(val_datasets) == len(val_counts) == len(train_counts) == len(dataset_names), \"Dataset lengths are inconsistent.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute Force Combinatorial Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_combos = [] # [(0,), (1,), (0, 1), ...] where 0, 1 are the indices of the datasets within their respective lists\n",
    "for r in range(1, len(dataset_names) + 1):\n",
    "    dataset_combos.extend(combinations(range(len(dataset_names)), r))\n",
    "    \n",
    "combined_training_datasets = []\n",
    "combined_val_datasets = []\n",
    "combined_dataset_names = []\n",
    "steps_per_epoch_list = []\n",
    "validation_steps_list = []\n",
    "train_counts_list = []\n",
    "val_counts_list = []\n",
    "\n",
    "for combo in dataset_combos:\n",
    "    training_dataset = None\n",
    "    val_dataset = None\n",
    "    train_size = None\n",
    "    val_size = None\n",
    "    train_count = None\n",
    "    val_count = None\n",
    "    for idx in combo:\n",
    "        if training_dataset is None:\n",
    "            training_dataset = train_datasets[idx]\n",
    "            val_dataset = val_datasets[idx]\n",
    "            train_size = train_sizes[idx]\n",
    "            val_size = val_sizes[idx]\n",
    "            train_count = train_counts[idx]\n",
    "            val_count = val_counts[idx]\n",
    "        else:\n",
    "            training_dataset = training_dataset.concatenate(train_datasets[idx])\n",
    "            val_dataset = val_dataset.concatenate(val_datasets[idx])\n",
    "            train_size += train_sizes[idx]\n",
    "            val_size += val_sizes[idx]\n",
    "            train_count = {k: train_count.get(k, 0) + train_counts[idx].get(k, 0) for k in set(train_count) | set(train_counts[idx])}\n",
    "            val_count = {k: val_count.get(k, 0) + val_counts[idx].get(k, 0) for k in set(val_count) | set(val_counts[idx])}\n",
    "        train_count = {k: int(v) for k, v in train_count.items()}\n",
    "        val_count = {k: int(v) for k, v in val_count.items()}\n",
    "\n",
    "    combined_dataset_names.append(\"_\".join([dataset_names[idx] for idx in combo]))\n",
    "    combined_training_datasets.append(training_dataset)\n",
    "    combined_val_datasets.append(val_dataset)\n",
    "    steps_per_epoch_list.append(train_size // batch_size)\n",
    "    validation_steps_list.append(val_size // batch_size)\n",
    "    train_counts_list.append(train_count)\n",
    "    val_counts_list.append(val_count)\n",
    "\n",
    "    training_params = zip(combined_dataset_names, combined_training_datasets, combined_val_datasets, steps_per_epoch_list, validation_steps_list, train_counts_list, val_counts_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 999 images belonging to 2 classes.\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "\n",
      "\n",
      "Test Dataset Class Counts:\n",
      "fire: 755\n",
      "nofire: 244\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if full_test_dir is None:\n",
    "    test_generators = []\n",
    "    print(\"No target test directory provided, merging all tests from provided datasets if available.\")\n",
    "    for d in test_dirs:\n",
    "        if d is not None:\n",
    "            test_generators.append(create_generators(d, batch_size=batch_size, augment=False, shuffle=False, img_height=img_height, img_width=img_width)[0]) # No augmentation/shuffle for testing\n",
    "    if len(test_generators) == 0:\n",
    "        raise ValueError(\"No tests found in the provided datasets.\")\n",
    "    true_labels = np.concatenate([gen.classes for gen in test_generators])\n",
    "    test_dataset = generators_to_dataset(test_generators, batch_size=batch_size)\n",
    "    test_steps = sum([gen.samples for gen in test_generators]) // batch_size\n",
    "    print(\"Test Dataset Class Counts:\")\n",
    "    for gen in test_generators:\n",
    "        print(\"Class indices:\", gen.class_indices)\n",
    "        for class_name, class_index in gen.class_indices.items():\n",
    "            print(f\"{class_name}: {sum(gen.classes == class_index)}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "else:\n",
    "    test_generator, augmented_test_generator = create_generators(full_test_dir, batch_size=batch_size, augment=False, shuffle=False, img_height=img_height, img_width=img_width) # No augmentation/shuffle for testing\n",
    "    test_dataset = create_dataset(test_generator, batch_size=batch_size, img_height=img_height, img_width=img_width)\n",
    "    test_steps = test_generator.samples // batch_size\n",
    "    true_labels = test_generator.classes\n",
    "    print(\"Class indices:\", test_generator.class_indices)\n",
    "    print(\"\\n\")\n",
    "    print(\"Test Dataset Class Counts:\")\n",
    "    for class_name, class_index in test_generator.class_indices.items():\n",
    "        print(f\"{class_name}: {sum(test_generator.classes == class_index)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "true_labels = true_labels[: (len(true_labels) // batch_size) * batch_size] # Ensure that the true labels are divisible by the batch size to avoid size mismatch with predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_model(bm, custom=False, to_dir=TEMP_DIR):\n",
    "    if custom:\n",
    "        model = bm\n",
    "        model.compile(optimizer=optimizer_fn, loss=loss_fn, metrics=metrics_list)\n",
    "        os.makedirs(os.path.join(to_dir, model.name), exist_ok=True)\n",
    "        model.save_weights(os.path.join(to_dir, model.name, f\"{model.name}_initial.weights.h5\"))\n",
    "        return model\n",
    "    \n",
    "    base_model = bm(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(img_height, img_width, 3)\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create the model\n",
    "    inputs = Input(shape=(img_height, img_width, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name=bm.__name__)\n",
    "    model.compile(optimizer=optimizer_fn, loss=loss_fn, metrics=metrics_list)\n",
    "    os.makedirs(os.path.join(to_dir, model.name), exist_ok=True)\n",
    "    model.save_weights(os.path.join(to_dir, model.name, f\"{model.name}_initial.weights.h5\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the models and combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "run_number = len([d for d in os.listdir(\"runs\") if os.path.isdir(os.path.join(\"runs\", d)) and d.startswith('run_')]) + 1\n",
    "run_dir = os.path.join(\"runs\", f\"run_{run_number}\")\n",
    "os.makedirs(run_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    \"datasets\": training_datasets,\n",
    "    \"val_size\": val_size,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"test_dirs\": test_dirs,\n",
    "    \"full_test\": full_test_dir,\n",
    "}\n",
    "\n",
    "with open(os.path.join(run_dir, \"run_config.json\"), \"w\") as f:\n",
    "    json.dump(run_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"VGG19\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"VGG19\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ vgg19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">20,024,384</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ vgg19 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m20,024,384\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,159,041</span> (76.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,159,041\u001b[0m (76.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">133,121</span> (520.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m133,121\u001b[0m (520.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,025,920</span> (76.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m20,025,920\u001b[0m (76.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: VGG19 on dataset: The Wildfire Dataset\n",
      "Epoch 1/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 2s/step - accuracy: 0.6217 - auc: 0.6734 - f1_score: 0.6672 - loss: 0.7895 - precision: 0.7198 - recall: 0.6285 - val_accuracy: 0.7475 - val_auc: 0.8410 - val_f1_score: 0.7787 - val_loss: 0.5965 - val_precision: 0.8190 - val_recall: 0.7556 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 3s/step - accuracy: 0.7568 - auc: 0.8269 - f1_score: 0.7988 - loss: 0.5315 - precision: 0.8006 - recall: 0.8002 - val_accuracy: 0.8225 - val_auc: 0.8984 - val_f1_score: 0.8476 - val_loss: 0.5177 - val_precision: 0.8699 - val_recall: 0.8344 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 3s/step - accuracy: 0.7590 - auc: 0.8396 - f1_score: 0.8043 - loss: 0.4943 - precision: 0.8015 - recall: 0.8121 - val_accuracy: 0.8250 - val_auc: 0.9241 - val_f1_score: 0.8433 - val_loss: 0.4496 - val_precision: 0.9135 - val_recall: 0.7851 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 3s/step - accuracy: 0.7627 - auc: 0.8436 - f1_score: 0.8082 - loss: 0.4826 - precision: 0.7985 - recall: 0.8210 - val_accuracy: 0.8375 - val_auc: 0.9255 - val_f1_score: 0.8705 - val_loss: 0.3810 - val_precision: 0.8563 - val_recall: 0.8841 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 3s/step - accuracy: 0.7850 - auc: 0.8672 - f1_score: 0.8244 - loss: 0.4482 - precision: 0.7978 - recall: 0.8585 - val_accuracy: 0.8813 - val_auc: 0.9491 - val_f1_score: 0.9009 - val_loss: 0.3331 - val_precision: 0.8968 - val_recall: 0.9097 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 3s/step - accuracy: 0.7951 - auc: 0.8730 - f1_score: 0.8353 - loss: 0.4319 - precision: 0.8146 - recall: 0.8606 - val_accuracy: 0.8650 - val_auc: 0.9464 - val_f1_score: 0.8850 - val_loss: 0.3254 - val_precision: 0.9221 - val_recall: 0.8554 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 3s/step - accuracy: 0.8039 - auc: 0.8835 - f1_score: 0.8386 - loss: 0.4143 - precision: 0.8300 - recall: 0.8516 - val_accuracy: 0.8975 - val_auc: 0.9593 - val_f1_score: 0.9164 - val_loss: 0.2896 - val_precision: 0.9293 - val_recall: 0.9030 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 3s/step - accuracy: 0.8095 - auc: 0.8911 - f1_score: 0.8456 - loss: 0.4025 - precision: 0.8294 - recall: 0.8654 - val_accuracy: 0.9075 - val_auc: 0.9664 - val_f1_score: 0.9247 - val_loss: 0.2722 - val_precision: 0.9400 - val_recall: 0.9098 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 3s/step - accuracy: 0.8127 - auc: 0.8817 - f1_score: 0.8474 - loss: 0.4180 - precision: 0.8323 - recall: 0.8664 - val_accuracy: 0.9075 - val_auc: 0.9661 - val_f1_score: 0.9203 - val_loss: 0.2673 - val_precision: 0.9466 - val_recall: 0.9057 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 3s/step - accuracy: 0.8278 - auc: 0.8937 - f1_score: 0.8605 - loss: 0.3996 - precision: 0.8519 - recall: 0.8722 - val_accuracy: 0.9038 - val_auc: 0.9703 - val_f1_score: 0.9193 - val_loss: 0.2600 - val_precision: 0.9349 - val_recall: 0.9063 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 3s/step - accuracy: 0.8070 - auc: 0.8976 - f1_score: 0.8411 - loss: 0.3933 - precision: 0.8282 - recall: 0.8602 - val_accuracy: 0.8900 - val_auc: 0.9659 - val_f1_score: 0.9051 - val_loss: 0.2730 - val_precision: 0.9310 - val_recall: 0.8852 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 2s/step - accuracy: 0.8369 - auc: 0.9094 - f1_score: 0.8681 - loss: 0.3704 - precision: 0.8726 - recall: 0.8675 - val_accuracy: 0.9050 - val_auc: 0.9668 - val_f1_score: 0.9217 - val_loss: 0.2603 - val_precision: 0.9209 - val_recall: 0.9283 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 2s/step - accuracy: 0.8299 - auc: 0.9028 - f1_score: 0.8605 - loss: 0.3831 - precision: 0.8620 - recall: 0.8629 - val_accuracy: 0.9050 - val_auc: 0.9696 - val_f1_score: 0.9207 - val_loss: 0.2599 - val_precision: 0.9338 - val_recall: 0.9066 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 2s/step - accuracy: 0.8344 - auc: 0.9099 - f1_score: 0.8640 - loss: 0.3707 - precision: 0.8537 - recall: 0.8773 - val_accuracy: 0.9013 - val_auc: 0.9698 - val_f1_score: 0.9178 - val_loss: 0.2506 - val_precision: 0.9148 - val_recall: 0.9242 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 2s/step - accuracy: 0.8290 - auc: 0.9084 - f1_score: 0.8586 - loss: 0.3710 - precision: 0.8462 - recall: 0.8778 - val_accuracy: 0.9175 - val_auc: 0.9717 - val_f1_score: 0.9342 - val_loss: 0.2452 - val_precision: 0.9481 - val_recall: 0.9223 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 3s/step - accuracy: 0.8259 - auc: 0.9068 - f1_score: 0.8593 - loss: 0.3726 - precision: 0.8507 - recall: 0.8721 - val_accuracy: 0.9137 - val_auc: 0.9781 - val_f1_score: 0.9291 - val_loss: 0.2277 - val_precision: 0.9387 - val_recall: 0.9217 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 3s/step - accuracy: 0.8413 - auc: 0.9158 - f1_score: 0.8718 - loss: 0.3582 - precision: 0.8531 - recall: 0.8941 - val_accuracy: 0.9187 - val_auc: 0.9774 - val_f1_score: 0.9329 - val_loss: 0.2292 - val_precision: 0.9246 - val_recall: 0.9419 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 3s/step - accuracy: 0.8316 - auc: 0.9110 - f1_score: 0.8634 - loss: 0.3649 - precision: 0.8492 - recall: 0.8815 - val_accuracy: 0.9250 - val_auc: 0.9833 - val_f1_score: 0.9347 - val_loss: 0.2155 - val_precision: 0.9434 - val_recall: 0.9272 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 2s/step - accuracy: 0.8507 - auc: 0.9198 - f1_score: 0.8779 - loss: 0.3467 - precision: 0.8882 - recall: 0.8726 - val_accuracy: 0.9262 - val_auc: 0.9801 - val_f1_score: 0.9394 - val_loss: 0.2165 - val_precision: 0.9312 - val_recall: 0.9485 - learning_rate: 0.0010\n",
      "Epoch 20/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 3s/step - accuracy: 0.8447 - auc: 0.9267 - f1_score: 0.8724 - loss: 0.3359 - precision: 0.8649 - recall: 0.8813 - val_accuracy: 0.9212 - val_auc: 0.9813 - val_f1_score: 0.9303 - val_loss: 0.2215 - val_precision: 0.9489 - val_recall: 0.9196 - learning_rate: 0.0010\n",
      "Epoch 21/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 5s/step - accuracy: 0.8414 - auc: 0.9175 - f1_score: 0.8713 - loss: 0.3518 - precision: 0.8686 - recall: 0.8786 - val_accuracy: 0.9300 - val_auc: 0.9819 - val_f1_score: 0.9389 - val_loss: 0.2179 - val_precision: 0.9440 - val_recall: 0.9401 - learning_rate: 0.0010\n",
      "Epoch 22/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 5s/step - accuracy: 0.8446 - auc: 0.9285 - f1_score: 0.8744 - loss: 0.3308 - precision: 0.8681 - recall: 0.8847 - val_accuracy: 0.9413 - val_auc: 0.9892 - val_f1_score: 0.9481 - val_loss: 0.2001 - val_precision: 0.9571 - val_recall: 0.9429 - learning_rate: 0.0010\n",
      "Epoch 23/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 5s/step - accuracy: 0.8412 - auc: 0.9190 - f1_score: 0.8741 - loss: 0.3453 - precision: 0.8670 - recall: 0.8838 - val_accuracy: 0.9225 - val_auc: 0.9798 - val_f1_score: 0.9321 - val_loss: 0.2234 - val_precision: 0.9600 - val_recall: 0.9138 - learning_rate: 0.0010\n",
      "Epoch 24/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 5s/step - accuracy: 0.8429 - auc: 0.9186 - f1_score: 0.8694 - loss: 0.3526 - precision: 0.8657 - recall: 0.8773 - val_accuracy: 0.9413 - val_auc: 0.9878 - val_f1_score: 0.9514 - val_loss: 0.1946 - val_precision: 0.9525 - val_recall: 0.9544 - learning_rate: 0.0010\n",
      "Epoch 25/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 3s/step - accuracy: 0.8518 - auc: 0.9346 - f1_score: 0.8763 - loss: 0.3209 - precision: 0.8598 - recall: 0.8971 - val_accuracy: 0.9350 - val_auc: 0.9864 - val_f1_score: 0.9470 - val_loss: 0.1879 - val_precision: 0.9395 - val_recall: 0.9549 - learning_rate: 0.0010\n",
      "Epoch 26/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 3s/step - accuracy: 0.8534 - auc: 0.9285 - f1_score: 0.8800 - loss: 0.3275 - precision: 0.8817 - recall: 0.8825 - val_accuracy: 0.9300 - val_auc: 0.9848 - val_f1_score: 0.9406 - val_loss: 0.2075 - val_precision: 0.9636 - val_recall: 0.9202 - learning_rate: 0.0010\n",
      "Epoch 27/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 3s/step - accuracy: 0.8578 - auc: 0.9301 - f1_score: 0.8830 - loss: 0.3276 - precision: 0.8708 - recall: 0.8985 - val_accuracy: 0.9463 - val_auc: 0.9914 - val_f1_score: 0.9543 - val_loss: 0.1869 - val_precision: 0.9805 - val_recall: 0.9302 - learning_rate: 0.0010\n",
      "Epoch 28/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 3s/step - accuracy: 0.8567 - auc: 0.9280 - f1_score: 0.8834 - loss: 0.3347 - precision: 0.8750 - recall: 0.8937 - val_accuracy: 0.9350 - val_auc: 0.9899 - val_f1_score: 0.9458 - val_loss: 0.1975 - val_precision: 0.9832 - val_recall: 0.9141 - learning_rate: 0.0010\n",
      "Epoch 29/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 3s/step - accuracy: 0.8498 - auc: 0.9268 - f1_score: 0.8741 - loss: 0.3353 - precision: 0.8667 - recall: 0.8872 - val_accuracy: 0.9513 - val_auc: 0.9916 - val_f1_score: 0.9586 - val_loss: 0.1798 - val_precision: 0.9607 - val_recall: 0.9587 - learning_rate: 0.0010\n",
      "Epoch 30/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 3s/step - accuracy: 0.8574 - auc: 0.9351 - f1_score: 0.8841 - loss: 0.3141 - precision: 0.8827 - recall: 0.8868 - val_accuracy: 0.9563 - val_auc: 0.9930 - val_f1_score: 0.9652 - val_loss: 0.1697 - val_precision: 0.9583 - val_recall: 0.9718 - learning_rate: 0.0010\n",
      "Epoch 31/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 3s/step - accuracy: 0.8611 - auc: 0.9359 - f1_score: 0.8860 - loss: 0.3130 - precision: 0.8710 - recall: 0.9046 - val_accuracy: 0.9575 - val_auc: 0.9934 - val_f1_score: 0.9631 - val_loss: 0.1693 - val_precision: 0.9597 - val_recall: 0.9714 - learning_rate: 0.0010\n",
      "Epoch 32/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 3s/step - accuracy: 0.8707 - auc: 0.9392 - f1_score: 0.8915 - loss: 0.3108 - precision: 0.8931 - recall: 0.8945 - val_accuracy: 0.9575 - val_auc: 0.9929 - val_f1_score: 0.9655 - val_loss: 0.1623 - val_precision: 0.9638 - val_recall: 0.9677 - learning_rate: 0.0010\n",
      "Epoch 33/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 3s/step - accuracy: 0.8597 - auc: 0.9332 - f1_score: 0.8852 - loss: 0.3218 - precision: 0.8810 - recall: 0.8938 - val_accuracy: 0.9675 - val_auc: 0.9960 - val_f1_score: 0.9711 - val_loss: 0.1526 - val_precision: 0.9751 - val_recall: 0.9710 - learning_rate: 0.0010\n",
      "Epoch 34/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 3s/step - accuracy: 0.8618 - auc: 0.9381 - f1_score: 0.8866 - loss: 0.3115 - precision: 0.8838 - recall: 0.8906 - val_accuracy: 0.9638 - val_auc: 0.9945 - val_f1_score: 0.9688 - val_loss: 0.1503 - val_precision: 0.9567 - val_recall: 0.9858 - learning_rate: 0.0010\n",
      "Epoch 35/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 3s/step - accuracy: 0.8692 - auc: 0.9392 - f1_score: 0.8941 - loss: 0.3088 - precision: 0.8829 - recall: 0.9080 - val_accuracy: 0.9500 - val_auc: 0.9888 - val_f1_score: 0.9596 - val_loss: 0.1686 - val_precision: 0.9641 - val_recall: 0.9564 - learning_rate: 0.0010\n",
      "Epoch 36/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 3s/step - accuracy: 0.8736 - auc: 0.9412 - f1_score: 0.8937 - loss: 0.3016 - precision: 0.8969 - recall: 0.8950 - val_accuracy: 0.9550 - val_auc: 0.9948 - val_f1_score: 0.9611 - val_loss: 0.1603 - val_precision: 0.9674 - val_recall: 0.9596 - learning_rate: 0.0010\n",
      "Epoch 37/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 3s/step - accuracy: 0.8852 - auc: 0.9491 - f1_score: 0.9049 - loss: 0.2809 - precision: 0.8953 - recall: 0.9184 - val_accuracy: 0.9650 - val_auc: 0.9933 - val_f1_score: 0.9715 - val_loss: 0.1606 - val_precision: 0.9781 - val_recall: 0.9665 - learning_rate: 0.0010\n",
      "Epoch 38/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 3s/step - accuracy: 0.8626 - auc: 0.9431 - f1_score: 0.8871 - loss: 0.2953 - precision: 0.8758 - recall: 0.9004 - val_accuracy: 0.9538 - val_auc: 0.9954 - val_f1_score: 0.9632 - val_loss: 0.1632 - val_precision: 0.9833 - val_recall: 0.9421 - learning_rate: 0.0010\n",
      "Epoch 39/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 3s/step - accuracy: 0.8797 - auc: 0.9420 - f1_score: 0.9044 - loss: 0.3021 - precision: 0.9132 - recall: 0.8963 - val_accuracy: 0.9700 - val_auc: 0.9951 - val_f1_score: 0.9736 - val_loss: 0.1484 - val_precision: 0.9770 - val_recall: 0.9729 - learning_rate: 0.0010\n",
      "Epoch 40/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 3s/step - accuracy: 0.8709 - auc: 0.9427 - f1_score: 0.8931 - loss: 0.2975 - precision: 0.8861 - recall: 0.9045 - val_accuracy: 0.9750 - val_auc: 0.9960 - val_f1_score: 0.9792 - val_loss: 0.1464 - val_precision: 0.9811 - val_recall: 0.9769 - learning_rate: 0.0010\n",
      "Epoch 41/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 4s/step - accuracy: 0.8726 - auc: 0.9437 - f1_score: 0.8957 - loss: 0.2942 - precision: 0.8889 - recall: 0.9071 - val_accuracy: 0.9700 - val_auc: 0.9963 - val_f1_score: 0.9740 - val_loss: 0.1518 - val_precision: 0.9839 - val_recall: 0.9683 - learning_rate: 0.0010\n",
      "Epoch 42/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m575s\u001b[0m 5s/step - accuracy: 0.8823 - auc: 0.9507 - f1_score: 0.9016 - loss: 0.2787 - precision: 0.8969 - recall: 0.9091 - val_accuracy: 0.9575 - val_auc: 0.9939 - val_f1_score: 0.9631 - val_loss: 0.1492 - val_precision: 0.9551 - val_recall: 0.9750 - learning_rate: 0.0010\n",
      "Epoch 43/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 3s/step - accuracy: 0.8936 - auc: 0.9532 - f1_score: 0.9134 - loss: 0.2680 - precision: 0.9051 - recall: 0.9228 - val_accuracy: 0.9588 - val_auc: 0.9951 - val_f1_score: 0.9640 - val_loss: 0.1434 - val_precision: 0.9733 - val_recall: 0.9594 - learning_rate: 0.0010\n",
      "Epoch 44/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 4s/step - accuracy: 0.8767 - auc: 0.9472 - f1_score: 0.8986 - loss: 0.2850 - precision: 0.8892 - recall: 0.9122 - val_accuracy: 0.9712 - val_auc: 0.9960 - val_f1_score: 0.9746 - val_loss: 0.1381 - val_precision: 0.9842 - val_recall: 0.9707 - learning_rate: 0.0010\n",
      "Epoch 45/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 3s/step - accuracy: 0.8745 - auc: 0.9477 - f1_score: 0.8932 - loss: 0.2849 - precision: 0.8888 - recall: 0.9030 - val_accuracy: 0.9750 - val_auc: 0.9963 - val_f1_score: 0.9807 - val_loss: 0.1273 - val_precision: 0.9805 - val_recall: 0.9805 - learning_rate: 0.0010\n",
      "Epoch 46/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 3s/step - accuracy: 0.8731 - auc: 0.9485 - f1_score: 0.8954 - loss: 0.2828 - precision: 0.8978 - recall: 0.8973 - val_accuracy: 0.9800 - val_auc: 0.9988 - val_f1_score: 0.9843 - val_loss: 0.1266 - val_precision: 0.9936 - val_recall: 0.9730 - learning_rate: 0.0010\n",
      "Epoch 47/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 3s/step - accuracy: 0.8764 - auc: 0.9443 - f1_score: 0.8996 - loss: 0.2899 - precision: 0.8939 - recall: 0.9064 - val_accuracy: 0.9712 - val_auc: 0.9977 - val_f1_score: 0.9748 - val_loss: 0.1295 - val_precision: 0.9913 - val_recall: 0.9602 - learning_rate: 0.0010\n",
      "Epoch 48/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 3s/step - accuracy: 0.8863 - auc: 0.9547 - f1_score: 0.9088 - loss: 0.2630 - precision: 0.9079 - recall: 0.9094 - val_accuracy: 0.9613 - val_auc: 0.9962 - val_f1_score: 0.9663 - val_loss: 0.1421 - val_precision: 0.9937 - val_recall: 0.9438 - learning_rate: 0.0010\n",
      "Epoch 49/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 3s/step - accuracy: 0.8836 - auc: 0.9546 - f1_score: 0.9042 - loss: 0.2648 - precision: 0.9069 - recall: 0.9028 - val_accuracy: 0.9750 - val_auc: 0.9984 - val_f1_score: 0.9788 - val_loss: 0.1222 - val_precision: 0.9897 - val_recall: 0.9698 - learning_rate: 0.0010\n",
      "Epoch 50/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 3s/step - accuracy: 0.8866 - auc: 0.9579 - f1_score: 0.9072 - loss: 0.2568 - precision: 0.9006 - recall: 0.9153 - val_accuracy: 0.9712 - val_auc: 0.9966 - val_f1_score: 0.9759 - val_loss: 0.1332 - val_precision: 0.9937 - val_recall: 0.9593 - learning_rate: 0.0010\n",
      "Epoch 51/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 3s/step - accuracy: 0.8801 - auc: 0.9492 - f1_score: 0.9005 - loss: 0.2798 - precision: 0.8955 - recall: 0.9097 - val_accuracy: 0.9700 - val_auc: 0.9972 - val_f1_score: 0.9757 - val_loss: 0.1209 - val_precision: 0.9659 - val_recall: 0.9857 - learning_rate: 0.0010\n",
      "Epoch 52/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 3s/step - accuracy: 0.8848 - auc: 0.9484 - f1_score: 0.9059 - loss: 0.2808 - precision: 0.8964 - recall: 0.9178 - val_accuracy: 0.9750 - val_auc: 0.9970 - val_f1_score: 0.9786 - val_loss: 0.1315 - val_precision: 0.9872 - val_recall: 0.9707 - learning_rate: 0.0010\n",
      "Epoch 53/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 3s/step - accuracy: 0.8775 - auc: 0.9511 - f1_score: 0.8995 - loss: 0.2753 - precision: 0.9011 - recall: 0.9016 - val_accuracy: 0.9675 - val_auc: 0.9976 - val_f1_score: 0.9728 - val_loss: 0.1245 - val_precision: 0.9854 - val_recall: 0.9613 - learning_rate: 0.0010\n",
      "Epoch 54/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 3s/step - accuracy: 0.8941 - auc: 0.9587 - f1_score: 0.9113 - loss: 0.2533 - precision: 0.9042 - recall: 0.9234 - val_accuracy: 0.9812 - val_auc: 0.9975 - val_f1_score: 0.9848 - val_loss: 0.1204 - val_precision: 0.9898 - val_recall: 0.9797 - learning_rate: 0.0010\n",
      "Epoch 55/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 3s/step - accuracy: 0.8772 - auc: 0.9486 - f1_score: 0.9002 - loss: 0.2798 - precision: 0.8972 - recall: 0.9038 - val_accuracy: 0.9812 - val_auc: 0.9978 - val_f1_score: 0.9841 - val_loss: 0.1131 - val_precision: 0.9935 - val_recall: 0.9746 - learning_rate: 0.0010\n",
      "Epoch 56/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 3s/step - accuracy: 0.8838 - auc: 0.9511 - f1_score: 0.9040 - loss: 0.2744 - precision: 0.8914 - recall: 0.9216 - val_accuracy: 0.9775 - val_auc: 0.9977 - val_f1_score: 0.9803 - val_loss: 0.1151 - val_precision: 0.9851 - val_recall: 0.9768 - learning_rate: 0.0010\n",
      "Epoch 57/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 3s/step - accuracy: 0.8838 - auc: 0.9525 - f1_score: 0.9050 - loss: 0.2677 - precision: 0.8983 - recall: 0.9166 - val_accuracy: 0.9787 - val_auc: 0.9985 - val_f1_score: 0.9826 - val_loss: 0.1182 - val_precision: 0.9800 - val_recall: 0.9859 - learning_rate: 0.0010\n",
      "Epoch 58/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 3s/step - accuracy: 0.8963 - auc: 0.9585 - f1_score: 0.9157 - loss: 0.2550 - precision: 0.9050 - recall: 0.9278 - val_accuracy: 0.9787 - val_auc: 0.9982 - val_f1_score: 0.9825 - val_loss: 0.1227 - val_precision: 0.9896 - val_recall: 0.9754 - learning_rate: 0.0010\n",
      "Epoch 59/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 6s/step - accuracy: 0.8876 - auc: 0.9550 - f1_score: 0.9070 - loss: 0.2640 - precision: 0.9095 - recall: 0.9057 - val_accuracy: 0.9737 - val_auc: 0.9975 - val_f1_score: 0.9790 - val_loss: 0.1177 - val_precision: 0.9782 - val_recall: 0.9802 - learning_rate: 0.0010\n",
      "Epoch 60/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m593s\u001b[0m 5s/step - accuracy: 0.8863 - auc: 0.9543 - f1_score: 0.9060 - loss: 0.2648 - precision: 0.8960 - recall: 0.9197 - val_accuracy: 0.9725 - val_auc: 0.9980 - val_f1_score: 0.9765 - val_loss: 0.1128 - val_precision: 0.9756 - val_recall: 0.9796 - learning_rate: 0.0010\n",
      "Epoch 61/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m605s\u001b[0m 5s/step - accuracy: 0.8855 - auc: 0.9549 - f1_score: 0.9049 - loss: 0.2606 - precision: 0.9006 - recall: 0.9152 - val_accuracy: 0.9850 - val_auc: 0.9988 - val_f1_score: 0.9873 - val_loss: 0.1084 - val_precision: 0.9937 - val_recall: 0.9814 - learning_rate: 0.0010\n",
      "Epoch 62/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 5s/step - accuracy: 0.8909 - auc: 0.9533 - f1_score: 0.9102 - loss: 0.2677 - precision: 0.9211 - recall: 0.9025 - val_accuracy: 0.9737 - val_auc: 0.9975 - val_f1_score: 0.9784 - val_loss: 0.1156 - val_precision: 0.9777 - val_recall: 0.9797 - learning_rate: 0.0010\n",
      "Epoch 63/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m580s\u001b[0m 5s/step - accuracy: 0.8863 - auc: 0.9523 - f1_score: 0.9059 - loss: 0.2703 - precision: 0.8996 - recall: 0.9180 - val_accuracy: 0.9750 - val_auc: 0.9986 - val_f1_score: 0.9800 - val_loss: 0.1030 - val_precision: 0.9901 - val_recall: 0.9710 - learning_rate: 0.0010\n",
      "Epoch 64/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m569s\u001b[0m 5s/step - accuracy: 0.8910 - auc: 0.9550 - f1_score: 0.9083 - loss: 0.2645 - precision: 0.9034 - recall: 0.9157 - val_accuracy: 0.9862 - val_auc: 0.9993 - val_f1_score: 0.9871 - val_loss: 0.1019 - val_precision: 0.9897 - val_recall: 0.9876 - learning_rate: 0.0010\n",
      "Epoch 65/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 5s/step - accuracy: 0.8918 - auc: 0.9595 - f1_score: 0.9125 - loss: 0.2493 - precision: 0.9104 - recall: 0.9162 - val_accuracy: 0.9837 - val_auc: 0.9990 - val_f1_score: 0.9868 - val_loss: 0.0967 - val_precision: 0.9840 - val_recall: 0.9899 - learning_rate: 0.0010\n",
      "Epoch 66/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 5s/step - accuracy: 0.8911 - auc: 0.9588 - f1_score: 0.9103 - loss: 0.2523 - precision: 0.9059 - recall: 0.9173 - val_accuracy: 0.9850 - val_auc: 0.9992 - val_f1_score: 0.9866 - val_loss: 0.0988 - val_precision: 0.9937 - val_recall: 0.9813 - learning_rate: 0.0010\n",
      "Epoch 67/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 4s/step - accuracy: 0.8957 - auc: 0.9616 - f1_score: 0.9139 - loss: 0.2434 - precision: 0.9077 - recall: 0.9233 - val_accuracy: 0.9862 - val_auc: 0.9992 - val_f1_score: 0.9890 - val_loss: 0.0990 - val_precision: 0.9878 - val_recall: 0.9898 - learning_rate: 0.0010\n",
      "Epoch 68/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 3s/step - accuracy: 0.8776 - auc: 0.9499 - f1_score: 0.8985 - loss: 0.2803 - precision: 0.8846 - recall: 0.9156 - val_accuracy: 0.9850 - val_auc: 0.9986 - val_f1_score: 0.9870 - val_loss: 0.0943 - val_precision: 0.9873 - val_recall: 0.9873 - learning_rate: 0.0010\n",
      "Epoch 69/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 3s/step - accuracy: 0.9021 - auc: 0.9607 - f1_score: 0.9205 - loss: 0.2467 - precision: 0.9066 - recall: 0.9370 - val_accuracy: 0.9862 - val_auc: 0.9993 - val_f1_score: 0.9890 - val_loss: 0.0887 - val_precision: 0.9819 - val_recall: 0.9959 - learning_rate: 0.0010\n",
      "Epoch 70/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 3s/step - accuracy: 0.8968 - auc: 0.9615 - f1_score: 0.9146 - loss: 0.2445 - precision: 0.9171 - recall: 0.9148 - val_accuracy: 0.9787 - val_auc: 0.9982 - val_f1_score: 0.9821 - val_loss: 0.1024 - val_precision: 0.9775 - val_recall: 0.9876 - learning_rate: 0.0010\n",
      "Epoch 71/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 3s/step - accuracy: 0.9138 - auc: 0.9670 - f1_score: 0.9293 - loss: 0.2268 - precision: 0.9192 - recall: 0.9414 - val_accuracy: 0.9787 - val_auc: 0.9984 - val_f1_score: 0.9830 - val_loss: 0.1022 - val_precision: 0.9807 - val_recall: 0.9864 - learning_rate: 0.0010\n",
      "Epoch 72/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 3s/step - accuracy: 0.8969 - auc: 0.9567 - f1_score: 0.9152 - loss: 0.2577 - precision: 0.9113 - recall: 0.9224 - val_accuracy: 0.9850 - val_auc: 0.9992 - val_f1_score: 0.9882 - val_loss: 0.1000 - val_precision: 0.9918 - val_recall: 0.9837 - learning_rate: 0.0010\n",
      "Epoch 73/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 5s/step - accuracy: 0.9054 - auc: 0.9591 - f1_score: 0.9214 - loss: 0.2517 - precision: 0.9134 - recall: 0.9334 - val_accuracy: 0.9875 - val_auc: 0.9998 - val_f1_score: 0.9907 - val_loss: 0.0894 - val_precision: 0.9960 - val_recall: 0.9843 - learning_rate: 0.0010\n",
      "Epoch 74/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.8997 - auc: 0.9595 - f1_score: 0.9179 - loss: 0.2528 - precision: 0.9220 - recall: 0.9152\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m572s\u001b[0m 5s/step - accuracy: 0.8996 - auc: 0.9595 - f1_score: 0.9178 - loss: 0.2528 - precision: 0.9218 - recall: 0.9152 - val_accuracy: 0.9850 - val_auc: 0.9987 - val_f1_score: 0.9879 - val_loss: 0.1031 - val_precision: 0.9919 - val_recall: 0.9839 - learning_rate: 0.0010\n",
      "Training time: 27330.85 seconds\n",
      "Evaluating VGG19 on The Wildfire Dataset...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 4s/step - accuracy: 0.7608 - auc: 0.2823 - f1_score: 0.0535 - loss: 0.5906 - precision: 0.1166 - recall: 0.2776\n",
      "Training results:\n",
      "{'evaluation': {'accuracy': 0.7701612710952759,\n",
      "                'auc': 0.9100688695907593,\n",
      "                'f1_score': 0.25713610649108887,\n",
      "                'loss': 0.5676019787788391,\n",
      "                'precision': 0.5189573168754578,\n",
      "                'recall': 0.8975409865379333},\n",
      " 'history': {'accuracy': [0.6704059839248657,\n",
      "                          0.7564102411270142,\n",
      "                          0.7678952813148499,\n",
      "                          0.7708333134651184,\n",
      "                          0.7873931527137756,\n",
      "                          0.7996794581413269,\n",
      "                          0.8039529919624329,\n",
      "                          0.8122329115867615,\n",
      "                          0.8114316463470459,\n",
      "                          0.8194444179534912,\n",
      "                          0.8162392973899841,\n",
      "                          0.8298611044883728,\n",
      "                          0.8255876302719116,\n",
      "                          0.8400107026100159,\n",
      "                          0.8330662250518799,\n",
      "                          0.8271901607513428,\n",
      "                          0.8376068472862244,\n",
      "                          0.8282585740089417,\n",
      "                          0.8434829115867615,\n",
      "                          0.8525640964508057,\n",
      "                          0.8501602411270142,\n",
      "                          0.8514957427978516,\n",
      "                          0.8450854420661926,\n",
      "                          0.8461538553237915,\n",
      "                          0.8560363054275513,\n",
      "                          0.8549679517745972,\n",
      "                          0.8576388955116272,\n",
      "                          0.8576388955116272,\n",
      "                          0.8477563858032227,\n",
      "                          0.8589743375778198,\n",
      "                          0.8517628312110901,\n",
      "                          0.8691239356994629,\n",
      "                          0.8659188151359558,\n",
      "                          0.8635149598121643,\n",
      "                          0.87232905626297,\n",
      "                          0.8669871687889099,\n",
      "                          0.8747329115867615,\n",
      "                          0.87232905626297,\n",
      "                          0.8816773295402527,\n",
      "                          0.875,\n",
      "                          0.8752670884132385,\n",
      "                          0.875534176826477,\n",
      "                          0.8883547186851501,\n",
      "                          0.875,\n",
      "                          0.8747329115867615,\n",
      "                          0.8760683536529541,\n",
      "                          0.8822115659713745,\n",
      "                          0.8832799196243286,\n",
      "                          0.8822115659713745,\n",
      "                          0.877136766910553,\n",
      "                          0.8827457427978516,\n",
      "                          0.8827457427978516,\n",
      "                          0.8803418874740601,\n",
      "                          0.8867521286010742,\n",
      "                          0.8830128312110901,\n",
      "                          0.8856837749481201,\n",
      "                          0.8883547186851501,\n",
      "                          0.8886218070983887,\n",
      "                          0.8854166865348816,\n",
      "                          0.8880876302719116,\n",
      "                          0.8827457427978516,\n",
      "                          0.8902243375778198,\n",
      "                          0.8883547186851501,\n",
      "                          0.8958333134651184,\n",
      "                          0.8931623697280884,\n",
      "                          0.8931623697280884,\n",
      "                          0.8979700803756714,\n",
      "                          0.8814102411270142,\n",
      "                          0.8963675498962402,\n",
      "                          0.8979700803756714,\n",
      "                          0.9030448794364929,\n",
      "                          0.8910256624221802,\n",
      "                          0.9030448794364929,\n",
      "                          0.8944978713989258],\n",
      "             'auc': [0.7293781638145447,\n",
      "                     0.8230652213096619,\n",
      "                     0.8431764245033264,\n",
      "                     0.8479715585708618,\n",
      "                     0.8643354177474976,\n",
      "                     0.8779622316360474,\n",
      "                     0.8838172554969788,\n",
      "                     0.8909332156181335,\n",
      "                     0.8863850831985474,\n",
      "                     0.8912795782089233,\n",
      "                     0.8982967734336853,\n",
      "                     0.9023325443267822,\n",
      "                     0.9013550281524658,\n",
      "                     0.9094852805137634,\n",
      "                     0.9157902598381042,\n",
      "                     0.9067903757095337,\n",
      "                     0.9137119650840759,\n",
      "                     0.912136435508728,\n",
      "                     0.9170985221862793,\n",
      "                     0.9282941818237305,\n",
      "                     0.9226962327957153,\n",
      "                     0.9275614023208618,\n",
      "                     0.9231873154640198,\n",
      "                     0.923790454864502,\n",
      "                     0.9329583048820496,\n",
      "                     0.9301102161407471,\n",
      "                     0.9322187304496765,\n",
      "                     0.9264187812805176,\n",
      "                     0.9257863163948059,\n",
      "                     0.9351850152015686,\n",
      "                     0.9295755624771118,\n",
      "                     0.9385775327682495,\n",
      "                     0.9358497262001038,\n",
      "                     0.9373552799224854,\n",
      "                     0.9398788809776306,\n",
      "                     0.9389482736587524,\n",
      "                     0.9445911049842834,\n",
      "                     0.9437696933746338,\n",
      "                     0.944144606590271,\n",
      "                     0.9438356757164001,\n",
      "                     0.9439699053764343,\n",
      "                     0.9437150955200195,\n",
      "                     0.9513629674911499,\n",
      "                     0.9462345242500305,\n",
      "                     0.9466656446456909,\n",
      "                     0.9481880068778992,\n",
      "                     0.947030782699585,\n",
      "                     0.9528374671936035,\n",
      "                     0.9502159953117371,\n",
      "                     0.9516164064407349,\n",
      "                     0.9499298334121704,\n",
      "                     0.9509245157241821,\n",
      "                     0.9521488547325134,\n",
      "                     0.9533916711807251,\n",
      "                     0.9493910074234009,\n",
      "                     0.9526286125183105,\n",
      "                     0.9539246559143066,\n",
      "                     0.95555579662323,\n",
      "                     0.9521991014480591,\n",
      "                     0.9576560258865356,\n",
      "                     0.9527566432952881,\n",
      "                     0.9532645344734192,\n",
      "                     0.9543133974075317,\n",
      "                     0.958163857460022,\n",
      "                     0.9599281549453735,\n",
      "                     0.9572392106056213,\n",
      "                     0.9613764882087708,\n",
      "                     0.953228235244751,\n",
      "                     0.958904504776001,\n",
      "                     0.9615532159805298,\n",
      "                     0.9614298939704895,\n",
      "                     0.954454243183136,\n",
      "                     0.9588592648506165,\n",
      "                     0.9581422805786133],\n",
      "             'f1_score': [0.718741774559021,\n",
      "                          0.800487756729126,\n",
      "                          0.8116466403007507,\n",
      "                          0.8155795335769653,\n",
      "                          0.8269457221031189,\n",
      "                          0.838420569896698,\n",
      "                          0.8391357660293579,\n",
      "                          0.8479775190353394,\n",
      "                          0.8457042574882507,\n",
      "                          0.8534583449363708,\n",
      "                          0.8486958146095276,\n",
      "                          0.8615869283676147,\n",
      "                          0.8569669723510742,\n",
      "                          0.869714617729187,\n",
      "                          0.8623114824295044,\n",
      "                          0.8587707877159119,\n",
      "                          0.8680927753448486,\n",
      "                          0.8602842688560486,\n",
      "                          0.8724682331085205,\n",
      "                          0.878405749797821,\n",
      "                          0.8779927492141724,\n",
      "                          0.8782021999359131,\n",
      "                          0.875883936882019,\n",
      "                          0.8739193081855774,\n",
      "                          0.8806452751159668,\n",
      "                          0.8819527626037598,\n",
      "                          0.8840502500534058,\n",
      "                          0.8838182687759399,\n",
      "                          0.8734806180000305,\n",
      "                          0.885165274143219,\n",
      "                          0.878335177898407,\n",
      "                          0.8920389413833618,\n",
      "                          0.8898196816444397,\n",
      "                          0.8880621194839478,\n",
      "                          0.895770251750946,\n",
      "                          0.8896157741546631,\n",
      "                          0.8971192836761475,\n",
      "                          0.8936799764633179,\n",
      "                          0.904597818851471,\n",
      "                          0.8976203799247742,\n",
      "                          0.897262692451477,\n",
      "                          0.8974304795265198,\n",
      "                          0.9090417623519897,\n",
      "                          0.8967316746711731,\n",
      "                          0.8947635889053345,\n",
      "                          0.8982335329055786,\n",
      "                          0.9042161107063293,\n",
      "                          0.9052695631980896,\n",
      "                          0.903203547000885,\n",
      "                          0.899598240852356,\n",
      "                          0.9032322764396667,\n",
      "                          0.9040455222129822,\n",
      "                          0.90211021900177,\n",
      "                          0.9058497548103333,\n",
      "                          0.9048435091972351,\n",
      "                          0.9064804911613464,\n",
      "                          0.9084875583648682,\n",
      "                          0.9087867140769958,\n",
      "                          0.9061031341552734,\n",
      "                          0.908085286617279,\n",
      "                          0.9026316404342651,\n",
      "                          0.9097232818603516,\n",
      "                          0.9078205823898315,\n",
      "                          0.9126927852630615,\n",
      "                          0.9126800298690796,\n",
      "                          0.9123433828353882,\n",
      "                          0.9162549376487732,\n",
      "                          0.9031346440315247,\n",
      "                          0.9157446026802063,\n",
      "                          0.9161038398742676,\n",
      "                          0.919701874256134,\n",
      "                          0.9097720384597778,\n",
      "                          0.9194446802139282,\n",
      "                          0.913488507270813],\n",
      "             'learning_rate': [0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513],\n",
      "             'loss': [0.6946319937705994,\n",
      "                      0.5314223170280457,\n",
      "                      0.48676565289497375,\n",
      "                      0.47087931632995605,\n",
      "                      0.4470135569572449,\n",
      "                      0.4231947362422943,\n",
      "                      0.41423362493515015,\n",
      "                      0.4025082290172577,\n",
      "                      0.4104297459125519,\n",
      "                      0.4024016857147217,\n",
      "                      0.39107847213745117,\n",
      "                      0.38139545917510986,\n",
      "                      0.3846425414085388,\n",
      "                      0.36922842264175415,\n",
      "                      0.3570503890514374,\n",
      "                      0.3741142153739929,\n",
      "                      0.3616422712802887,\n",
      "                      0.36252790689468384,\n",
      "                      0.35122358798980713,\n",
      "                      0.33155715465545654,\n",
      "                      0.3416356146335602,\n",
      "                      0.3330192565917969,\n",
      "                      0.33829858899116516,\n",
      "                      0.3400651216506958,\n",
      "                      0.32144033908843994,\n",
      "                      0.32536351680755615,\n",
      "                      0.3206459581851959,\n",
      "                      0.33685746788978577,\n",
      "                      0.3358687162399292,\n",
      "                      0.31319522857666016,\n",
      "                      0.32680097222328186,\n",
      "                      0.30891352891921997,\n",
      "                      0.3143220841884613,\n",
      "                      0.31039106845855713,\n",
      "                      0.30493593215942383,\n",
      "                      0.3074304163455963,\n",
      "                      0.2912002205848694,\n",
      "                      0.29618510603904724,\n",
      "                      0.2928369641304016,\n",
      "                      0.2940376400947571,\n",
      "                      0.29463961720466614,\n",
      "                      0.29503992199897766,\n",
      "                      0.27324870228767395,\n",
      "                      0.28763487935066223,\n",
      "                      0.2869110405445099,\n",
      "                      0.2812829613685608,\n",
      "                      0.28365981578826904,\n",
      "                      0.26926133036613464,\n",
      "                      0.27720576524734497,\n",
      "                      0.273244172334671,\n",
      "                      0.27724170684814453,\n",
      "                      0.2732033431529999,\n",
      "                      0.2702264189720154,\n",
      "                      0.2691662907600403,\n",
      "                      0.2796989977359772,\n",
      "                      0.2685365378856659,\n",
      "                      0.2647149860858917,\n",
      "                      0.26155465841293335,\n",
      "                      0.27007046341896057,\n",
      "                      0.25470995903015137,\n",
      "                      0.26868221163749695,\n",
      "                      0.26728251576423645,\n",
      "                      0.26504403352737427,\n",
      "                      0.25509801506996155,\n",
      "                      0.24818773567676544,\n",
      "                      0.2565616965293884,\n",
      "                      0.24339202046394348,\n",
      "                      0.26870161294937134,\n",
      "                      0.25037458539009094,\n",
      "                      0.24385367333889008,\n",
      "                      0.24504096806049347,\n",
      "                      0.265525221824646,\n",
      "                      0.25261834263801575,\n",
      "                      0.2540128529071808],\n",
      "             'precision': [0.747903048992157,\n",
      "                           0.7937526702880859,\n",
      "                           0.8043110966682434,\n",
      "                           0.8096243143081665,\n",
      "                           0.816067636013031,\n",
      "                           0.8248327970504761,\n",
      "                           0.8294243216514587,\n",
      "                           0.8376896977424622,\n",
      "                           0.833759605884552,\n",
      "                           0.8454045057296753,\n",
      "                           0.8381601572036743,\n",
      "                           0.8531527519226074,\n",
      "                           0.8589409589767456,\n",
      "                           0.8621863126754761,\n",
      "                           0.8515358567237854,\n",
      "                           0.8520408272743225,\n",
      "                           0.8552131652832031,\n",
      "                           0.8489847779273987,\n",
      "                           0.8705832362174988,\n",
      "                           0.8749455213546753,\n",
      "                           0.8760683536529541,\n",
      "                           0.871257483959198,\n",
      "                           0.8654087781906128,\n",
      "                           0.8711893558502197,\n",
      "                           0.8715835213661194,\n",
      "                           0.876227080821991,\n",
      "                           0.8737288117408752,\n",
      "                           0.8741467595100403,\n",
      "                           0.8692074418067932,\n",
      "                           0.8815959095954895,\n",
      "                           0.8718279600143433,\n",
      "                           0.890043318271637,\n",
      "                           0.8834329843521118,\n",
      "                           0.8852953910827637,\n",
      "                           0.8896521925926208,\n",
      "                           0.8899123072624207,\n",
      "                           0.8924406170845032,\n",
      "                           0.8850173950195312,\n",
      "                           0.9038050174713135,\n",
      "                           0.895098865032196,\n",
      "                           0.8925053477287292,\n",
      "                           0.894305408000946,\n",
      "                           0.897915780544281,\n",
      "                           0.8917170166969299,\n",
      "                           0.8918206095695496,\n",
      "                           0.89625483751297,\n",
      "                           0.8989338874816895,\n",
      "                           0.8980291485786438,\n",
      "                           0.9051987528800964,\n",
      "                           0.8945093154907227,\n",
      "                           0.8992248177528381,\n",
      "                           0.8945795893669128,\n",
      "                           0.9059792160987854,\n",
      "                           0.904347836971283,\n",
      "                           0.8995726704597473,\n",
      "                           0.9015411138534546,\n",
      "                           0.9046193361282349,\n",
      "                           0.8992744088172913,\n",
      "                           0.905586838722229,\n",
      "                           0.9018510580062866,\n",
      "                           0.8969957232475281,\n",
      "                           0.9103507995605469,\n",
      "                           0.9044750332832336,\n",
      "                           0.9086115956306458,\n",
      "                           0.9078834652900696,\n",
      "                           0.9045376777648926,\n",
      "                           0.9149762392044067,\n",
      "                           0.8966995477676392,\n",
      "                           0.9050847291946411,\n",
      "                           0.9165225028991699,\n",
      "                           0.9140625,\n",
      "                           0.9039450883865356,\n",
      "                           0.9150779843330383,\n",
      "                           0.9071061611175537],\n",
      "             'recall': [0.6984333992004395,\n",
      "                        0.8118162155151367,\n",
      "                        0.8241662979125977,\n",
      "                        0.8249462246894836,\n",
      "                        0.8424268960952759,\n",
      "                        0.8563368320465088,\n",
      "                        0.853444516658783,\n",
      "                        0.8620390295982361,\n",
      "                        0.86091548204422,\n",
      "                        0.8651928901672363,\n",
      "                        0.8646748661994934,\n",
      "                        0.8742411136627197,\n",
      "                        0.8578240275382996,\n",
      "                        0.8805386424064636,\n",
      "                        0.8781346082687378,\n",
      "                        0.8701693415641785,\n",
      "                        0.8843299746513367,\n",
      "                        0.8752725720405579,\n",
      "                        0.8788139224052429,\n",
      "                        0.8834139704704285,\n",
      "                        0.8832399845123291,\n",
      "                        0.8887434601783752,\n",
      "                        0.8885062336921692,\n",
      "                        0.8802602887153625,\n",
      "                        0.8920959234237671,\n",
      "                        0.8902862071990967,\n",
      "                        0.8976926207542419,\n",
      "                        0.8959335088729858,\n",
      "                        0.8821977972984314,\n",
      "                        0.8907672166824341,\n",
      "                        0.8874781131744385,\n",
      "                        0.897033154964447,\n",
      "                        0.9003481268882751,\n",
      "                        0.8933855295181274,\n",
      "                        0.9036197066307068,\n",
      "                        0.8914762735366821,\n",
      "                        0.9037620425224304,\n",
      "                        0.9047194719314575,\n",
      "                        0.9065179824829102,\n",
      "                        0.9028620719909668,\n",
      "                        0.9060869812965393,\n",
      "                        0.9036617279052734,\n",
      "                        0.9222368001937866,\n",
      "                        0.9049912691116333,\n",
      "                        0.9009329080581665,\n",
      "                        0.9032537937164307,\n",
      "                        0.9117646813392639,\n",
      "                        0.9132897853851318,\n",
      "                        0.9024389982223511,\n",
      "                        0.9054704308509827,\n",
      "                        0.9105974435806274,\n",
      "                        0.9160839319229126,\n",
      "                        0.9005168080329895,\n",
      "                        0.9106830358505249,\n",
      "                        0.9120450615882874,\n",
      "                        0.9140625,\n",
      "                        0.9155844449996948,\n",
      "                        0.9208915829658508,\n",
      "                        0.908340573310852,\n",
      "                        0.9164479374885559,\n",
      "                        0.9130624532699585,\n",
      "                        0.9115351438522339,\n",
      "                        0.9147084355354309,\n",
      "                        0.9191111326217651,\n",
      "                        0.9197048544883728,\n",
      "                        0.9227074384689331,\n",
      "                        0.9197397232055664,\n",
      "                        0.9115468263626099,\n",
      "                        0.9286956787109375,\n",
      "                        0.9181109070777893,\n",
      "                        0.927344799041748,\n",
      "                        0.9197207689285278,\n",
      "                        0.9267222285270691,\n",
      "                        0.922507643699646],\n",
      "             'val_accuracy': [0.7475000023841858,\n",
      "                              0.8224999904632568,\n",
      "                              0.824999988079071,\n",
      "                              0.8374999761581421,\n",
      "                              0.8812500238418579,\n",
      "                              0.8650000095367432,\n",
      "                              0.8974999785423279,\n",
      "                              0.9075000286102295,\n",
      "                              0.9075000286102295,\n",
      "                              0.9037500023841858,\n",
      "                              0.8899999856948853,\n",
      "                              0.9049999713897705,\n",
      "                              0.9049999713897705,\n",
      "                              0.9012500047683716,\n",
      "                              0.9175000190734863,\n",
      "                              0.9137499928474426,\n",
      "                              0.918749988079071,\n",
      "                              0.925000011920929,\n",
      "                              0.9262499809265137,\n",
      "                              0.9212499856948853,\n",
      "                              0.9300000071525574,\n",
      "                              0.9412500262260437,\n",
      "                              0.9225000143051147,\n",
      "                              0.9412500262260437,\n",
      "                              0.9350000023841858,\n",
      "                              0.9300000071525574,\n",
      "                              0.9462500214576721,\n",
      "                              0.9350000023841858,\n",
      "                              0.9512500166893005,\n",
      "                              0.956250011920929,\n",
      "                              0.9574999809265137,\n",
      "                              0.9574999809265137,\n",
      "                              0.9674999713897705,\n",
      "                              0.9637500047683716,\n",
      "                              0.949999988079071,\n",
      "                              0.9549999833106995,\n",
      "                              0.9649999737739563,\n",
      "                              0.9537500143051147,\n",
      "                              0.9700000286102295,\n",
      "                              0.9750000238418579,\n",
      "                              0.9700000286102295,\n",
      "                              0.9574999809265137,\n",
      "                              0.9587500095367432,\n",
      "                              0.9712499976158142,\n",
      "                              0.9750000238418579,\n",
      "                              0.9800000190734863,\n",
      "                              0.9712499976158142,\n",
      "                              0.9612500071525574,\n",
      "                              0.9750000238418579,\n",
      "                              0.9712499976158142,\n",
      "                              0.9700000286102295,\n",
      "                              0.9750000238418579,\n",
      "                              0.9674999713897705,\n",
      "                              0.981249988079071,\n",
      "                              0.981249988079071,\n",
      "                              0.9775000214576721,\n",
      "                              0.9787499904632568,\n",
      "                              0.9787499904632568,\n",
      "                              0.9737499952316284,\n",
      "                              0.9725000262260437,\n",
      "                              0.9850000143051147,\n",
      "                              0.9737499952316284,\n",
      "                              0.9750000238418579,\n",
      "                              0.9862499833106995,\n",
      "                              0.9837499856948853,\n",
      "                              0.9850000143051147,\n",
      "                              0.9862499833106995,\n",
      "                              0.9850000143051147,\n",
      "                              0.9862499833106995,\n",
      "                              0.9787499904632568,\n",
      "                              0.9787499904632568,\n",
      "                              0.9850000143051147,\n",
      "                              0.987500011920929,\n",
      "                              0.9850000143051147],\n",
      "             'val_auc': [0.8410383462905884,\n",
      "                         0.8983554840087891,\n",
      "                         0.9240931272506714,\n",
      "                         0.9255127906799316,\n",
      "                         0.9491377472877502,\n",
      "                         0.946398138999939,\n",
      "                         0.9592647552490234,\n",
      "                         0.9663679599761963,\n",
      "                         0.9661116600036621,\n",
      "                         0.9702739715576172,\n",
      "                         0.9658798575401306,\n",
      "                         0.9667671322822571,\n",
      "                         0.9695875644683838,\n",
      "                         0.969791054725647,\n",
      "                         0.9716675281524658,\n",
      "                         0.9780911803245544,\n",
      "                         0.9774426221847534,\n",
      "                         0.9833451509475708,\n",
      "                         0.9800556302070618,\n",
      "                         0.9812730550765991,\n",
      "                         0.9818658828735352,\n",
      "                         0.9891705513000488,\n",
      "                         0.9797801971435547,\n",
      "                         0.9878203868865967,\n",
      "                         0.9863519668579102,\n",
      "                         0.9847842454910278,\n",
      "                         0.991425633430481,\n",
      "                         0.9899055361747742,\n",
      "                         0.9916472434997559,\n",
      "                         0.9930175542831421,\n",
      "                         0.9933574199676514,\n",
      "                         0.9928796291351318,\n",
      "                         0.9960256814956665,\n",
      "                         0.9945226907730103,\n",
      "                         0.988803505897522,\n",
      "                         0.9947540760040283,\n",
      "                         0.9933360815048218,\n",
      "                         0.9954137802124023,\n",
      "                         0.9951041340827942,\n",
      "                         0.9960246086120605,\n",
      "                         0.9963199496269226,\n",
      "                         0.9939453601837158,\n",
      "                         0.9951305389404297,\n",
      "                         0.9959987998008728,\n",
      "                         0.9963447451591492,\n",
      "                         0.9987682104110718,\n",
      "                         0.9976666569709778,\n",
      "                         0.9962432384490967,\n",
      "                         0.998408317565918,\n",
      "                         0.9965915679931641,\n",
      "                         0.9972120523452759,\n",
      "                         0.9970080852508545,\n",
      "                         0.9975876808166504,\n",
      "                         0.9974992275238037,\n",
      "                         0.9978373050689697,\n",
      "                         0.9977090954780579,\n",
      "                         0.9984607696533203,\n",
      "                         0.9981532692909241,\n",
      "                         0.9974930286407471,\n",
      "                         0.997962474822998,\n",
      "                         0.9988113045692444,\n",
      "                         0.9975123405456543,\n",
      "                         0.9985954761505127,\n",
      "                         0.9992505311965942,\n",
      "                         0.9989786744117737,\n",
      "                         0.9992399215698242,\n",
      "                         0.999240517616272,\n",
      "                         0.9986197352409363,\n",
      "                         0.9993194341659546,\n",
      "                         0.9982398152351379,\n",
      "                         0.998420238494873,\n",
      "                         0.9991817474365234,\n",
      "                         0.9997670650482178,\n",
      "                         0.9987366795539856],\n",
      "             'val_f1_score': [0.7786653637886047,\n",
      "                              0.8475523591041565,\n",
      "                              0.8433064222335815,\n",
      "                              0.8705423474311829,\n",
      "                              0.9008996486663818,\n",
      "                              0.8850200772285461,\n",
      "                              0.9164125323295593,\n",
      "                              0.9246919751167297,\n",
      "                              0.9202908277511597,\n",
      "                              0.9192602634429932,\n",
      "                              0.9051060676574707,\n",
      "                              0.921664834022522,\n",
      "                              0.920656681060791,\n",
      "                              0.9177810549736023,\n",
      "                              0.9342412352561951,\n",
      "                              0.9291353821754456,\n",
      "                              0.932873010635376,\n",
      "                              0.9347201585769653,\n",
      "                              0.939354419708252,\n",
      "                              0.930270254611969,\n",
      "                              0.9389336109161377,\n",
      "                              0.9481275677680969,\n",
      "                              0.9321160316467285,\n",
      "                              0.9514181613922119,\n",
      "                              0.9470043778419495,\n",
      "                              0.9406040906906128,\n",
      "                              0.95432448387146,\n",
      "                              0.9457749128341675,\n",
      "                              0.9586456418037415,\n",
      "                              0.9651678204536438,\n",
      "                              0.9631136059761047,\n",
      "                              0.9654769897460938,\n",
      "                              0.9711496829986572,\n",
      "                              0.9687914848327637,\n",
      "                              0.959564745426178,\n",
      "                              0.9611315727233887,\n",
      "                              0.9714553952217102,\n",
      "                              0.9631707668304443,\n",
      "                              0.9736270904541016,\n",
      "                              0.9792139530181885,\n",
      "                              0.9739598035812378,\n",
      "                              0.9631434679031372,\n",
      "                              0.9640491008758545,\n",
      "                              0.9746282696723938,\n",
      "                              0.9807278513908386,\n",
      "                              0.9843363165855408,\n",
      "                              0.9747722148895264,\n",
      "                              0.9663146138191223,\n",
      "                              0.9787653088569641,\n",
      "                              0.9758537411689758,\n",
      "                              0.9756845235824585,\n",
      "                              0.9786055684089661,\n",
      "                              0.9727693200111389,\n",
      "                              0.9847614765167236,\n",
      "                              0.9841066598892212,\n",
      "                              0.9803000688552856,\n",
      "                              0.9825567007064819,\n",
      "                              0.9825023412704468,\n",
      "                              0.9789574146270752,\n",
      "                              0.9764773845672607,\n",
      "                              0.9872777462005615,\n",
      "                              0.9784057140350342,\n",
      "                              0.9800472259521484,\n",
      "                              0.987092137336731,\n",
      "                              0.986839771270752,\n",
      "                              0.9865512251853943,\n",
      "                              0.9890346527099609,\n",
      "                              0.9870293140411377,\n",
      "                              0.9889872670173645,\n",
      "                              0.9821488261222839,\n",
      "                              0.9829823970794678,\n",
      "                              0.9882311224937439,\n",
      "                              0.9907382726669312,\n",
      "                              0.9879127740859985],\n",
      "             'val_loss': [0.5964674949645996,\n",
      "                          0.5177435874938965,\n",
      "                          0.44955959916114807,\n",
      "                          0.3809867203235626,\n",
      "                          0.33311963081359863,\n",
      "                          0.3254232108592987,\n",
      "                          0.28961560130119324,\n",
      "                          0.2722472548484802,\n",
      "                          0.26726678013801575,\n",
      "                          0.26002299785614014,\n",
      "                          0.2729872167110443,\n",
      "                          0.26032018661499023,\n",
      "                          0.2599051594734192,\n",
      "                          0.25059545040130615,\n",
      "                          0.24520745873451233,\n",
      "                          0.22768914699554443,\n",
      "                          0.22922413051128387,\n",
      "                          0.2154528647661209,\n",
      "                          0.2165174037218094,\n",
      "                          0.22150950133800507,\n",
      "                          0.21790429949760437,\n",
      "                          0.20009008049964905,\n",
      "                          0.22336287796497345,\n",
      "                          0.19455444812774658,\n",
      "                          0.18786537647247314,\n",
      "                          0.20750708878040314,\n",
      "                          0.18694454431533813,\n",
      "                          0.19745713472366333,\n",
      "                          0.179785817861557,\n",
      "                          0.16971927881240845,\n",
      "                          0.16930975019931793,\n",
      "                          0.16229234635829926,\n",
      "                          0.15258674323558807,\n",
      "                          0.15032179653644562,\n",
      "                          0.168596088886261,\n",
      "                          0.16031894087791443,\n",
      "                          0.16061940789222717,\n",
      "                          0.16319450736045837,\n",
      "                          0.14840629696846008,\n",
      "                          0.14637985825538635,\n",
      "                          0.15182140469551086,\n",
      "                          0.14920412003993988,\n",
      "                          0.1433911919593811,\n",
      "                          0.13813914358615875,\n",
      "                          0.12731707096099854,\n",
      "                          0.12661230564117432,\n",
      "                          0.1295299381017685,\n",
      "                          0.1420888751745224,\n",
      "                          0.12220536917448044,\n",
      "                          0.13315795361995697,\n",
      "                          0.12088727205991745,\n",
      "                          0.13146544992923737,\n",
      "                          0.12448436766862869,\n",
      "                          0.12040754407644272,\n",
      "                          0.11309818178415298,\n",
      "                          0.1151120737195015,\n",
      "                          0.11823569238185883,\n",
      "                          0.12271061539649963,\n",
      "                          0.1176922395825386,\n",
      "                          0.11278781294822693,\n",
      "                          0.10841058939695358,\n",
      "                          0.11563335359096527,\n",
      "                          0.10300571471452713,\n",
      "                          0.10194980353116989,\n",
      "                          0.09674181044101715,\n",
      "                          0.09883416444063187,\n",
      "                          0.09895572066307068,\n",
      "                          0.09431008249521255,\n",
      "                          0.08871583640575409,\n",
      "                          0.10237216204404831,\n",
      "                          0.10221549868583679,\n",
      "                          0.09996724873781204,\n",
      "                          0.08937185257673264,\n",
      "                          0.10310561209917068],\n",
      "             'val_precision': [0.8189845681190491,\n",
      "                               0.8699360489845276,\n",
      "                               0.9134615659713745,\n",
      "                               0.8562992215156555,\n",
      "                               0.8967611193656921,\n",
      "                               0.9220778942108154,\n",
      "                               0.9293139576911926,\n",
      "                               0.9399585723876953,\n",
      "                               0.9466118812561035,\n",
      "                               0.9348739385604858,\n",
      "                               0.931034505367279,\n",
      "                               0.9209486246109009,\n",
      "                               0.9337607026100159,\n",
      "                               0.9148073196411133,\n",
      "                               0.9481037855148315,\n",
      "                               0.9386503100395203,\n",
      "                               0.9246435761451721,\n",
      "                               0.943355143070221,\n",
      "                               0.931174099445343,\n",
      "                               0.9489361643791199,\n",
      "                               0.9439833760261536,\n",
      "                               0.9570815563201904,\n",
      "                               0.9599999785423279,\n",
      "                               0.9524752497673035,\n",
      "                               0.9395161271095276,\n",
      "                               0.9635974168777466,\n",
      "                               0.9805194735527039,\n",
      "                               0.9831932783126831,\n",
      "                               0.9606625437736511,\n",
      "                               0.9583333134651184,\n",
      "                               0.9596773982048035,\n",
      "                               0.9637826681137085,\n",
      "                               0.975051999092102,\n",
      "                               0.9566929340362549,\n",
      "                               0.9640718698501587,\n",
      "                               0.9674134254455566,\n",
      "                               0.9780876636505127,\n",
      "                               0.9833333492279053,\n",
      "                               0.9769874215126038,\n",
      "                               0.9810526371002197,\n",
      "                               0.9838709831237793,\n",
      "                               0.9551020264625549,\n",
      "                               0.9732510447502136,\n",
      "                               0.9841583967208862,\n",
      "                               0.98046875,\n",
      "                               0.993630588054657,\n",
      "                               0.9913420081138611,\n",
      "                               0.9936575293540955,\n",
      "                               0.9897119402885437,\n",
      "                               0.9936842322349548,\n",
      "                               0.9659318923950195,\n",
      "                               0.9872340559959412,\n",
      "                               0.9853861927986145,\n",
      "                               0.9897540807723999,\n",
      "                               0.993534505367279,\n",
      "                               0.9851064085960388,\n",
      "                               0.9800398945808411,\n",
      "                               0.9895833134651184,\n",
      "                               0.9782178401947021,\n",
      "                               0.9756097793579102,\n",
      "                               0.9937106966972351,\n",
      "                               0.97773277759552,\n",
      "                               0.990138053894043,\n",
      "                               0.9896694421768188,\n",
      "                               0.9839679598808289,\n",
      "                               0.993697464466095,\n",
      "                               0.9877551198005676,\n",
      "                               0.9873149991035461,\n",
      "                               0.9818548560142517,\n",
      "                               0.9774590134620667,\n",
      "                               0.9807322025299072,\n",
      "                               0.9918032884597778,\n",
      "                               0.9960238337516785,\n",
      "                               0.9919028282165527],\n",
      "             'val_recall': [0.7556008100509644,\n",
      "                            0.8343558311462402,\n",
      "                            0.7851239442825317,\n",
      "                            0.8841463327407837,\n",
      "                            0.9096509218215942,\n",
      "                            0.8554216623306274,\n",
      "                            0.903030276298523,\n",
      "                            0.9098196625709534,\n",
      "                            0.9056974649429321,\n",
      "                            0.9063136577606201,\n",
      "                            0.8852459192276001,\n",
      "                            0.9282868504524231,\n",
      "                            0.9066389799118042,\n",
      "                            0.9241803288459778,\n",
      "                            0.9223300814628601,\n",
      "                            0.9216867685317993,\n",
      "                            0.9419087171554565,\n",
      "                            0.9271948337554932,\n",
      "                            0.9484536051750183,\n",
      "                            0.9195876121520996,\n",
      "                            0.9400826692581177,\n",
      "                            0.94291752576828,\n",
      "                            0.913827657699585,\n",
      "                            0.954365074634552,\n",
      "                            0.9549180269241333,\n",
      "                            0.9202454090118408,\n",
      "                            0.9301847815513611,\n",
      "                            0.9140625,\n",
      "                            0.9586777091026306,\n",
      "                            0.9718309640884399,\n",
      "                            0.9714285731315613,\n",
      "                            0.9676767587661743,\n",
      "                            0.9710144996643066,\n",
      "                            0.9858012199401855,\n",
      "                            0.9564356207847595,\n",
      "                            0.9595959782600403,\n",
      "                            0.9665354490280151,\n",
      "                            0.9421157836914062,\n",
      "                            0.9729166626930237,\n",
      "                            0.9769392013549805,\n",
      "                            0.9682539701461792,\n",
      "                            0.9750000238418579,\n",
      "                            0.959432065486908,\n",
      "                            0.970703125,\n",
      "                            0.98046875,\n",
      "                            0.9729729890823364,\n",
      "                            0.9601677060127258,\n",
      "                            0.9437751173973083,\n",
      "                            0.9697580933570862,\n",
      "                            0.9593495726585388,\n",
      "                            0.9856850504875183,\n",
      "                            0.9707112908363342,\n",
      "                            0.9613034725189209,\n",
      "                            0.9797160029411316,\n",
      "                            0.9746299982070923,\n",
      "                            0.9767932295799255,\n",
      "                            0.9859437942504883,\n",
      "                            0.975359320640564,\n",
      "                            0.9801587462425232,\n",
      "                            0.9795918464660645,\n",
      "                            0.9813664555549622,\n",
      "                            0.9797160029411316,\n",
      "                            0.9709864854812622,\n",
      "                            0.9876288771629333,\n",
      "                            0.9899193644523621,\n",
      "                            0.9813277721405029,\n",
      "                            0.9897750616073608,\n",
      "                            0.9873149991035461,\n",
      "                            0.9959100484848022,\n",
      "                            0.9875776171684265,\n",
      "                            0.9864341020584106,\n",
      "                            0.9837398529052734,\n",
      "                            0.984282910823822,\n",
      "                            0.9839357137680054]},\n",
      " 'optimal_threshold': 0.11194741725921631,\n",
      " 'train_counts': {'fire': 730, 'nofire': 1157},\n",
      " 'train_counts_total': 1887,\n",
      " 'train_dataset_size': 3744,\n",
      " 'training_time': 27330.846193790436,\n",
      " 'val_counts': {'fire': 156, 'nofire': 246},\n",
      " 'val_counts_total': 402,\n",
      " 'val_dataset_size': 800}\n",
      "Training model: VGG19 on dataset: DeepFire\n",
      "Epoch 1/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 5s/step - accuracy: 0.7897 - auc: 0.8829 - f1_score: 0.7846 - loss: 0.4885 - precision: 0.6782 - recall: 0.8299 - val_accuracy: 0.6645 - val_auc: 0.9599 - val_f1_score: 0.4915 - val_loss: 0.5389 - val_precision: 0.9907 - val_recall: 0.3430 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 6s/step - accuracy: 0.8954 - auc: 0.9639 - f1_score: 0.8925 - loss: 0.2495 - precision: 0.8973 - recall: 0.8892 - val_accuracy: 0.8273 - val_auc: 0.9821 - val_f1_score: 0.7731 - val_loss: 0.3960 - val_precision: 0.9948 - val_recall: 0.6486 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 6s/step - accuracy: 0.8980 - auc: 0.9632 - f1_score: 0.8953 - loss: 0.2499 - precision: 0.8941 - recall: 0.8994 - val_accuracy: 0.9194 - val_auc: 0.9853 - val_f1_score: 0.9128 - val_loss: 0.2722 - val_precision: 0.9737 - val_recall: 0.8605 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 6s/step - accuracy: 0.9125 - auc: 0.9714 - f1_score: 0.9132 - loss: 0.2248 - precision: 0.9006 - recall: 0.9275 - val_accuracy: 0.9523 - val_auc: 0.9940 - val_f1_score: 0.9508 - val_loss: 0.1969 - val_precision: 0.9793 - val_recall: 0.9251 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 5s/step - accuracy: 0.9223 - auc: 0.9769 - f1_score: 0.9170 - loss: 0.1940 - precision: 0.9199 - recall: 0.9205 - val_accuracy: 0.9605 - val_auc: 0.9945 - val_f1_score: 0.9581 - val_loss: 0.1477 - val_precision: 0.9692 - val_recall: 0.9497 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 4s/step - accuracy: 0.9288 - auc: 0.9800 - f1_score: 0.9309 - loss: 0.1847 - precision: 0.9483 - recall: 0.9147 - val_accuracy: 0.9770 - val_auc: 0.9972 - val_f1_score: 0.9771 - val_loss: 0.1150 - val_precision: 0.9742 - val_recall: 0.9805 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 5s/step - accuracy: 0.9265 - auc: 0.9795 - f1_score: 0.9245 - loss: 0.1863 - precision: 0.9175 - recall: 0.9323 - val_accuracy: 0.9868 - val_auc: 0.9976 - val_f1_score: 0.9879 - val_loss: 0.0802 - val_precision: 0.9868 - val_recall: 0.9868 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 5s/step - accuracy: 0.9266 - auc: 0.9811 - f1_score: 0.9256 - loss: 0.1775 - precision: 0.9207 - recall: 0.9312 - val_accuracy: 0.9688 - val_auc: 0.9978 - val_f1_score: 0.9660 - val_loss: 0.0898 - val_precision: 0.9934 - val_recall: 0.9462 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 5s/step - accuracy: 0.9371 - auc: 0.9801 - f1_score: 0.9331 - loss: 0.1785 - precision: 0.9315 - recall: 0.9375 - val_accuracy: 0.9737 - val_auc: 0.9972 - val_f1_score: 0.9727 - val_loss: 0.0737 - val_precision: 0.9767 - val_recall: 0.9703 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 5s/step - accuracy: 0.9392 - auc: 0.9845 - f1_score: 0.9373 - loss: 0.1581 - precision: 0.9386 - recall: 0.9407 - val_accuracy: 0.9852 - val_auc: 0.9991 - val_f1_score: 0.9849 - val_loss: 0.0534 - val_precision: 0.9865 - val_recall: 0.9832 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 5s/step - accuracy: 0.9484 - auc: 0.9834 - f1_score: 0.9470 - loss: 0.1581 - precision: 0.9507 - recall: 0.9474 - val_accuracy: 0.9901 - val_auc: 0.9995 - val_f1_score: 0.9898 - val_loss: 0.0515 - val_precision: 0.9967 - val_recall: 0.9837 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 5s/step - accuracy: 0.9146 - auc: 0.9812 - f1_score: 0.9143 - loss: 0.1779 - precision: 0.9149 - recall: 0.9178 - val_accuracy: 0.9934 - val_auc: 0.9997 - val_f1_score: 0.9923 - val_loss: 0.0418 - val_precision: 0.9966 - val_recall: 0.9899 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2s/step - accuracy: 0.9309 - auc: 0.9814 - f1_score: 0.9310 - loss: 0.1774 - precision: 0.9357 - recall: 0.9271 - val_accuracy: 0.9951 - val_auc: 0.9998 - val_f1_score: 0.9948 - val_loss: 0.0393 - val_precision: 0.9935 - val_recall: 0.9967 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 3s/step - accuracy: 0.9502 - auc: 0.9899 - f1_score: 0.9508 - loss: 0.1278 - precision: 0.9479 - recall: 0.9542 - val_accuracy: 0.9868 - val_auc: 0.9997 - val_f1_score: 0.9863 - val_loss: 0.0425 - val_precision: 0.9933 - val_recall: 0.9802 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 3s/step - accuracy: 0.9355 - auc: 0.9837 - f1_score: 0.9317 - loss: 0.1649 - precision: 0.9440 - recall: 0.9238 - val_accuracy: 0.9951 - val_auc: 0.9999 - val_f1_score: 0.9950 - val_loss: 0.0327 - val_precision: 0.9968 - val_recall: 0.9937 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 2s/step - accuracy: 0.9466 - auc: 0.9867 - f1_score: 0.9441 - loss: 0.1469 - precision: 0.9466 - recall: 0.9426 - val_accuracy: 0.9918 - val_auc: 0.9994 - val_f1_score: 0.9910 - val_loss: 0.0445 - val_precision: 0.9935 - val_recall: 0.9904 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 3s/step - accuracy: 0.9322 - auc: 0.9818 - f1_score: 0.9261 - loss: 0.1763 - precision: 0.9229 - recall: 0.9353 - val_accuracy: 0.9885 - val_auc: 0.9997 - val_f1_score: 0.9870 - val_loss: 0.0427 - val_precision: 0.9859 - val_recall: 0.9894 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 3s/step - accuracy: 0.9463 - auc: 0.9873 - f1_score: 0.9459 - loss: 0.1434 - precision: 0.9519 - recall: 0.9421 - val_accuracy: 0.9901 - val_auc: 0.9989 - val_f1_score: 0.9902 - val_loss: 0.0433 - val_precision: 0.9967 - val_recall: 0.9836 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 3s/step - accuracy: 0.9491 - auc: 0.9882 - f1_score: 0.9469 - loss: 0.1380 - precision: 0.9486 - recall: 0.9483 - val_accuracy: 0.9901 - val_auc: 0.9996 - val_f1_score: 0.9896 - val_loss: 0.0352 - val_precision: 0.9931 - val_recall: 0.9863 - learning_rate: 0.0010\n",
      "Epoch 20/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 3s/step - accuracy: 0.9423 - auc: 0.9877 - f1_score: 0.9429 - loss: 0.1424 - precision: 0.9384 - recall: 0.9498 - val_accuracy: 0.9967 - val_auc: 0.9999 - val_f1_score: 0.9974 - val_loss: 0.0314 - val_precision: 0.9933 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 21/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 3s/step - accuracy: 0.9423 - auc: 0.9880 - f1_score: 0.9404 - loss: 0.1385 - precision: 0.9404 - recall: 0.9411 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0266 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 22/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 3s/step - accuracy: 0.9573 - auc: 0.9925 - f1_score: 0.9577 - loss: 0.1118 - precision: 0.9633 - recall: 0.9532 - val_accuracy: 0.9819 - val_auc: 0.9991 - val_f1_score: 0.9820 - val_loss: 0.0506 - val_precision: 0.9899 - val_recall: 0.9736 - learning_rate: 0.0010\n",
      "Epoch 23/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 3s/step - accuracy: 0.9521 - auc: 0.9919 - f1_score: 0.9513 - loss: 0.1118 - precision: 0.9513 - recall: 0.9537 - val_accuracy: 0.9934 - val_auc: 0.9997 - val_f1_score: 0.9937 - val_loss: 0.0334 - val_precision: 0.9967 - val_recall: 0.9901 - learning_rate: 0.0010\n",
      "Epoch 24/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 3s/step - accuracy: 0.9429 - auc: 0.9878 - f1_score: 0.9391 - loss: 0.1412 - precision: 0.9421 - recall: 0.9423 - val_accuracy: 0.9934 - val_auc: 0.9997 - val_f1_score: 0.9935 - val_loss: 0.0319 - val_precision: 0.9967 - val_recall: 0.9903 - learning_rate: 0.0010\n",
      "Epoch 25/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 3s/step - accuracy: 0.9563 - auc: 0.9906 - f1_score: 0.9542 - loss: 0.1187 - precision: 0.9618 - recall: 0.9507 - val_accuracy: 0.9967 - val_auc: 0.9999 - val_f1_score: 0.9972 - val_loss: 0.0265 - val_precision: 0.9968 - val_recall: 0.9968 - learning_rate: 0.0010\n",
      "Epoch 26/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9541 - auc: 0.9911 - f1_score: 0.9498 - loss: 0.1210 - precision: 0.9454 - recall: 0.9584\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 3s/step - accuracy: 0.9541 - auc: 0.9911 - f1_score: 0.9498 - loss: 0.1211 - precision: 0.9454 - recall: 0.9584 - val_accuracy: 0.9967 - val_auc: 0.9999 - val_f1_score: 0.9961 - val_loss: 0.0272 - val_precision: 1.0000 - val_recall: 0.9931 - learning_rate: 0.0010\n",
      "Epoch 27/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 3s/step - accuracy: 0.9621 - auc: 0.9946 - f1_score: 0.9624 - loss: 0.0987 - precision: 0.9774 - recall: 0.9468 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0214 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 28/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 3s/step - accuracy: 0.9619 - auc: 0.9932 - f1_score: 0.9606 - loss: 0.1064 - precision: 0.9575 - recall: 0.9661 - val_accuracy: 0.9967 - val_auc: 0.9999 - val_f1_score: 0.9966 - val_loss: 0.0246 - val_precision: 1.0000 - val_recall: 0.9935 - learning_rate: 5.0000e-04\n",
      "Epoch 29/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 3s/step - accuracy: 0.9556 - auc: 0.9939 - f1_score: 0.9561 - loss: 0.1026 - precision: 0.9530 - recall: 0.9575 - val_accuracy: 0.9984 - val_auc: 1.0000 - val_f1_score: 0.9986 - val_loss: 0.0228 - val_precision: 1.0000 - val_recall: 0.9968 - learning_rate: 5.0000e-04\n",
      "Epoch 30/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 3s/step - accuracy: 0.9568 - auc: 0.9936 - f1_score: 0.9528 - loss: 0.1054 - precision: 0.9441 - recall: 0.9685 - val_accuracy: 0.9918 - val_auc: 0.9997 - val_f1_score: 0.9914 - val_loss: 0.0285 - val_precision: 0.9968 - val_recall: 0.9874 - learning_rate: 5.0000e-04\n",
      "Epoch 31/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 3s/step - accuracy: 0.9581 - auc: 0.9925 - f1_score: 0.9595 - loss: 0.1146 - precision: 0.9712 - recall: 0.9473 - val_accuracy: 0.9951 - val_auc: 0.9998 - val_f1_score: 0.9948 - val_loss: 0.0242 - val_precision: 0.9936 - val_recall: 0.9968 - learning_rate: 5.0000e-04\n",
      "Epoch 32/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9623 - auc: 0.9909 - f1_score: 0.9627 - loss: 0.1163 - precision: 0.9595 - recall: 0.9657\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 3s/step - accuracy: 0.9623 - auc: 0.9909 - f1_score: 0.9626 - loss: 0.1163 - precision: 0.9595 - recall: 0.9657 - val_accuracy: 0.9967 - val_auc: 0.9998 - val_f1_score: 0.9968 - val_loss: 0.0227 - val_precision: 1.0000 - val_recall: 0.9935 - learning_rate: 5.0000e-04\n",
      "Training time: 8617.43 seconds\n",
      "Evaluating VGG19 on DeepFire...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 4s/step - accuracy: 0.7624 - auc: 0.7047 - f1_score: 0.4028 - loss: 0.7208 - precision: 0.8214 - recall: 0.6219\n",
      "Training results:\n",
      "{'evaluation': {'accuracy': 0.8417338728904724,\n",
      "                'auc': 0.8753877878189087,\n",
      "                'f1_score': 0.21288634836673737,\n",
      "                'loss': 0.49882426857948303,\n",
      "                'precision': 0.6801801919937134,\n",
      "                'recall': 0.6371307969093323},\n",
      " 'history': {'accuracy': [0.8400493264198303,\n",
      "                          0.8972039222717285,\n",
      "                          0.8996710777282715,\n",
      "                          0.9111841917037964,\n",
      "                          0.9210526347160339,\n",
      "                          0.9272204041481018,\n",
      "                          0.9235197305679321,\n",
      "                          0.9210526347160339,\n",
      "                          0.9305098652839661,\n",
      "                          0.9313322305679321,\n",
      "                          0.9453125,\n",
      "                          0.9276315569877625,\n",
      "                          0.9296875,\n",
      "                          0.9494243264198303,\n",
      "                          0.9370887875556946,\n",
      "                          0.9412006735801697,\n",
      "                          0.9424341917037964,\n",
      "                          0.9453125,\n",
      "                          0.9494243264198303,\n",
      "                          0.9436677694320679,\n",
      "                          0.9457237124443054,\n",
      "                          0.9564144611358643,\n",
      "                          0.9514802694320679,\n",
      "                          0.9494243264198303,\n",
      "                          0.9498355388641357,\n",
      "                          0.9518914222717285,\n",
      "                          0.9605262875556946,\n",
      "                          0.9588815569877625,\n",
      "                          0.9568256735801697,\n",
      "                          0.9547697305679321,\n",
      "                          0.9597039222717285,\n",
      "                          0.9592927694320679],\n",
      "             'auc': [0.90583336353302,\n",
      "                     0.9621634483337402,\n",
      "                     0.9631800651550293,\n",
      "                     0.9692822694778442,\n",
      "                     0.9777184128761292,\n",
      "                     0.9785601496696472,\n",
      "                     0.9790675044059753,\n",
      "                     0.9794487357139587,\n",
      "                     0.9806104898452759,\n",
      "                     0.9821205735206604,\n",
      "                     0.9850296378135681,\n",
      "                     0.9829753041267395,\n",
      "                     0.9827492833137512,\n",
      "                     0.9884204268455505,\n",
      "                     0.9852503538131714,\n",
      "                     0.9849638938903809,\n",
      "                     0.9850379824638367,\n",
      "                     0.9877123236656189,\n",
      "                     0.9890645146369934,\n",
      "                     0.9884017109870911,\n",
      "                     0.9891908764839172,\n",
      "                     0.9922815561294556,\n",
      "                     0.9908210635185242,\n",
      "                     0.9879838228225708,\n",
      "                     0.9885932803153992,\n",
      "                     0.9904199838638306,\n",
      "                     0.9928120374679565,\n",
      "                     0.9929148554801941,\n",
      "                     0.9929807782173157,\n",
      "                     0.9924238920211792,\n",
      "                     0.992385745048523,\n",
      "                     0.9912349581718445],\n",
      "             'f1_score': [0.8385042548179626,\n",
      "                          0.8970080018043518,\n",
      "                          0.8979275226593018,\n",
      "                          0.910733699798584,\n",
      "                          0.9191797971725464,\n",
      "                          0.926902174949646,\n",
      "                          0.923478901386261,\n",
      "                          0.9196840524673462,\n",
      "                          0.9276225566864014,\n",
      "                          0.9306880831718445,\n",
      "                          0.9438657164573669,\n",
      "                          0.9258977770805359,\n",
      "                          0.9287753105163574,\n",
      "                          0.94906085729599,\n",
      "                          0.9337254762649536,\n",
      "                          0.9397179484367371,\n",
      "                          0.9404082894325256,\n",
      "                          0.9444354176521301,\n",
      "                          0.9485839009284973,\n",
      "                          0.9437903165817261,\n",
      "                          0.9444555044174194,\n",
      "                          0.9556347727775574,\n",
      "                          0.9502819180488586,\n",
      "                          0.9466655254364014,\n",
      "                          0.9479236602783203,\n",
      "                          0.9499679207801819,\n",
      "                          0.9601060748100281,\n",
      "                          0.9580860137939453,\n",
      "                          0.9561981558799744,\n",
      "                          0.9513279795646667,\n",
      "                          0.958291232585907,\n",
      "                          0.958926260471344],\n",
      "             'learning_rate': [0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0005000000237487257,\n",
      "                               0.0005000000237487257,\n",
      "                               0.0005000000237487257,\n",
      "                               0.0005000000237487257,\n",
      "                               0.0005000000237487257,\n",
      "                               0.0005000000237487257],\n",
      "             'loss': [0.3928162157535553,\n",
      "                      0.25519105792045593,\n",
      "                      0.24969768524169922,\n",
      "                      0.23118500411510468,\n",
      "                      0.1909453123807907,\n",
      "                      0.18632572889328003,\n",
      "                      0.1858707219362259,\n",
      "                      0.1858178675174713,\n",
      "                      0.17679136991500854,\n",
      "                      0.17083245515823364,\n",
      "                      0.1534443348646164,\n",
      "                      0.16778385639190674,\n",
      "                      0.1692761927843094,\n",
      "                      0.1341506987810135,\n",
      "                      0.15514592826366425,\n",
      "                      0.15598110854625702,\n",
      "                      0.15222883224487305,\n",
      "                      0.1405402421951294,\n",
      "                      0.13337339460849762,\n",
      "                      0.13714149594306946,\n",
      "                      0.13277335464954376,\n",
      "                      0.1117371916770935,\n",
      "                      0.12009626626968384,\n",
      "                      0.13610565662384033,\n",
      "                      0.1325974464416504,\n",
      "                      0.12461768835783005,\n",
      "                      0.10731881856918335,\n",
      "                      0.10767830163240433,\n",
      "                      0.10670792311429977,\n",
      "                      0.11170977354049683,\n",
      "                      0.11040208488702774,\n",
      "                      0.1144975870847702],\n",
      "             'precision': [0.7567893862724304,\n",
      "                           0.8981100916862488,\n",
      "                           0.9027205109596252,\n",
      "                           0.9052631855010986,\n",
      "                           0.919966995716095,\n",
      "                           0.9307817816734314,\n",
      "                           0.9242174625396729,\n",
      "                           0.9183841943740845,\n",
      "                           0.9291076064109802,\n",
      "                           0.9343185424804688,\n",
      "                           0.9435352087020874,\n",
      "                           0.9285127520561218,\n",
      "                           0.9298100471496582,\n",
      "                           0.946297824382782,\n",
      "                           0.9385004043579102,\n",
      "                           0.9443521499633789,\n",
      "                           0.9452282190322876,\n",
      "                           0.9432278871536255,\n",
      "                           0.9470683932304382,\n",
      "                           0.9407526254653931,\n",
      "                           0.9485903978347778,\n",
      "                           0.9538461565971375,\n",
      "                           0.9493877291679382,\n",
      "                           0.9540901780128479,\n",
      "                           0.9535654783248901,\n",
      "                           0.9457237124443054,\n",
      "                           0.9667221307754517,\n",
      "                           0.9551386833190918,\n",
      "                           0.9554087519645691,\n",
      "                           0.9484192728996277,\n",
      "                           0.9611570239067078,\n",
      "                           0.958949089050293],\n",
      "             'recall': [0.8542234301567078,\n",
      "                        0.8966366052627563,\n",
      "                        0.8968058824539185,\n",
      "                        0.9186524152755737,\n",
      "                        0.9214876294136047,\n",
      "                        0.925506055355072,\n",
      "                        0.9226973652839661,\n",
      "                        0.9229494333267212,\n",
      "                        0.9298831224441528,\n",
      "                        0.9289795756340027,\n",
      "                        0.9474116563796997,\n",
      "                        0.9269893169403076,\n",
      "                        0.9290428757667542,\n",
      "                        0.953278660774231,\n",
      "                        0.9329982995986938,\n",
      "                        0.9373454451560974,\n",
      "                        0.9389942288398743,\n",
      "                        0.9486134052276611,\n",
      "                        0.9524979591369629,\n",
      "                        0.9491114616394043,\n",
      "                        0.9423393607139587,\n",
      "                        0.9600651860237122,\n",
      "                        0.9540607333183289,\n",
      "                        0.9438480734825134,\n",
      "                        0.9457237124443054,\n",
      "                        0.9575353860855103,\n",
      "                        0.954023003578186,\n",
      "                        0.9629934430122375,\n",
      "                        0.9577814340591431,\n",
      "                        0.9595959782600403,\n",
      "                        0.9579901099205017,\n",
      "                        0.9597370624542236],\n",
      "             'val_accuracy': [0.6644737124443054,\n",
      "                              0.8273026347160339,\n",
      "                              0.9194079041481018,\n",
      "                              0.9523026347160339,\n",
      "                              0.9605262875556946,\n",
      "                              0.9769737124443054,\n",
      "                              0.9868420958518982,\n",
      "                              0.96875,\n",
      "                              0.9736841917037964,\n",
      "                              0.9851973652839661,\n",
      "                              0.9901315569877625,\n",
      "                              0.9934210777282715,\n",
      "                              0.9950658082962036,\n",
      "                              0.9868420958518982,\n",
      "                              0.9950658082962036,\n",
      "                              0.9917762875556946,\n",
      "                              0.9884868264198303,\n",
      "                              0.9901315569877625,\n",
      "                              0.9901315569877625,\n",
      "                              0.9967105388641357,\n",
      "                              1.0,\n",
      "                              0.9819079041481018,\n",
      "                              0.9934210777282715,\n",
      "                              0.9934210777282715,\n",
      "                              0.9967105388641357,\n",
      "                              0.9967105388641357,\n",
      "                              1.0,\n",
      "                              0.9967105388641357,\n",
      "                              0.9983552694320679,\n",
      "                              0.9917762875556946,\n",
      "                              0.9950658082962036,\n",
      "                              0.9967105388641357],\n",
      "             'val_auc': [0.9598662257194519,\n",
      "                         0.9820523262023926,\n",
      "                         0.9852663278579712,\n",
      "                         0.9940317869186401,\n",
      "                         0.9944630861282349,\n",
      "                         0.9971915483474731,\n",
      "                         0.9976031184196472,\n",
      "                         0.9978379011154175,\n",
      "                         0.9972299337387085,\n",
      "                         0.9991340637207031,\n",
      "                         0.9995293617248535,\n",
      "                         0.9996697902679443,\n",
      "                         0.9998160600662231,\n",
      "                         0.9997239112854004,\n",
      "                         0.9998968839645386,\n",
      "                         0.9993883371353149,\n",
      "                         0.9996519684791565,\n",
      "                         0.9989396333694458,\n",
      "                         0.9996100664138794,\n",
      "                         0.9999133348464966,\n",
      "                         1.0,\n",
      "                         0.9990586042404175,\n",
      "                         0.9997132420539856,\n",
      "                         0.9997076988220215,\n",
      "                         0.9999457597732544,\n",
      "                         0.9999349117279053,\n",
      "                         0.9999999403953552,\n",
      "                         0.9999458193778992,\n",
      "                         0.9999999403953552,\n",
      "                         0.9996856451034546,\n",
      "                         0.9998266696929932,\n",
      "                         0.999832272529602],\n",
      "             'val_f1_score': [0.4914797246456146,\n",
      "                              0.7731043100357056,\n",
      "                              0.9128127694129944,\n",
      "                              0.9508206248283386,\n",
      "                              0.9580941200256348,\n",
      "                              0.9771069884300232,\n",
      "                              0.9879146218299866,\n",
      "                              0.9660423398017883,\n",
      "                              0.9727174639701843,\n",
      "                              0.9849216938018799,\n",
      "                              0.9898012280464172,\n",
      "                              0.9922517538070679,\n",
      "                              0.9947612285614014,\n",
      "                              0.9862663149833679,\n",
      "                              0.9949519634246826,\n",
      "                              0.9910066723823547,\n",
      "                              0.9870414733886719,\n",
      "                              0.9902450442314148,\n",
      "                              0.9896010160446167,\n",
      "                              0.9973534941673279,\n",
      "                              1.0,\n",
      "                              0.9820452332496643,\n",
      "                              0.9937290549278259,\n",
      "                              0.9935428500175476,\n",
      "                              0.9972125887870789,\n",
      "                              0.9961168169975281,\n",
      "                              1.0,\n",
      "                              0.996590256690979,\n",
      "                              0.9985775351524353,\n",
      "                              0.991417646408081,\n",
      "                              0.9947517514228821,\n",
      "                              0.9968266487121582],\n",
      "             'val_loss': [0.5389416217803955,\n",
      "                          0.3959583640098572,\n",
      "                          0.2722189724445343,\n",
      "                          0.19688892364501953,\n",
      "                          0.14774499833583832,\n",
      "                          0.11500964313745499,\n",
      "                          0.08024733513593674,\n",
      "                          0.08980558067560196,\n",
      "                          0.07374702394008636,\n",
      "                          0.05339795723557472,\n",
      "                          0.05151072144508362,\n",
      "                          0.041761741042137146,\n",
      "                          0.039258167147636414,\n",
      "                          0.042548272758722305,\n",
      "                          0.03265185281634331,\n",
      "                          0.04450735077261925,\n",
      "                          0.04269836097955704,\n",
      "                          0.04326844960451126,\n",
      "                          0.03523219749331474,\n",
      "                          0.03142562881112099,\n",
      "                          0.026587091386318207,\n",
      "                          0.05055810138583183,\n",
      "                          0.033362917602062225,\n",
      "                          0.03193158283829689,\n",
      "                          0.02652990259230137,\n",
      "                          0.02719864808022976,\n",
      "                          0.021412674337625504,\n",
      "                          0.024579033255577087,\n",
      "                          0.022776296362280846,\n",
      "                          0.028450965881347656,\n",
      "                          0.02420698292553425,\n",
      "                          0.022731542587280273],\n",
      "             'val_precision': [0.9906542301177979,\n",
      "                               0.9948186278343201,\n",
      "                               0.9736841917037964,\n",
      "                               0.9793103337287903,\n",
      "                               0.9691780805587769,\n",
      "                               0.9741935729980469,\n",
      "                               0.9867549538612366,\n",
      "                               0.9933554530143738,\n",
      "                               0.9767441749572754,\n",
      "                               0.9865319728851318,\n",
      "                               0.996688723564148,\n",
      "                               0.9965986609458923,\n",
      "                               0.9935064911842346,\n",
      "                               0.9933110475540161,\n",
      "                               0.9968454241752625,\n",
      "                               0.9935483932495117,\n",
      "                               0.98591548204422,\n",
      "                               0.996666669845581,\n",
      "                               0.993127167224884,\n",
      "                               0.9933333396911621,\n",
      "                               1.0,\n",
      "                               0.9899328947067261,\n",
      "                               0.9966777563095093,\n",
      "                               0.9967426657676697,\n",
      "                               0.9968454241752625,\n",
      "                               1.0,\n",
      "                               1.0,\n",
      "                               1.0,\n",
      "                               1.0,\n",
      "                               0.9968152642250061,\n",
      "                               0.9936102032661438,\n",
      "                               1.0],\n",
      "             'val_recall': [0.3430420756340027,\n",
      "                            0.6486486196517944,\n",
      "                            0.8604651093482971,\n",
      "                            0.9250814318656921,\n",
      "                            0.9496644139289856,\n",
      "                            0.9805194735527039,\n",
      "                            0.9867549538612366,\n",
      "                            0.9462025165557861,\n",
      "                            0.9702970385551453,\n",
      "                            0.9832214713096619,\n",
      "                            0.983660101890564,\n",
      "                            0.9898648858070374,\n",
      "                            0.9967426657676697,\n",
      "                            0.9801980257034302,\n",
      "                            0.9937106966972351,\n",
      "                            0.9903537034988403,\n",
      "                            0.9893993139266968,\n",
      "                            0.9835526347160339,\n",
      "                            0.9863481521606445,\n",
      "                            1.0,\n",
      "                            1.0,\n",
      "                            0.9735973477363586,\n",
      "                            0.9900990128517151,\n",
      "                            0.9902912378311157,\n",
      "                            0.9968454241752625,\n",
      "                            0.993127167224884,\n",
      "                            1.0,\n",
      "                            0.9935275316238403,\n",
      "                            0.9967741966247559,\n",
      "                            0.9873816967010498,\n",
      "                            0.9967948794364929,\n",
      "                            0.9934640526771545]},\n",
      " 'optimal_threshold': 0.0005118966801092029,\n",
      " 'train_counts': {'fire': 760, 'nofire': 760},\n",
      " 'train_counts_total': 1520,\n",
      " 'train_dataset_size': 2432,\n",
      " 'training_time': 8617.434987783432,\n",
      " 'val_counts': {'fire': 0, 'nofire': 0},\n",
      " 'val_counts_total': 0,\n",
      " 'val_dataset_size': 608}\n",
      "Training model: VGG19 on dataset: The Wildfire Dataset_DeepFire\n",
      "Epoch 1/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 3s/step - accuracy: 0.6510 - auc: 0.8076 - f1_score: 0.6948 - loss: 0.7265 - precision: 0.7324 - recall: 0.6719 - val_accuracy: 0.8011 - val_auc: 0.9001 - val_f1_score: 0.8231 - val_loss: 0.5502 - val_precision: 0.8973 - val_recall: 0.7650 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7656 - auc: 0.8355 - f1_score: 0.8095 - loss: 0.4995 - precision: 0.8005 - recall: 0.8239"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initial training of the model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Record the end time\u001b[39;00m\n\u001b[0;32m     28\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:392\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    383\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m    384\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    391\u001b[0m     )\n\u001b[1;32m--> 392\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    404\u001b[0m }\n\u001b[0;32m    405\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:481\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    480\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m--> 481\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    482\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_evaluating:\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_results = {}\n",
    "results_file = os.path.join(run_dir, 'training_results.json')\n",
    "\n",
    "for base_model, custom_bool in zip(all_models, is_custom_model):\n",
    "    model = generate_model(base_model, custom=custom_bool, to_dir=run_dir) # To display the model summary\n",
    "    model.summary()\n",
    "    model_dir = os.path.join(run_dir, model.name)\n",
    "    training_results[model.name] = {}\n",
    "    plot_model(model, show_shapes=True, show_layer_names=True, to_file=os.path.join(model_dir, f\"{model.name}_architecture.png\"))\n",
    "    for dataset_id, train_dataset, val_dataset, steps_per_epoch, validation_steps, train_counts_dict, val_counts_dict in training_params:\n",
    "        model.load_weights(os.path.join(run_dir, model.name, f\"{model.name}_initial.weights.h5\"))\n",
    "        print(f\"Training model: {model.name} on dataset: {dataset_id}\")\n",
    "        \n",
    "        # Record the start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Initial training of the model\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=val_dataset,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks_list\n",
    "        )\n",
    "\n",
    "        # Record the end time\n",
    "        end_time = time.time()\n",
    "        # Calculate the training time\n",
    "        training_time = end_time - start_time\n",
    "        print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "        model_ds_dir = os.path.join(model_dir, dataset_id)\n",
    "        os.makedirs(model_ds_dir, exist_ok=True)\n",
    "        # Save the model\n",
    "        model.save(os.path.join(model_ds_dir, f\"{model.name}_{dataset_id}.keras\"))\n",
    "\n",
    "        ### Evaluation stage ###\n",
    "        optimal_threshold = full_eval(model_ds_dir, history, model, dataset_id, test_dataset, true_labels, test_steps)\n",
    "        \n",
    "        training_results[model.name][dataset_id] = {\n",
    "            'history': history.history,\n",
    "            'training_time': training_time,\n",
    "            'optimal_threshold': float(optimal_threshold),\n",
    "            'train_dataset_size': steps_per_epoch * batch_size, # Includes augmented data (2x)\n",
    "            'val_dataset_size': validation_steps * batch_size, # Includes augmented data (2x)\n",
    "            'train_counts': train_counts_dict,\n",
    "            'val_counts': val_counts_dict,\n",
    "            'train_counts_total': sum(train_counts_dict.values()),\n",
    "            'val_counts_total': sum(val_counts_dict.values()),\n",
    "            \"evaluation\": model.evaluate(test_dataset, return_dict=True, steps=test_steps)\n",
    "        }\n",
    "        print(\"Training results:\")\n",
    "        pprint(training_results[model.name][dataset_id])\n",
    "        # Save the training results to a file after each iteration\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(training_results, f, indent=4)\n",
    "        \n",
    "        model.compile(optimizer=optimizer_fn, loss=loss_fn, metrics=metrics_list) # Reset the model for the next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brute force loop completed!\n",
      "All models and evaluations are available at: runs\\run_11\n"
     ]
    }
   ],
   "source": [
    "print(\"Brute force loop completed!\")\n",
    "print(f\"All models are now available at: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = os.path.join(run_dir, \"evaluations\")\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "rows = extract_evaluation_data(training_results)\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(os.path.join(eval_dir, \"training_data.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_chart(df, \"Evaluation F1 Score\", eval_dir)\n",
    "plot_metric_chart(df, \"Evaluation Accuracy\", eval_dir)\n",
    "plot_metric_chart(df, \"Evaluation Precision\", eval_dir)\n",
    "plot_metric_chart(df, \"Evaluation Recall\", eval_dir)\n",
    "plot_metric_chart(df, \"Evaluation AUC\", eval_dir)\n",
    "plot_metric_chart(df, \"Training Time\", eval_dir)\n",
    "plot_metric_chart(df, \"Train Size\", eval_dir)\n",
    "plot_metric_chart(df, \"Val Size\", eval_dir)\n",
    "\n",
    "plot_time_extrapolation(df, eval_dir)\n",
    "\n",
    "print(\"All evaluations completed!\")\n",
    "print(f\"Results are available at: {eval_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1351460,
     "sourceId": 2247205,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
