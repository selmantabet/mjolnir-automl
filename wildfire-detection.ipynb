{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Brute-Force Training and Evaluation Pipeline for Datasets and Models - by Selman Tabet @ https://selman.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import socket\n",
    "\n",
    "TEMP_DIR = \"tmp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostname:  Chaos\n"
     ]
    }
   ],
   "source": [
    "print(\"Hostname: \", socket.gethostname())\n",
    "try: # for CUDA enviroment\n",
    "    os.system(\"nvidia-smi\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Data processing libraries\n",
    "import numpy as np\n",
    "from itertools import combinations # For brute force combinatoric search\n",
    "import json # For saving and loading training results\n",
    "import argparse # For command line arguments\n",
    "\n",
    "# Tensorflow-Keras ML libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model # To plot model architecture\n",
    "\n",
    "from IPython import get_ipython # To check if code is running in Jupyter notebook\n",
    "import importlib.util # To import config module from str\n",
    "from pprint import pprint # To show config\n",
    "\n",
    "# Custom helper libraries\n",
    "from notebook_cfg import * # Default parameters\n",
    "from utils.img_processing import enforce_image_params\n",
    "from utils.dataset_processors import * # Dataset and generator processing functions\n",
    "from utils.plot_functions import * # Plotting functions\n",
    "from utils.evaluator import * # Complete evaluation program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: None\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES')\n",
    "print(f\"CUDA_VISIBLE_DEVICES: {cuda_visible_devices}\")\n",
    "print(tf.config.get_visible_devices())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse arguments from command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Python config file specified, using default (notebook) config.\n"
     ]
    }
   ],
   "source": [
    "# Detect if running in a Jupyter notebook\n",
    "# Generated using GPT-4o. Prompt: \"Detect if running in a Jupyter notebook\"\n",
    "def in_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        else:\n",
    "            return False  # Other type (terminal, etc.)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "    \n",
    "from_py = False\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Parse command line arguments\")\n",
    "parser.add_argument('--from-py-cfg', type=str,\n",
    "                    help='Path to the config Python file')\n",
    "if not in_notebook():\n",
    "    args = parser.parse_args()\n",
    "    config_file_path = args.from_py_cfg\n",
    "    print(f\"Python Config Path: {config_file_path}\")\n",
    "else:\n",
    "    config_file_path = False\n",
    "\n",
    "if config_file_path:\n",
    "    spec = importlib.util.spec_from_file_location(\"config_module\", config_file_path)\n",
    "    config_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(config_module)\n",
    "    config = config_module.cfg\n",
    "    print(\"Loaded config from Python file:\")\n",
    "    pprint(config)\n",
    "    # Datasets, models, and hyperparameters are mandatory and must be processed now.\n",
    "    training_datasets = config.get('datasets', {})\n",
    "    full_test_dir = config.get('test')\n",
    "    base_models = config.get('keras_models', [])\n",
    "    custom_models = config.get('custom_models', [])\n",
    "    hyperparameters = config.get('hyperparameters')\n",
    "    default_hyperparameters = default_cfg.get('hyperparameters', {})\n",
    "    if hyperparameters is None or len(hyperparameters) == 0:\n",
    "        print(\"No training hyperparameters defined in config, using defaults.\")\n",
    "        hyperparameters = default_hyperparameters\n",
    "    else:\n",
    "        for key, value in default_hyperparameters.items():\n",
    "            if key not in hyperparameters:\n",
    "                print(f\"Missing hyperparameter - falling back to default {key}:{default_hyperparameters[key]}\")\n",
    "                hyperparameters[key] = default_hyperparameters[key]\n",
    "    from_py = True # Successfully completed the import\n",
    "else:\n",
    "    print(\"No Python config file specified, using default (notebook) config.\")\n",
    "    config = default_cfg\n",
    "    training_datasets = config.get('datasets', {})\n",
    "    base_models = config.get('keras_models', [])\n",
    "    custom_models = config.get('custom_models', [])\n",
    "    hyperparameters = config.get('hyperparameters', {\"epochs\": 50, \"batch_size\": 32})\n",
    "    full_test_dir = config.get('test')\n",
    "\n",
    "if training_datasets is None or len(training_datasets) == 0:\n",
    "    raise ValueError(\"No train datasets defined in config.\")\n",
    "\n",
    "if base_models is None or len(base_models) == 0:\n",
    "    if custom_models is None or len(custom_models) == 0:\n",
    "        raise ValueError(\"No models defined in config.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dirs = [training_datasets[ds].get('train') for ds in training_datasets]\n",
    "test_dirs = [training_datasets[ds].get('test') for ds in training_datasets]\n",
    "val_dirs = [training_datasets[ds].get('val') for ds in training_datasets]\n",
    "\n",
    "all_dirs = train_dirs + test_dirs + val_dirs + [full_test_dir]\n",
    "all_dirs = [d for d in all_dirs if d is not None] # Remove None values\n",
    "\n",
    "# Combine base_models and custom_models\n",
    "all_models = base_models + custom_models\n",
    "# Create a list to keep track of which models are custom\n",
    "is_custom_model = [False] * len(base_models) + [True] * len(custom_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if from_py:\n",
    "    epochs = hyperparameters.get('epochs') # Guaranteed to be present\n",
    "    batch_size = hyperparameters.get('batch_size') # Guaranteed to be present\n",
    "    img_height = config.get('image_height', default_cfg.get('image_height'))\n",
    "    img_width = config.get('image_width', default_cfg.get('image_width'))\n",
    "    optimizer_fn = config.get('optimizer', default_cfg.get('optimizer'))\n",
    "    loss_fn = config.get('loss', default_cfg.get('loss'))\n",
    "    callbacks_list = config.get('callbacks', default_cfg.get('callbacks'))\n",
    "    metrics_list = config.get('metrics', default_cfg.get('metrics'))\n",
    "    enforce_image_size = config.get('enforce_image_settings', default_cfg.get('enforce_image_settings'))\n",
    "    val_size = config.get('val_size', default_cfg.get('val_size'))\n",
    "else:\n",
    "    epochs = hyperparameters.get('epochs', 50)\n",
    "    batch_size = hyperparameters.get('batch_size', 32)\n",
    "    img_height = default_cfg.get('image_height', 224)\n",
    "    img_width = default_cfg.get('image_width', 224)\n",
    "    optimizer_fn = default_cfg.get('optimizer', 'adam')\n",
    "    loss_fn = default_cfg.get('loss', 'binary_crossentropy')\n",
    "    callbacks_list = default_cfg.get('callbacks', [])\n",
    "    metrics_list = default_cfg.get('metrics', ['accuracy'])\n",
    "    enforce_image_size = default_cfg.get('enforce_image_settings', False)\n",
    "    val_size = default_cfg.get('val_size', 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enforce defined resolution and colour mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting image properties in datasets\\dataset_1\\train\n",
      "Adjusting image properties in datasets\\dataset_2\\Training\n",
      "Adjusting image properties in datasets\\dataset_1\\test\n",
      "Adjusting image properties in datasets\\dataset_2\\Testing\n",
      "Adjusting image properties in datasets\\dataset_1\\val\n",
      "Adjusting image properties in datasets\\d4_test\n"
     ]
    }
   ],
   "source": [
    "if enforce_image_size:\n",
    "    for directory in all_dirs:\n",
    "        print(f\"Adjusting image properties in {directory}\")\n",
    "        enforce_image_params(directory, target_size=(img_width, img_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: The Wildfire Dataset\n",
      "Augmenting The Wildfire Dataset\n",
      "Creating generators for training\n",
      "Found 1887 images belonging to 2 classes.\n",
      "Found 1887 images belonging to 2 classes.\n",
      "Found 402 images belonging to 2 classes.\n",
      "Found 402 images belonging to 2 classes.\n",
      "--------------------\n",
      "Number of samples in generator: 1887\n",
      "Number of classes: 2\n",
      "--------------------\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "Class names: ['fire', 'nofire']\n",
      "Dataset Class Counts:\n",
      "fire: 730\n",
      "nofire: 1157\n",
      "\n",
      "Augmented Dataset Class Counts:\n",
      "fire: 730\n",
      "nofire: 1157\n",
      "\n",
      "\n",
      "Combined Dataset Class Counts:\n",
      "fire: 1460\n",
      "nofire: 2314\n",
      "--------------------\n",
      "--------------------\n",
      "Number of samples in generator: 402\n",
      "Number of classes: 2\n",
      "--------------------\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "Class names: ['fire', 'nofire']\n",
      "Dataset Class Counts:\n",
      "fire: 156\n",
      "nofire: 246\n",
      "\n",
      "Augmented Dataset Class Counts:\n",
      "fire: 156\n",
      "nofire: 246\n",
      "\n",
      "\n",
      "Combined Dataset Class Counts:\n",
      "fire: 312\n",
      "nofire: 492\n",
      "--------------------\n",
      "Processing: DeepFire\n",
      "Augmenting DeepFire\n",
      "Creating generators for training\n",
      "Found 1216 images belonging to 2 classes.\n",
      "Found 304 images belonging to 2 classes.\n",
      "Found 1216 images belonging to 2 classes.\n",
      "Found 304 images belonging to 2 classes.\n",
      "--------------------\n",
      "Number of samples in generator: 1216\n",
      "Number of classes: 2\n",
      "--------------------\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "Class names: ['fire', 'nofire']\n",
      "Dataset Class Counts:\n",
      "fire: 608\n",
      "nofire: 608\n",
      "\n",
      "Augmented Dataset Class Counts:\n",
      "fire: 608\n",
      "nofire: 608\n",
      "\n",
      "\n",
      "Combined Dataset Class Counts:\n",
      "fire: 1216\n",
      "nofire: 1216\n",
      "--------------------\n",
      "--------------------\n",
      "Number of samples in generator: 304\n",
      "Number of classes: 2\n",
      "--------------------\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "Class names: ['fire', 'nofire']\n",
      "Dataset Class Counts:\n",
      "fire: 152\n",
      "nofire: 152\n",
      "\n",
      "Augmented Dataset Class Counts:\n",
      "fire: 152\n",
      "nofire: 152\n",
      "\n",
      "\n",
      "Combined Dataset Class Counts:\n",
      "fire: 304\n",
      "nofire: 304\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_names = []\n",
    "train_datasets = [] # [ (dataset_1_train, dataset_2_train), ... ]\n",
    "train_sizes = [] # [ (dataset_1_train_size, dataset_2_train_size), ... ]\n",
    "val_datasets = [] # [ (dataset_1_val, dataset_2_val), ... ]\n",
    "val_sizes = [] # [ (dataset_1_val_size, dataset_2_val_size), ... ]\n",
    "train_counts = [] # [ (dataset_1_train_counts, dataset_2_train_counts), ... ]\n",
    "val_counts = [] # [ (dataset_1_val_counts, dataset_2_val_counts), ... ]\n",
    "\n",
    "for d in training_datasets:\n",
    "    print(f\"Processing: {d}\")\n",
    "    train_dir = training_datasets[d].get('train')\n",
    "    augment = training_datasets[d].get('augment', True)\n",
    "    print(\"Augmenting\" if augment else \"Not augmenting\", d)\n",
    "    # Apply original and augmented data generators for training\n",
    "    print(\"Creating generators for training\")\n",
    "    if \"val\" in training_datasets[d]:\n",
    "        train_generator, augmented_train_generator = create_generators(train_dir, batch_size=batch_size, augment=augment, img_width=img_width, img_height=img_height)\n",
    "        val_generator, augmented_val_generator = create_generators(training_datasets[d]['val'], batch_size=batch_size, augment=augment, shuffle=False, img_width=img_width, img_height=img_height)\n",
    "    else:\n",
    "        train_generator, augmented_train_generator, val_generator, augmented_val_generator = create_split_generators(train_dir, val_size=val_size, batch_size=batch_size, augment=augment, img_width=img_width, img_height=img_height)\n",
    "\n",
    "    train_samples = samples_from_generators([train_generator, augmented_train_generator])\n",
    "    train_count_dict = class_counts_from_generators(train_generator, augmented_train_generator)\n",
    "    train_dataset = generators_to_dataset([train_generator, augmented_train_generator], batch_size=batch_size, img_height=img_height, img_width=img_width)\n",
    "    \n",
    "    val_samples = samples_from_generators([val_generator, augmented_val_generator])\n",
    "    val_count_dict = class_counts_from_generators(val_generator, augmented_val_generator)\n",
    "    val_dataset = generators_to_dataset([val_generator, augmented_val_generator], batch_size=batch_size, img_height=img_height, img_width=img_width)\n",
    "    \n",
    "    # Calculate the number of samples for training and validation\n",
    "    train_sizes.append(train_samples)\n",
    "    val_sizes.append(val_samples)\n",
    "    \n",
    "    train_counts.append(train_count_dict)\n",
    "    val_counts.append(val_count_dict)\n",
    "    train_datasets.append(train_dataset)\n",
    "    val_datasets.append(val_dataset)\n",
    "    dataset_names.append(d)\n",
    "    \n",
    "# Ensure that the lengths are consistent across the board before continuing\n",
    "assert len(train_sizes) == len(train_datasets) == len(val_sizes) == len(val_datasets) == len(val_counts) == len(train_counts) == len(dataset_names), \"Dataset lengths are inconsistent.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute Force Combinatorial Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_combos = [] # [(0,), (1,), (0, 1), ...] where 0, 1 are the indices of the datasets within their respective lists\n",
    "for r in range(1, len(dataset_names) + 1):\n",
    "    dataset_combos.extend(combinations(range(len(dataset_names)), r))\n",
    "    \n",
    "combined_training_datasets = []\n",
    "combined_val_datasets = []\n",
    "combined_dataset_names = []\n",
    "steps_per_epoch_list = []\n",
    "validation_steps_list = []\n",
    "train_counts_list = []\n",
    "val_counts_list = []\n",
    "\n",
    "for combo in dataset_combos:\n",
    "    training_dataset = None\n",
    "    val_dataset = None\n",
    "    train_size = None\n",
    "    val_size = None\n",
    "    train_count = None\n",
    "    val_count = None\n",
    "    for idx in combo:\n",
    "        if training_dataset is None:\n",
    "            training_dataset = train_datasets[idx]\n",
    "            val_dataset = val_datasets[idx]\n",
    "            train_size = train_sizes[idx]\n",
    "            val_size = val_sizes[idx]\n",
    "            train_count = train_counts[idx]\n",
    "            val_count = val_counts[idx]\n",
    "        else:\n",
    "            training_dataset = training_dataset.concatenate(train_datasets[idx])\n",
    "            val_dataset = val_dataset.concatenate(val_datasets[idx])\n",
    "            train_size += train_sizes[idx]\n",
    "            val_size += val_sizes[idx]\n",
    "            train_count = {k: train_count.get(k, 0) + train_counts[idx].get(k, 0) for k in set(train_count) | set(train_counts[idx])}\n",
    "            val_count = {k: val_count.get(k, 0) + val_counts[idx].get(k, 0) for k in set(val_count) | set(val_counts[idx])}\n",
    "        train_count = {k: int(v) for k, v in train_count.items()}\n",
    "        val_count = {k: int(v) for k, v in val_count.items()}\n",
    "\n",
    "    combined_dataset_names.append(\"_\".join([dataset_names[idx] for idx in combo]))\n",
    "    combined_training_datasets.append(training_dataset)\n",
    "    combined_val_datasets.append(val_dataset)\n",
    "    steps_per_epoch_list.append(train_size // batch_size)\n",
    "    validation_steps_list.append(val_size // batch_size)\n",
    "    train_counts_list.append(train_count)\n",
    "    val_counts_list.append(val_count)\n",
    "\n",
    "    training_params = list(zip(combined_dataset_names, combined_training_datasets, combined_val_datasets, steps_per_epoch_list, validation_steps_list, train_counts_list, val_counts_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "\n",
      "\n",
      "Test Dataset Class Counts:\n",
      "fire: 100\n",
      "nofire: 100\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if full_test_dir is None:\n",
    "    test_generators = []\n",
    "    print(\"No target test directory provided, merging all tests from provided datasets if available.\")\n",
    "    for d in test_dirs:\n",
    "        if d is not None:\n",
    "            test_generators.append(create_generators(d, batch_size=batch_size, augment=False, shuffle=False, img_height=img_height, img_width=img_width)[0]) # No augmentation/shuffle for testing\n",
    "    if len(test_generators) == 0:\n",
    "        raise ValueError(\"No tests found in the provided datasets.\")\n",
    "    \n",
    "    true_labels = np.concatenate([gen.classes for gen in test_generators])\n",
    "    test_dataset = generators_to_dataset(test_generators, batch_size=batch_size)\n",
    "    test_steps = sum([gen.samples for gen in test_generators]) // batch_size\n",
    "    print(\"Test Dataset Class Counts:\")\n",
    "    for gen in test_generators:\n",
    "        print(\"Class indices:\", gen.class_indices)\n",
    "        for class_name, class_index in gen.class_indices.items():\n",
    "            print(f\"{class_name}: {sum(gen.classes == class_index)}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "else:\n",
    "    test_generator, augmented_test_generator = create_generators(full_test_dir, batch_size=batch_size, augment=False, shuffle=False, img_height=img_height, img_width=img_width) # No augmentation/shuffle for testing\n",
    "    test_dataset = create_dataset(test_generator, batch_size=batch_size, img_height=img_height, img_width=img_width)\n",
    "    test_steps = test_generator.samples // batch_size\n",
    "    true_labels = test_generator.classes\n",
    "    print(\"Class indices:\", test_generator.class_indices)\n",
    "    print(\"\\n\")\n",
    "    print(\"Test Dataset Class Counts:\")\n",
    "    for class_name, class_index in test_generator.class_indices.items():\n",
    "        print(f\"{class_name}: {sum(test_generator.classes == class_index)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "true_labels = true_labels[: (len(true_labels) // batch_size) * batch_size] # Ensure that the true labels are divisible by the batch size to avoid size mismatch with predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_model(bm, custom=False, to_dir=TEMP_DIR):\n",
    "    if custom:\n",
    "        model = bm\n",
    "        model.compile(optimizer=optimizer_fn, loss=loss_fn, metrics=metrics_list)\n",
    "        os.makedirs(os.path.join(to_dir, model.name), exist_ok=True)\n",
    "        model.save_weights(os.path.join(to_dir, model.name, f\"{model.name}_initial.weights.h5\"))\n",
    "        return model\n",
    "    \n",
    "    base_model = bm(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(img_height, img_width, 3)\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create the model\n",
    "    inputs = Input(shape=(img_height, img_width, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name=bm.__name__)\n",
    "    model.compile(optimizer=optimizer_fn, loss=loss_fn, metrics=metrics_list)\n",
    "    os.makedirs(os.path.join(to_dir, model.name), exist_ok=True)\n",
    "    model.save_weights(os.path.join(to_dir, model.name, f\"{model.name}_initial.weights.h5\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the models and combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "run_number = len([d for d in os.listdir(\"runs\") if os.path.isdir(os.path.join(\"runs\", d)) and d.startswith('run_')]) + 1\n",
    "run_dir = os.path.join(\"runs\", f\"run_{run_number}\")\n",
    "os.makedirs(run_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    \"datasets\": training_datasets,\n",
    "    \"val_size\": val_size,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"test_dirs\": test_dirs,\n",
    "    \"full_test\": full_test_dir,\n",
    "}\n",
    "\n",
    "with open(os.path.join(run_dir, \"run_config.json\"), \"w\") as f:\n",
    "    json.dump(run_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MobileNetV3Small\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"MobileNetV3Small\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ MobileNetV3Small (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ MobileNetV3Small (\u001b[38;5;33mFunctional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      │       \u001b[38;5;34m939,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │         \u001b[38;5;34m2,304\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m147,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,090,417</span> (4.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,090,417\u001b[0m (4.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">149,633</span> (584.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m149,633\u001b[0m (584.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">940,784</span> (3.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m940,784\u001b[0m (3.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: MobileNetV3Small on dataset: The Wildfire Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "\u001b[1m 59/117\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 198ms/step - accuracy: 0.5927 - auc: 0.6247 - f1_score: 0.6372 - loss: 0.8493 - precision: 0.6788 - recall: 0.6060"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 135ms/step - accuracy: 0.6018 - auc: 0.6314 - f1_score: 0.6493 - loss: 0.8254 - precision: 0.6909 - recall: 0.6192 - val_accuracy: 0.6119 - val_auc: 0.6751 - val_f1_score: 0.6325 - val_loss: 0.6694 - val_precision: 0.6119 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 119ms/step - accuracy: 0.6614 - auc: 0.7014 - f1_score: 0.7157 - loss: 0.7051 - precision: 0.7229 - recall: 0.7165 - val_accuracy: 0.6119 - val_auc: 0.6833 - val_f1_score: 0.6325 - val_loss: 0.6943 - val_precision: 0.6119 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 121ms/step - accuracy: 0.6760 - auc: 0.7098 - f1_score: 0.7438 - loss: 0.6664 - precision: 0.7326 - recall: 0.7630 - val_accuracy: 0.6119 - val_auc: 0.7436 - val_f1_score: 0.6325 - val_loss: 0.6794 - val_precision: 0.6119 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 126ms/step - accuracy: 0.6739 - auc: 0.7251 - f1_score: 0.7334 - loss: 0.6320 - precision: 0.7224 - recall: 0.7556 - val_accuracy: 0.6119 - val_auc: 0.7253 - val_f1_score: 0.6325 - val_loss: 0.6674 - val_precision: 0.6119 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 125ms/step - accuracy: 0.6788 - auc: 0.7315 - f1_score: 0.7436 - loss: 0.6246 - precision: 0.7274 - recall: 0.7685 - val_accuracy: 0.6119 - val_auc: 0.7495 - val_f1_score: 0.6325 - val_loss: 0.6504 - val_precision: 0.6119 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 123ms/step - accuracy: 0.6901 - auc: 0.7320 - f1_score: 0.7552 - loss: 0.6195 - precision: 0.7330 - recall: 0.7836 - val_accuracy: 0.6095 - val_auc: 0.7550 - val_f1_score: 0.6313 - val_loss: 0.6339 - val_precision: 0.6110 - val_recall: 0.9959 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 126ms/step - accuracy: 0.6963 - auc: 0.7489 - f1_score: 0.7552 - loss: 0.5999 - precision: 0.7399 - recall: 0.7786 - val_accuracy: 0.6343 - val_auc: 0.7616 - val_f1_score: 0.6291 - val_loss: 0.6133 - val_precision: 0.6286 - val_recall: 0.9837 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 124ms/step - accuracy: 0.7060 - auc: 0.7523 - f1_score: 0.7615 - loss: 0.5926 - precision: 0.7573 - recall: 0.7694 - val_accuracy: 0.6741 - val_auc: 0.7816 - val_f1_score: 0.6226 - val_loss: 0.5882 - val_precision: 0.6593 - val_recall: 0.9675 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 124ms/step - accuracy: 0.7011 - auc: 0.7619 - f1_score: 0.7629 - loss: 0.5787 - precision: 0.7331 - recall: 0.7990 - val_accuracy: 0.7139 - val_auc: 0.7815 - val_f1_score: 0.5896 - val_loss: 0.5800 - val_precision: 0.7191 - val_recall: 0.8740 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 122ms/step - accuracy: 0.6936 - auc: 0.7492 - f1_score: 0.7554 - loss: 0.5819 - precision: 0.7305 - recall: 0.7870 - val_accuracy: 0.7537 - val_auc: 0.7886 - val_f1_score: 0.5884 - val_loss: 0.5637 - val_precision: 0.7692 - val_recall: 0.8537 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 121ms/step - accuracy: 0.7002 - auc: 0.7648 - f1_score: 0.7590 - loss: 0.5662 - precision: 0.7460 - recall: 0.7784 - val_accuracy: 0.7637 - val_auc: 0.8159 - val_f1_score: 0.5974 - val_loss: 0.5328 - val_precision: 0.7668 - val_recall: 0.8821 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 120ms/step - accuracy: 0.6978 - auc: 0.7525 - f1_score: 0.7556 - loss: 0.5817 - precision: 0.7403 - recall: 0.7857 - val_accuracy: 0.6915 - val_auc: 0.7681 - val_f1_score: 0.5688 - val_loss: 0.5565 - val_precision: 0.7179 - val_recall: 0.8171 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 119ms/step - accuracy: 0.7090 - auc: 0.7599 - f1_score: 0.7717 - loss: 0.5700 - precision: 0.7483 - recall: 0.8020 - val_accuracy: 0.7338 - val_auc: 0.7845 - val_f1_score: 0.5569 - val_loss: 0.5500 - val_precision: 0.7884 - val_recall: 0.7724 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 120ms/step - accuracy: 0.7192 - auc: 0.7720 - f1_score: 0.7699 - loss: 0.5560 - precision: 0.7545 - recall: 0.7918 - val_accuracy: 0.7313 - val_auc: 0.8058 - val_f1_score: 0.5748 - val_loss: 0.5315 - val_precision: 0.7575 - val_recall: 0.8252 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 125ms/step - accuracy: 0.7053 - auc: 0.7596 - f1_score: 0.7680 - loss: 0.5659 - precision: 0.7431 - recall: 0.7980 - val_accuracy: 0.7438 - val_auc: 0.8096 - val_f1_score: 0.5656 - val_loss: 0.5270 - val_precision: 0.7849 - val_recall: 0.8008 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 121ms/step - accuracy: 0.6918 - auc: 0.7482 - f1_score: 0.7522 - loss: 0.5798 - precision: 0.7349 - recall: 0.7757 - val_accuracy: 0.7040 - val_auc: 0.7979 - val_f1_score: 0.5752 - val_loss: 0.5366 - val_precision: 0.7276 - val_recall: 0.8252 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 123ms/step - accuracy: 0.6977 - auc: 0.7612 - f1_score: 0.7626 - loss: 0.5601 - precision: 0.7341 - recall: 0.7955 - val_accuracy: 0.7114 - val_auc: 0.7843 - val_f1_score: 0.5715 - val_loss: 0.5430 - val_precision: 0.7407 - val_recall: 0.8130 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 119ms/step - accuracy: 0.7189 - auc: 0.7744 - f1_score: 0.7753 - loss: 0.5527 - precision: 0.7558 - recall: 0.7995 - val_accuracy: 0.7139 - val_auc: 0.7876 - val_f1_score: 0.5432 - val_loss: 0.5463 - val_precision: 0.7652 - val_recall: 0.7683 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 120ms/step - accuracy: 0.7191 - auc: 0.7898 - f1_score: 0.7734 - loss: 0.5393 - precision: 0.7485 - recall: 0.8056 - val_accuracy: 0.7214 - val_auc: 0.7991 - val_f1_score: 0.5711 - val_loss: 0.5287 - val_precision: 0.7597 - val_recall: 0.7967 - learning_rate: 0.0010\n",
      "Epoch 20/80\n",
      "\u001b[1m 59/117\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 208ms/step - accuracy: 0.7152 - auc: 0.7712 - f1_score: 0.7762 - loss: 0.5545 - precision: 0.7513 - recall: 0.8091\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 126ms/step - accuracy: 0.7166 - auc: 0.7742 - f1_score: 0.7772 - loss: 0.5531 - precision: 0.7477 - recall: 0.8163 - val_accuracy: 0.7463 - val_auc: 0.7883 - val_f1_score: 0.5805 - val_loss: 0.5425 - val_precision: 0.7707 - val_recall: 0.8333 - learning_rate: 0.0010\n",
      "Training time: 294.78 seconds\n",
      "Evaluating MobileNetV3Small on The Wildfire Dataset...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 772ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5820 - auc: 0.5535 - f1_score: 0.3282 - loss: 0.7984 - precision: 0.4138 - recall: 0.5964            \n",
      "Training results:\n",
      "{'evaluation': {'accuracy': 0.625,\n",
      "                'auc': 0.73375004529953,\n",
      "                'f1_score': 0.49713388085365295,\n",
      "                'loss': 0.6906709671020508,\n",
      "                'precision': 0.607692301273346,\n",
      "                'recall': 0.7900000214576721},\n",
      " 'history': {'accuracy': [0.6110227704048157,\n",
      "                          0.6576576828956604,\n",
      "                          0.6671966314315796,\n",
      "                          0.6767355799674988,\n",
      "                          0.6762056350708008,\n",
      "                          0.6894541382789612,\n",
      "                          0.6968733668327332,\n",
      "                          0.6995230317115784,\n",
      "                          0.6979332566261292,\n",
      "                          0.7027027010917664,\n",
      "                          0.6958134770393372,\n",
      "                          0.6995230317115784,\n",
      "                          0.7053524255752563,\n",
      "                          0.7244303226470947,\n",
      "                          0.7095919251441956,\n",
      "                          0.6936936974525452,\n",
      "                          0.6995230317115784,\n",
      "                          0.7101218700408936,\n",
      "                          0.7180709838867188,\n",
      "                          0.7180709838867188],\n",
      "             'auc': [0.6382029056549072,\n",
      "                     0.6960804462432861,\n",
      "                     0.7074323892593384,\n",
      "                     0.7270835638046265,\n",
      "                     0.7279554605484009,\n",
      "                     0.7355761528015137,\n",
      "                     0.750946044921875,\n",
      "                     0.7442559003829956,\n",
      "                     0.7541883587837219,\n",
      "                     0.7565497756004333,\n",
      "                     0.7577734589576721,\n",
      "                     0.7522246837615967,\n",
      "                     0.7594280242919922,\n",
      "                     0.7737085819244385,\n",
      "                     0.7652028799057007,\n",
      "                     0.7507991790771484,\n",
      "                     0.7634156346321106,\n",
      "                     0.7675459384918213,\n",
      "                     0.7854353189468384,\n",
      "                     0.7772350907325745],\n",
      "             'f1_score': [0.6616716980934143,\n",
      "                          0.7141907215118408,\n",
      "                          0.7344235777854919,\n",
      "                          0.736733078956604,\n",
      "                          0.7389940023422241,\n",
      "                          0.7539617419242859,\n",
      "                          0.7567024230957031,\n",
      "                          0.755842387676239,\n",
      "                          0.7598022818565369,\n",
      "                          0.7644432783126831,\n",
      "                          0.7560396790504456,\n",
      "                          0.7560508847236633,\n",
      "                          0.7669059634208679,\n",
      "                          0.7759448289871216,\n",
      "                          0.7691766619682312,\n",
      "                          0.7561438083648682,\n",
      "                          0.7636107802391052,\n",
      "                          0.7690035700798035,\n",
      "                          0.7749749422073364,\n",
      "                          0.7781214714050293],\n",
      "             'learning_rate': [0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513,\n",
      "                               0.0010000000474974513],\n",
      "             'loss': [0.8010343909263611,\n",
      "                      0.7078707218170166,\n",
      "                      0.6724861860275269,\n",
      "                      0.6314789056777954,\n",
      "                      0.6286563277244568,\n",
      "                      0.6130438446998596,\n",
      "                      0.5928035378456116,\n",
      "                      0.6038680076599121,\n",
      "                      0.5818384885787964,\n",
      "                      0.5724877119064331,\n",
      "                      0.5745338797569275,\n",
      "                      0.5847359895706177,\n",
      "                      0.573959231376648,\n",
      "                      0.5501011610031128,\n",
      "                      0.5617941617965698,\n",
      "                      0.5766581892967224,\n",
      "                      0.5600448846817017,\n",
      "                      0.5591293573379517,\n",
      "                      0.5436663031578064,\n",
      "                      0.5517281293869019],\n",
      "             'precision': [0.7031700015068054,\n",
      "                           0.724319577217102,\n",
      "                           0.7134786248207092,\n",
      "                           0.7284879088401794,\n",
      "                           0.7237704992294312,\n",
      "                           0.7300564050674438,\n",
      "                           0.738386332988739,\n",
      "                           0.7478991746902466,\n",
      "                           0.7372675538063049,\n",
      "                           0.7395498156547546,\n",
      "                           0.7387387156486511,\n",
      "                           0.7386731505393982,\n",
      "                           0.7398244142532349,\n",
      "                           0.7647547721862793,\n",
      "                           0.7465587258338928,\n",
      "                           0.7344129681587219,\n",
      "                           0.7348726391792297,\n",
      "                           0.7483713626861572,\n",
      "                           0.7538586258888245,\n",
      "                           0.7439500093460083],\n",
      "             'recall': [0.6326707005500793,\n",
      "                        0.7130510210990906,\n",
      "                        0.7640449404716492,\n",
      "                        0.7536733150482178,\n",
      "                        0.7631806135177612,\n",
      "                        0.7830596566200256,\n",
      "                        0.7830596566200256,\n",
      "                        0.7692307829856873,\n",
      "                        0.788245439529419,\n",
      "                        0.7951598763465881,\n",
      "                        0.7796024084091187,\n",
      "                        0.7891097664833069,\n",
      "                        0.8012100458145142,\n",
      "                        0.7951598763465881,\n",
      "                        0.796888530254364,\n",
      "                        0.7839239239692688,\n",
      "                        0.7977527976036072,\n",
      "                        0.794295608997345,\n",
      "                        0.8020743131637573,\n",
      "                        0.8236819505691528],\n",
      "             'val_accuracy': [0.611940324306488,\n",
      "                              0.611940324306488,\n",
      "                              0.611940324306488,\n",
      "                              0.611940324306488,\n",
      "                              0.611940324306488,\n",
      "                              0.6094527244567871,\n",
      "                              0.6343283653259277,\n",
      "                              0.6741293668746948,\n",
      "                              0.7139303684234619,\n",
      "                              0.753731369972229,\n",
      "                              0.7636815905570984,\n",
      "                              0.6915422677993774,\n",
      "                              0.7338308691978455,\n",
      "                              0.7313432693481445,\n",
      "                              0.7437810897827148,\n",
      "                              0.7039800882339478,\n",
      "                              0.711442768573761,\n",
      "                              0.7139303684234619,\n",
      "                              0.7213930487632751,\n",
      "                              0.746268630027771],\n",
      "             'val_auc': [0.6751093864440918,\n",
      "                         0.6833046674728394,\n",
      "                         0.7436288595199585,\n",
      "                         0.7253361940383911,\n",
      "                         0.7495179176330566,\n",
      "                         0.7549901008605957,\n",
      "                         0.7615827322006226,\n",
      "                         0.7816343307495117,\n",
      "                         0.7815040349960327,\n",
      "                         0.7886048555374146,\n",
      "                         0.8158614635467529,\n",
      "                         0.7681493759155273,\n",
      "                         0.7845267653465271,\n",
      "                         0.8058030605316162,\n",
      "                         0.8096206188201904,\n",
      "                         0.7978554368019104,\n",
      "                         0.7842792272567749,\n",
      "                         0.7876276969909668,\n",
      "                         0.7991061806678772,\n",
      "                         0.7883051633834839],\n",
      "             'val_f1_score': [0.632478654384613,\n",
      "                              0.632478654384613,\n",
      "                              0.632478654384613,\n",
      "                              0.632478654384613,\n",
      "                              0.632478654384613,\n",
      "                              0.6312576532363892,\n",
      "                              0.629109263420105,\n",
      "                              0.6226049661636353,\n",
      "                              0.5896160006523132,\n",
      "                              0.5884454250335693,\n",
      "                              0.5973542928695679,\n",
      "                              0.568831741809845,\n",
      "                              0.556948721408844,\n",
      "                              0.5748016238212585,\n",
      "                              0.5656449198722839,\n",
      "                              0.5752321481704712,\n",
      "                              0.5714828968048096,\n",
      "                              0.5432157516479492,\n",
      "                              0.5710812211036682,\n",
      "                              0.580513060092926],\n",
      "             'val_loss': [0.6694157719612122,\n",
      "                          0.6942663788795471,\n",
      "                          0.6794072985649109,\n",
      "                          0.6673961877822876,\n",
      "                          0.6503676176071167,\n",
      "                          0.633855938911438,\n",
      "                          0.6133162379264832,\n",
      "                          0.5881635546684265,\n",
      "                          0.580048143863678,\n",
      "                          0.5637485384941101,\n",
      "                          0.5328102111816406,\n",
      "                          0.5565474033355713,\n",
      "                          0.5500083565711975,\n",
      "                          0.5315058827400208,\n",
      "                          0.5270224213600159,\n",
      "                          0.5365792512893677,\n",
      "                          0.5430065989494324,\n",
      "                          0.5462902784347534,\n",
      "                          0.5286880731582642,\n",
      "                          0.5424970984458923],\n",
      "             'val_precision': [0.611940324306488,\n",
      "                               0.611940324306488,\n",
      "                               0.611940324306488,\n",
      "                               0.611940324306488,\n",
      "                               0.611940324306488,\n",
      "                               0.6109725832939148,\n",
      "                               0.6285714507102966,\n",
      "                               0.6592797636985779,\n",
      "                               0.7190635204315186,\n",
      "                               0.7692307829856873,\n",
      "                               0.7667844295501709,\n",
      "                               0.7178571224212646,\n",
      "                               0.7883817553520203,\n",
      "                               0.7574626803398132,\n",
      "                               0.7848605513572693,\n",
      "                               0.7275985479354858,\n",
      "                               0.7407407164573669,\n",
      "                               0.7651821970939636,\n",
      "                               0.7596899271011353,\n",
      "                               0.7706766724586487],\n",
      "             'val_recall': [1.0,\n",
      "                            1.0,\n",
      "                            1.0,\n",
      "                            1.0,\n",
      "                            1.0,\n",
      "                            0.9959349632263184,\n",
      "                            0.9837398529052734,\n",
      "                            0.9674796462059021,\n",
      "                            0.8739837408065796,\n",
      "                            0.8536585569381714,\n",
      "                            0.8821138143539429,\n",
      "                            0.8170731663703918,\n",
      "                            0.772357702255249,\n",
      "                            0.8252032399177551,\n",
      "                            0.8008130192756653,\n",
      "                            0.8252032399177551,\n",
      "                            0.8130081295967102,\n",
      "                            0.7682926654815674,\n",
      "                            0.7967479825019836,\n",
      "                            0.8333333134651184]},\n",
      " 'optimal_threshold': 0.5996112823486328,\n",
      " 'train_counts': {'fire': 730, 'nofire': 1157},\n",
      " 'train_counts_total': 1887,\n",
      " 'train_dataset_size': 3744,\n",
      " 'training_time': 294.7821295261383,\n",
      " 'val_counts': {'fire': 156, 'nofire': 246},\n",
      " 'val_counts_total': 402,\n",
      " 'val_dataset_size': 800}\n",
      "Training model: MobileNetV3Small on dataset: DeepFire\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "\u001b[1m38/76\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 204ms/step - accuracy: 0.6370 - auc: 0.6998 - f1_score: 0.6203 - loss: 0.7524 - precision: 0.6497 - recall: 0.6513"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 145ms/step - accuracy: 0.6536 - auc: 0.7143 - f1_score: 0.6374 - loss: 0.7216 - precision: 0.6599 - recall: 0.6526 - val_accuracy: 0.5000 - val_auc: 0.8227 - val_f1_score: 0.6490 - val_loss: 0.7143 - val_precision: 0.5000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - accuracy: 0.7497 - auc: 0.8193 - f1_score: 0.7550 - loss: 0.5707 - precision: 0.7322 - recall: 0.7824 - val_accuracy: 0.5000 - val_auc: 0.8596 - val_f1_score: 0.6700 - val_loss: 0.6910 - val_precision: 0.5000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.7417 - auc: 0.8164 - f1_score: 0.7374 - loss: 0.5849 - precision: 0.7382 - recall: 0.7457 - val_accuracy: 0.5066 - val_auc: 0.8331 - val_f1_score: 0.6644 - val_loss: 0.6823 - val_precision: 0.5033 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 129ms/step - accuracy: 0.7581 - auc: 0.8407 - f1_score: 0.7592 - loss: 0.5285 - precision: 0.7596 - recall: 0.7620 - val_accuracy: 0.5000 - val_auc: 0.7880 - val_f1_score: 0.6601 - val_loss: 0.7091 - val_precision: 0.5000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 130ms/step - accuracy: 0.7683 - auc: 0.8528 - f1_score: 0.7669 - loss: 0.4978 - precision: 0.7631 - recall: 0.7745 - val_accuracy: 0.5000 - val_auc: 0.8627 - val_f1_score: 0.6594 - val_loss: 0.6704 - val_precision: 0.5000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - accuracy: 0.7705 - auc: 0.8652 - f1_score: 0.7677 - loss: 0.4863 - precision: 0.7604 - recall: 0.7830 - val_accuracy: 0.5000 - val_auc: 0.9097 - val_f1_score: 0.6699 - val_loss: 0.6602 - val_precision: 0.5000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 135ms/step - accuracy: 0.7924 - auc: 0.8739 - f1_score: 0.7942 - loss: 0.4625 - precision: 0.7919 - recall: 0.7989 - val_accuracy: 0.5197 - val_auc: 0.8967 - val_f1_score: 0.6676 - val_loss: 0.6464 - val_precision: 0.5101 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 135ms/step - accuracy: 0.7856 - auc: 0.8674 - f1_score: 0.7866 - loss: 0.4901 - precision: 0.7856 - recall: 0.7922 - val_accuracy: 0.5592 - val_auc: 0.9030 - val_f1_score: 0.6998 - val_loss: 0.6302 - val_precision: 0.5315 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 129ms/step - accuracy: 0.8003 - auc: 0.8794 - f1_score: 0.8031 - loss: 0.4518 - precision: 0.7987 - recall: 0.8099 - val_accuracy: 0.5493 - val_auc: 0.8202 - val_f1_score: 0.6924 - val_loss: 0.6529 - val_precision: 0.5260 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - accuracy: 0.7912 - auc: 0.8655 - f1_score: 0.7959 - loss: 0.4750 - precision: 0.7830 - recall: 0.8094 - val_accuracy: 0.7237 - val_auc: 0.9148 - val_f1_score: 0.7793 - val_loss: 0.5706 - val_precision: 0.6453 - val_recall: 0.9934 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 130ms/step - accuracy: 0.7621 - auc: 0.8477 - f1_score: 0.7608 - loss: 0.5086 - precision: 0.7709 - recall: 0.7528 - val_accuracy: 0.5987 - val_auc: 0.8519 - val_f1_score: 0.7146 - val_loss: 0.6104 - val_precision: 0.5551 - val_recall: 0.9934 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.7936 - auc: 0.8702 - f1_score: 0.7859 - loss: 0.4711 - precision: 0.7950 - recall: 0.7867 - val_accuracy: 0.7434 - val_auc: 0.8844 - val_f1_score: 0.7773 - val_loss: 0.5462 - val_precision: 0.6907 - val_recall: 0.8816 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.7828 - auc: 0.8692 - f1_score: 0.7803 - loss: 0.4612 - precision: 0.7717 - recall: 0.7982 - val_accuracy: 0.8158 - val_auc: 0.9077 - val_f1_score: 0.8309 - val_loss: 0.4972 - val_precision: 0.7609 - val_recall: 0.9211 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 131ms/step - accuracy: 0.8051 - auc: 0.8738 - f1_score: 0.8002 - loss: 0.4581 - precision: 0.7894 - recall: 0.8183 - val_accuracy: 0.8553 - val_auc: 0.9373 - val_f1_score: 0.8608 - val_loss: 0.4448 - val_precision: 0.8214 - val_recall: 0.9079 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 131ms/step - accuracy: 0.7908 - auc: 0.8724 - f1_score: 0.7937 - loss: 0.4577 - precision: 0.7812 - recall: 0.8095 - val_accuracy: 0.8684 - val_auc: 0.9530 - val_f1_score: 0.8784 - val_loss: 0.4256 - val_precision: 0.8043 - val_recall: 0.9737 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 129ms/step - accuracy: 0.8045 - auc: 0.8884 - f1_score: 0.8010 - loss: 0.4239 - precision: 0.7958 - recall: 0.8180 - val_accuracy: 0.8026 - val_auc: 0.9014 - val_f1_score: 0.8148 - val_loss: 0.4673 - val_precision: 0.7527 - val_recall: 0.9013 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 137ms/step - accuracy: 0.8185 - auc: 0.8769 - f1_score: 0.8118 - loss: 0.4585 - precision: 0.8136 - recall: 0.8225 - val_accuracy: 0.8355 - val_auc: 0.9195 - val_f1_score: 0.8484 - val_loss: 0.4178 - val_precision: 0.7898 - val_recall: 0.9145 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 135ms/step - accuracy: 0.7929 - auc: 0.8756 - f1_score: 0.7879 - loss: 0.4555 - precision: 0.7788 - recall: 0.8035 - val_accuracy: 0.8289 - val_auc: 0.9176 - val_f1_score: 0.8378 - val_loss: 0.4026 - val_precision: 0.7809 - val_recall: 0.9145 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 139ms/step - accuracy: 0.7839 - auc: 0.8779 - f1_score: 0.7810 - loss: 0.4416 - precision: 0.7740 - recall: 0.7943 - val_accuracy: 0.8191 - val_auc: 0.9103 - val_f1_score: 0.8296 - val_loss: 0.4068 - val_precision: 0.7740 - val_recall: 0.9013 - learning_rate: 0.0010\n",
      "Epoch 20/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 139ms/step - accuracy: 0.7972 - auc: 0.8799 - f1_score: 0.7989 - loss: 0.4406 - precision: 0.7870 - recall: 0.8161 - val_accuracy: 0.8322 - val_auc: 0.9190 - val_f1_score: 0.8353 - val_loss: 0.3688 - val_precision: 0.8217 - val_recall: 0.8487 - learning_rate: 0.0010\n",
      "Epoch 21/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 138ms/step - accuracy: 0.8200 - auc: 0.9014 - f1_score: 0.8136 - loss: 0.3994 - precision: 0.8071 - recall: 0.8315 - val_accuracy: 0.8454 - val_auc: 0.9194 - val_f1_score: 0.8423 - val_loss: 0.3859 - val_precision: 0.8302 - val_recall: 0.8684 - learning_rate: 0.0010\n",
      "Epoch 22/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 136ms/step - accuracy: 0.8205 - auc: 0.8975 - f1_score: 0.8240 - loss: 0.4080 - precision: 0.8068 - recall: 0.8493 - val_accuracy: 0.8355 - val_auc: 0.9125 - val_f1_score: 0.8427 - val_loss: 0.3877 - val_precision: 0.8188 - val_recall: 0.8618 - learning_rate: 0.0010\n",
      "Epoch 23/80\n",
      "\u001b[1m34/76\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 213ms/step - accuracy: 0.8386 - auc: 0.9095 - f1_score: 0.8357 - loss: 0.3920 - precision: 0.8425 - recall: 0.8312"
     ]
    }
   ],
   "source": [
    "training_results = {}\n",
    "results_file = os.path.join(run_dir, 'training_results.json')\n",
    "\n",
    "for base_model, custom_bool in zip(all_models, is_custom_model):\n",
    "    model = generate_model(base_model, custom=custom_bool, to_dir=run_dir) # To display the model summary\n",
    "    model.summary()\n",
    "    model_dir = os.path.join(run_dir, model.name)\n",
    "    training_results[model.name] = {}\n",
    "    plot_model(model, show_shapes=True, show_layer_names=True, to_file=os.path.join(model_dir, f\"{model.name}_architecture.png\"))\n",
    "    for dataset_id, train_dataset, val_dataset, steps_per_epoch, validation_steps, train_counts_dict, val_counts_dict in training_params:\n",
    "        model.load_weights(os.path.join(run_dir, model.name, f\"{model.name}_initial.weights.h5\"))\n",
    "        print(f\"Training model: {model.name} on dataset: {dataset_id}\")\n",
    "        \n",
    "        # Record the start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Initial training of the model\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=val_dataset,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks_list\n",
    "        )\n",
    "\n",
    "        # Record the end time\n",
    "        end_time = time.time()\n",
    "        # Calculate the training time\n",
    "        training_time = end_time - start_time\n",
    "        print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "        model_ds_dir = os.path.join(model_dir, dataset_id)\n",
    "        os.makedirs(model_ds_dir, exist_ok=True)\n",
    "        # Save the model\n",
    "        model.save(os.path.join(model_ds_dir, f\"{model.name}_{dataset_id}.keras\"))\n",
    "\n",
    "        ### Evaluation stage ###\n",
    "        optimal_threshold = full_eval(model_ds_dir, history, model, dataset_id, test_dataset, true_labels, test_steps)\n",
    "        \n",
    "        training_results[model.name][dataset_id] = {\n",
    "            'history': history.history,\n",
    "            'training_time': training_time,\n",
    "            'optimal_threshold': float(optimal_threshold),\n",
    "            'train_dataset_size': steps_per_epoch * batch_size, # Includes augmented data (2x)\n",
    "            'val_dataset_size': validation_steps * batch_size, # Includes augmented data (2x)\n",
    "            'train_counts': train_counts_dict,\n",
    "            'val_counts': val_counts_dict,\n",
    "            'train_counts_total': sum(train_counts_dict.values()),\n",
    "            'val_counts_total': sum(val_counts_dict.values()),\n",
    "            \"evaluation\": model.evaluate(test_dataset, return_dict=True, steps=test_steps)\n",
    "        }\n",
    "        print(\"Training results:\")\n",
    "        pprint(training_results[model.name][dataset_id])\n",
    "        # Save the training results to a file after each iteration\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(training_results, f, indent=4)\n",
    "        \n",
    "        model.compile(optimizer=optimizer_fn, loss=loss_fn, metrics=metrics_list) # Reset the model for the next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brute force loop completed!\n",
      "All models are now available at: runs\\run_19\n"
     ]
    }
   ],
   "source": [
    "print(\"Brute force loop completed!\")\n",
    "print(f\"All models are now available at: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = os.path.join(run_dir, \"evaluations\")\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "rows = extract_evaluation_data(training_results)\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(os.path.join(eval_dir, \"training_data.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Time: 209.0201128323873\n",
      "Number of Distinct Models: 1\n",
      "Number of Singular Datasets: 2\n",
      "All evaluations completed!\n",
      "Results are available at: runs\\run_19\\evaluations\n"
     ]
    }
   ],
   "source": [
    "plot_metric_chart(df, \"Evaluation F1 Score\", eval_dir)\n",
    "plot_metric_chart(df, \"Evaluation Accuracy\", eval_dir)\n",
    "plot_metric_chart(df, \"Evaluation Precision\", eval_dir)\n",
    "plot_metric_chart(df, \"Evaluation Recall\", eval_dir)\n",
    "plot_metric_chart(df, \"Evaluation AUC\", eval_dir)\n",
    "plot_metric_chart(df, \"Training Time\", eval_dir)\n",
    "plot_metric_chart(df, \"Train Size\", eval_dir)\n",
    "plot_metric_chart(df, \"Val Size\", eval_dir)\n",
    "\n",
    "plot_time_extrapolation(df, eval_dir)\n",
    "\n",
    "print(\"All evaluations completed!\")\n",
    "print(f\"Results are available at: {eval_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1351460,
     "sourceId": 2247205,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
