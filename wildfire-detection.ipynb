{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Force Pipeline for Wildfire Detection - by Selman Tabet @ https://selman.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Dissertation\\Kaggle Data\\wildfire-detection\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chaos\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "print(socket.gethostname())\n",
    "import os\n",
    "try: # for CUDA enviroment\n",
    "    os.system(\"nvidia-smi\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "# ML libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# Chart generating libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from train_parameters import *\n",
    "from customnet import *\n",
    "from custom_metrics import *\n",
    "\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: None\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES')\n",
    "print(f\"CUDA_VISIBLE_DEVICES: {cuda_visible_devices}\")\n",
    "print(tf.config.get_visible_devices())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse arguments from command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Config Path: None\n"
     ]
    }
   ],
   "source": [
    "# Detect if running in a Jupyter notebook\n",
    "def in_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        else:\n",
    "            return False  # Other type (terminal, etc.)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "    \n",
    "from_py = False\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Parse command line arguments\")\n",
    "parser.add_argument('--from-py-cfg', type=str,\n",
    "                    help='Path to the config Python file')\n",
    "if in_notebook():\n",
    "    args = parser.parse_args([])  # Ignore sys.argv\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "config_file_path = args.from_py_cfg\n",
    "print(f\"Python Config Path: {config_file_path}\")\n",
    "\n",
    "if config_file_path:\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location(\"config_module\", config_file_path)\n",
    "    config_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(config_module)\n",
    "    config = config_module.config\n",
    "    print(\"Loaded config from Python file:\")\n",
    "    print(config)\n",
    "    # Dataset, model, and hyperparameters are mandatory and must be processed now.\n",
    "    datasets_from_cfg = config.get('datasets', {})\n",
    "    if datasets_from_cfg is None or len(datasets_from_cfg) == 0:\n",
    "        raise ValueError(\"No datasets defined in config\")\n",
    "\n",
    "    models_from_cfg = config.get('keras_models', [])\n",
    "    custom_models_from_cfg = config.get('custom_models', [])\n",
    "    if models_from_cfg is None or len(models_from_cfg) == 0:\n",
    "        if custom_models_from_cfg is None or len(custom_models_from_cfg) == 0:\n",
    "            raise ValueError(\"No models defined in config\")\n",
    "    \n",
    "    hyperparameters = config.get('hyperparameters')\n",
    "    if hyperparameters is None or len(hyperparameters) == 0:\n",
    "        print(\"No training hyperparameters defined in config, using defaults\")\n",
    "        hyperparameters = default_train_parameters.get('hyperparameters', {})\n",
    "    for key, value in hyperparameters.items():\n",
    "        if key not in default_train_parameters.get('hyperparameters'):\n",
    "            print(f\"Adding default hyperparameter {key} to {value}\")\n",
    "            hyperparameters[key] = value\n",
    "    from_py = True # Successfully completed the import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset_1\\\\train', 'dataset_2\\\\Training', 'dataset_3']\n",
      "['dataset_1\\\\test', 'dataset_2\\\\Testing', None]\n",
      "['dataset_1\\\\val', None, None]\n",
      "All models: [<function ResNet50V2 at 0x00000243AE66E7A0>, <function VGG19 at 0x00000243AE66F1A0>, <function MobileNetV3Small at 0x00000243AE66CAE0>, <Sequential name=WildfireNet, built=True>]\n",
      "Is custom model: [False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "if from_py:\n",
    "    datasets = datasets_from_cfg\n",
    "else:\n",
    "    datasets = default_train_parameters.get('datasets', {})\n",
    "    \n",
    "train_dirs = [datasets[ds].get('train') for ds in datasets]\n",
    "test_dirs = [datasets[ds].get('test') for ds in datasets]\n",
    "val_dirs = [datasets[ds].get('val') for ds in datasets]\n",
    "\n",
    "print(train_dirs)\n",
    "print(test_dirs)\n",
    "print(val_dirs)\n",
    "\n",
    "if from_py:\n",
    "    base_models = models_from_cfg\n",
    "    custom_models = custom_models_from_cfg\n",
    "else:\n",
    "    base_models = default_train_parameters.get('keras_models', [])\n",
    "    custom_models = default_train_parameters.get('custom_models', [])\n",
    "\n",
    "# Combine base_models and custom_models\n",
    "all_models = base_models + custom_models\n",
    "# Create a list to keep track of which models are custom\n",
    "is_custom_model = [False] * len(base_models) + [True] * len(custom_models)\n",
    "\n",
    "print(\"All models:\", all_models)\n",
    "print(\"Is custom model:\", is_custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize data generators with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ImageDataGenerator for original images (no augmentation)\n",
    "original_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# ImageDataGenerator for augmented images\n",
    "augmented_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if from_py:\n",
    "    epochs = hyperparameters.get('epochs')\n",
    "    batch_size = hyperparameters.get('batch_size')\n",
    "    img_height = config.get('image_height', default_train_parameters.get('image_height'))\n",
    "    img_width = config.get('image_width', default_train_parameters.get('image_width'))\n",
    "    optimizer_fn = config.get('optimizer', default_train_parameters.get('optimizer'))\n",
    "    loss_fn = config.get('loss', default_train_parameters.get('loss'))\n",
    "    callbacks_list = config.get('callbacks', default_train_parameters.get('callbacks'))\n",
    "    metrics_list = config.get('metrics', default_train_parameters.get('metrics'))\n",
    "else:\n",
    "    epochs = default_train_parameters.get('hyperparameters').get('epochs')\n",
    "    batch_size = default_train_parameters.get('hyperparameters').get('batch_size')\n",
    "    img_height = default_train_parameters.get('image_height')\n",
    "    img_width = default_train_parameters.get('image_width')\n",
    "    optimizer_fn = default_train_parameters.get('optimizer')\n",
    "    loss_fn = default_train_parameters.get('loss')\n",
    "    callbacks_list = default_train_parameters.get('callbacks')\n",
    "    metrics_list = default_train_parameters.get('metrics')\n",
    "\n",
    "checkpoint_path = os.path.join(\"checkpoint\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_from_generators(generators):\n",
    "    samples = 0\n",
    "    for generator in generators:\n",
    "        if generator is not None:\n",
    "            samples += generator.samples\n",
    "    return samples\n",
    "\n",
    "# Function to create a tf.data.Dataset from ImageDataGenerator\n",
    "def create_dataset(generator, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator,\n",
    "        output_types=(tf.float32, tf.float32),\n",
    "        output_shapes=([None, img_height, img_width, 3], [None])\n",
    "    )\n",
    "    dataset = dataset.unbatch().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def val_split(dataset, samples=None, val_size=0.2):\n",
    "    # Calculate the number of samples for validation and training\n",
    "    val_size = int(val_size * samples)\n",
    "    train_size = samples - val_size\n",
    "    # Print the sizes of the datasets\n",
    "    print(\"Splitted dataset:\")\n",
    "    print(f\"Training dataset size: {train_size} samples\")\n",
    "    print(f\"Validation dataset size: {val_size} samples\")\n",
    "    val_dataset = dataset.take(val_size)\n",
    "    train_dataset = dataset.skip(val_size)\n",
    "    return train_dataset, val_dataset, train_size, val_size\n",
    "\n",
    "def create_generators(directory, batch_size=32, img_height=224, img_width=224, augment=True, shuffle=True):\n",
    "    generator = original_datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        color_mode='rgb',\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    if augment:\n",
    "        augmented_generator = augmented_datagen.flow_from_directory(\n",
    "            directory,\n",
    "            target_size=(img_height, img_width),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='binary',\n",
    "            color_mode='rgb',\n",
    "            shuffle=shuffle\n",
    "        )\n",
    "        return generator, augmented_generator\n",
    "    return generator, None\n",
    "\n",
    "def generators_to_dataset(generators, batch_size=32):\n",
    "    dataset = None\n",
    "    for generator in generators:\n",
    "        if generator is not None:\n",
    "            if dataset is None:\n",
    "                dataset = create_dataset(generator, batch_size)\n",
    "            else:\n",
    "                dataset = dataset.concatenate(create_dataset(generator, batch_size))\n",
    "    return dataset\n",
    "\n",
    "def plot_images(directory, category, num_images):\n",
    "    category_dir = os.path.join(directory, category)\n",
    "    images = os.listdir(category_dir)[:num_images]\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    dataset_name = os.path.basename(os.path.dirname(directory))\n",
    "    plt.suptitle(f\"Dataset: {dataset_name} - Category: {category}\", y=0.8)\n",
    "    for i, img_name in enumerate(images):\n",
    "        img_path = os.path.join(category_dir, img_name)\n",
    "        img = load_img(img_path, target_size=(img_height, img_width))\n",
    "        img_array = img_to_array(img) / 255.0\n",
    "        \n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(img_array)\n",
    "        plt.title(category)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_counts_from_generators(generator, augmented_generator=None):\n",
    "    print(\"Number of samples in generator:\", generator.samples)\n",
    "    print(\"Number of classes:\", generator.num_classes)\n",
    "    print(\"--------------------\")\n",
    "    class_indices = generator.class_indices\n",
    "    print(\"Class indices:\", class_indices)\n",
    "    class_names = list(class_indices.keys())\n",
    "    print(\"Class names:\", class_names)\n",
    "\n",
    "    original_class_counts = {class_name: 0 for class_name in class_names}\n",
    "    augmented_class_counts = {class_name: 0 for class_name in class_names}\n",
    "\n",
    "    for class_name, class_index in class_indices.items():\n",
    "        original_class_counts[class_name] = sum(generator.classes == class_index)\n",
    "        if augmented_generator is not None:\n",
    "            augmented_class_counts[class_name] = sum(augmented_generator.classes == class_index)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Dataset Class Counts:\")\n",
    "    for class_name, count in original_class_counts.items():\n",
    "        print(f\"{class_name}: {count}\")\n",
    "    if augmented_generator is not None:\n",
    "        print(\"\\nAugmented Dataset Class Counts:\")\n",
    "        for class_name, count in augmented_class_counts.items():\n",
    "            print(f\"{class_name}: {count}\")\n",
    "        print(\"\\n\")\n",
    "        print(\"Combined Dataset Class Counts:\")\n",
    "        for class_name, count in augmented_class_counts.items():\n",
    "            print(f\"{class_name}: {count + original_class_counts[class_name]}\")\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "def consolidate_to_train(datasets):\n",
    "    if len(datasets) == 1:\n",
    "        return datasets[0]\n",
    "    elif len(datasets) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        train_dataset = datasets[0]\n",
    "        for dataset in datasets[1:]:\n",
    "            train_dataset = train_dataset.concatenate(dataset)\n",
    "        return train_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Generation and Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_roc_curve(model, generator, directory, dataset_name):\n",
    "    # Generate predictions on the validation set\n",
    "    y_pred_probs = model.predict(generator)\n",
    "\n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(generator.classes, y_pred_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join(directory, f'roc_curve_{model.name}_{dataset_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Find the optimal threshold\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold\n",
    "    \n",
    "def plot_confusion_matrix(cm, directory, model_name, dataset_name, optimal=False):\n",
    "    class_names = ['fire', 'nofire']\n",
    "    # Save the confusion matrix plot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    fname = f'confusion_matrix_{model_name}_{dataset_name}_optimal.png' if optimal else f'confusion_matrix_{model_name}_{dataset_name}.png'\n",
    "    plt.savefig(os.path.join(directory, fname))\n",
    "    plt.close()\n",
    "\n",
    "def plot_pr_curve(model, generator, directory, dataset_name):\n",
    "    generator.reset()\n",
    "    # Generate predictions on the validation set\n",
    "    y_pred_probs = model.predict(generator)\n",
    "    precision, recall, thresholds = precision_recall_curve(generator.classes, y_pred_probs)\n",
    "\n",
    "    # Plot PR curve\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label='PR Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join(directory, f'pr_curve_{model.name}_{dataset_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Function to plot and save training history\n",
    "def plot_history(run_dir, history, model_name, dataset_name):\n",
    "    # Ensure all arrays in history.history have the same length\n",
    "    min_length = min(len(values) for values in history.history.values())\n",
    "    history_dict = {key: values[:min_length] for key, values in history.history.items()}\n",
    "    history_df = pd.DataFrame(history_dict)\n",
    "    history_df.to_csv(os.path.join(run_dir, f\"history_{model_name}_{dataset_name}.csv\"), index=False)\n",
    "    ## Accuracy and Loss Metrics\n",
    "    acc = history_df['accuracy']\n",
    "    val_acc = history_df['val_accuracy']\n",
    "    loss = history_df['loss']\n",
    "    val_loss = history_df['val_loss']\n",
    "    precision = history_df['precision']\n",
    "    val_precision = history_df['val_precision']\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epochs, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epochs, loss, label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(os.path.join(run_dir, f\"loss_accuracy_{model_name}_{dataset_name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    ## Recall, F1 Score, and Precision Metrics\n",
    "    recall = history.history['recall']\n",
    "    val_recall = history.history['val_recall']\n",
    "    f1 = history.history['f1_score']\n",
    "    val_f1 = history.history['val_f1_score']\n",
    "    precision = history.history['precision']\n",
    "    val_precision = history.history['val_precision']\n",
    "    epochs = range(len(recall))\n",
    "\n",
    "    plt.figure(figsize=(12, 18))\n",
    "\n",
    "    # Plot recall\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(epochs, recall, label='Training Recall')\n",
    "    plt.plot(epochs, val_recall, label='Validation Recall')\n",
    "    plt.title('Training and Validation Recall')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot F1 score\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(epochs, f1, label='Training F1 Score')\n",
    "    plt.plot(epochs, val_f1, label='Validation F1 Score')\n",
    "    plt.title('Training and Validation F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot precision\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(epochs, precision, label='Training Precision')\n",
    "    plt.plot(epochs, val_precision, label='Validation Precision')\n",
    "    plt.title('Training and Validation Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(os.path.join(run_dir, f\"recall_f1_precision_{model_name}_{dataset_name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_test_images(test_generator, directory, dataset_name, model, threshold, optimal=False):\n",
    "    test_generator.reset()\n",
    "    while True:\n",
    "        try:\n",
    "            test_images, test_labels = next(test_generator)\n",
    "            predictions = model.predict(test_images)\n",
    "\n",
    "            fire_indices = np.where(test_labels == 1)[0]\n",
    "            nofire_indices = np.where(test_labels == 0)[0]\n",
    "\n",
    "            random_fire_indices = np.random.choice(fire_indices, 5, replace=False)\n",
    "            random_nofire_indices = np.random.choice(nofire_indices, 5, replace=False)\n",
    "\n",
    "            random_indices = np.concatenate((random_fire_indices, random_nofire_indices))\n",
    "            np.random.shuffle(random_indices)\n",
    "\n",
    "            # Plot the images with predictions\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            for i, idx in enumerate(random_indices):\n",
    "                plt.subplot(2, 5, i+1)\n",
    "                plt.imshow(test_images[idx])\n",
    "                plt.title(f\"Actual: {'No Fire' if test_labels[idx] == 1 else 'Fire'}\\nPredicted: {'No Fire' if predictions[idx] >= threshold else 'Fire'}\")\n",
    "                plt.axis('off')\n",
    "            plt.savefig(os.path.join(directory, f\"test_images_{model.name}_{dataset_name}_optimal.png\" if optimal else f\"test_images_{model.name}_{dataset_name}.png\"))\n",
    "            plt.close()\n",
    "            break\n",
    "\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "### Evaluation stage ###\n",
    "def full_eval(model_ds_dir, history, model, dataset_name, test_generator):\n",
    "    # Plot metrics and save the history\n",
    "    plot_history(model_ds_dir, history, model.name, dataset_name)\n",
    "    # Plot the test images with predictions\n",
    "    plot_test_images(test_generator, model_ds_dir, dataset_name, model, 0.5)\n",
    "\n",
    "    true_labels = test_generator.classes\n",
    "    predicted_labels = (model.predict(test_generator) >= 0.5).astype(int).flatten()\n",
    "    # Generate the confusion matrix using the default threshold of 0.5\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    plot_confusion_matrix(cm, model_ds_dir, model.name, dataset_name)\n",
    "\n",
    "    # Generate the ROC curve\n",
    "    optimal_threshold = generate_roc_curve(model, test_generator, model_ds_dir, dataset_name)\n",
    "    \n",
    "    plot_test_images(test_generator, model_ds_dir, dataset_name, model, optimal_threshold, optimal=True)\n",
    "\n",
    "    true_labels = test_generator.classes\n",
    "    predicted_labels_optimal = (model.predict(test_generator) >= optimal_threshold).astype(int).flatten()\n",
    "    # Generate the confusion matrix using the optimal threshold from the ROC curve\n",
    "    cm_optimal = confusion_matrix(true_labels, predicted_labels_optimal)\n",
    "    plot_confusion_matrix(cm_optimal, model_ds_dir, model.name, dataset_name, optimal=True)\n",
    "\n",
    "    # Generate the PR curve\n",
    "    plot_pr_curve(model, test_generator, model_ds_dir, dataset_name)\n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: The Wildfire Dataset\n",
      "Augmenting The Wildfire Dataset\n",
      "Creating generators for training\n",
      "Found 1887 images belonging to 2 classes.\n",
      "Found 1887 images belonging to 2 classes.\n",
      "Creating generators for validation\n",
      "Found 402 images belonging to 2 classes.\n",
      "Found 402 images belonging to 2 classes.\n",
      "Number of samples in generator: 1887\n",
      "Number of classes: 2\n",
      "--------------------\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "Class names: ['fire', 'nofire']\n",
      "Dataset Class Counts:\n",
      "fire: 730\n",
      "nofire: 1157\n",
      "\n",
      "Augmented Dataset Class Counts:\n",
      "fire: 730\n",
      "nofire: 1157\n",
      "\n",
      "\n",
      "Combined Dataset Class Counts:\n",
      "fire: 1460\n",
      "nofire: 2314\n",
      "--------------------\n",
      "Processing: DeepFire\n",
      "Augmenting DeepFire\n",
      "Creating generators for training\n",
      "Found 1520 images belonging to 2 classes.\n",
      "Found 1520 images belonging to 2 classes.\n",
      "Splitted dataset:\n",
      "Training dataset size: 2432 samples\n",
      "Validation dataset size: 608 samples\n",
      "Number of samples in generator: 1520\n",
      "Number of classes: 2\n",
      "--------------------\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "Class names: ['fire', 'nofire']\n",
      "Dataset Class Counts:\n",
      "fire: 760\n",
      "nofire: 760\n",
      "\n",
      "Augmented Dataset Class Counts:\n",
      "fire: 760\n",
      "nofire: 760\n",
      "\n",
      "\n",
      "Combined Dataset Class Counts:\n",
      "fire: 1520\n",
      "nofire: 1520\n",
      "--------------------\n",
      "Processing: FIRE\n",
      "Augmenting FIRE\n",
      "Creating generators for training\n",
      "Found 999 images belonging to 2 classes.\n",
      "Found 999 images belonging to 2 classes.\n",
      "Splitted dataset:\n",
      "Training dataset size: 1599 samples\n",
      "Validation dataset size: 399 samples\n",
      "Number of samples in generator: 999\n",
      "Number of classes: 2\n",
      "--------------------\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "Class names: ['fire', 'nofire']\n",
      "Dataset Class Counts:\n",
      "fire: 755\n",
      "nofire: 244\n",
      "\n",
      "Augmented Dataset Class Counts:\n",
      "fire: 755\n",
      "nofire: 244\n",
      "\n",
      "\n",
      "Combined Dataset Class Counts:\n",
      "fire: 1510\n",
      "nofire: 488\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_names = []\n",
    "train_datasets = [] # [ (dataset_1_train, dataset_2_train), ... ]\n",
    "train_sizes = [] # [ (dataset_1_train_size, dataset_2_train_size), ... ]\n",
    "val_datasets = [] # [ (dataset_1_val, dataset_2_val), ... ]\n",
    "val_sizes = [] # [ (dataset_1_val_size, dataset_2_val_size), ... ]\n",
    "\n",
    "\n",
    "for d in datasets:\n",
    "    print(f\"Processing: {d}\")\n",
    "    train_dir = datasets[d]['train']\n",
    "    augment = datasets[d].get('augment', True)\n",
    "    print(\"Augmenting\" if augment else \"Not augmenting\", d)\n",
    "    # Apply original and augmented data generators for training\n",
    "    print(\"Creating generators for training\")\n",
    "    train_generator, augmented_train_generator = create_generators(train_dir, augment=augment)\n",
    "    train_samples = samples_from_generators([train_generator, augmented_train_generator])  \n",
    "    train_dataset = generators_to_dataset([train_generator, augmented_train_generator], batch_size=batch_size)\n",
    "    \n",
    "    # Apply original and augmented data generators for validation\n",
    "    if \"val\" in datasets[d]:\n",
    "        val_dir = datasets[d]['val']\n",
    "        print(\"Creating generators for validation\")\n",
    "        val_generator, augmented_val_generator = create_generators(val_dir, augment=augment, shuffle=False)\n",
    "        val_samples = samples_from_generators([val_generator, augmented_val_generator])\n",
    "        val_dataset = generators_to_dataset([train_generator, augmented_train_generator], batch_size=batch_size)\n",
    "    else:\n",
    "        train_dataset, val_dataset, train_samples, val_samples = val_split(train_dataset, train_samples)\n",
    "        val_generator, augmented_val_generator = None, None\n",
    "    \n",
    "    # Calculate the number of samples for training and validation\n",
    "    train_sizes.append(train_samples)\n",
    "    val_sizes.append(val_samples)\n",
    "\n",
    "    show_counts_from_generators(train_generator, augmented_train_generator)\n",
    "\n",
    "    train_datasets.append(train_dataset)\n",
    "    val_datasets.append(val_dataset)\n",
    "    dataset_names.append(d)\n",
    "    \n",
    "# Ensure that the lengths are consistent across the board\n",
    "assert(len(train_sizes) == len(train_datasets) and len(train_sizes) == len(val_sizes) and len(val_sizes) == len(val_datasets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute Force Combinatorial Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combos [(0,), (1,), (2,), (0, 1), (0, 2), (1, 2), (0, 1, 2)]\n"
     ]
    }
   ],
   "source": [
    "dataset_combos = [] # [(0,), (1,), (0, 1), ...] where 0, 1 are the indices of the datasets within their respective lists\n",
    "for r in range(1, len(dataset_names) + 1):\n",
    "    dataset_combos.extend(combinations(range(len(dataset_names)), r))\n",
    "print(\"Combos\", dataset_combos)\n",
    "combined_training_datasets = []\n",
    "combined_val_datasets = []\n",
    "combined_dataset_names = []\n",
    "steps_per_epoch_list = []\n",
    "validation_steps_list = []\n",
    "\n",
    "for combo in dataset_combos:\n",
    "    training_dataset = None\n",
    "    val_dataset = None\n",
    "    train_size = None\n",
    "    val_size = None\n",
    "    for idx in combo:\n",
    "        if training_dataset is None:\n",
    "            training_dataset = train_datasets[idx]\n",
    "            val_dataset = val_datasets[idx]\n",
    "            train_size = train_sizes[idx]\n",
    "            val_size = val_sizes[idx]\n",
    "        else:\n",
    "            training_dataset = training_dataset.concatenate(train_datasets[idx])\n",
    "            val_dataset = val_dataset.concatenate(val_datasets[idx])\n",
    "            train_size += train_sizes[idx]\n",
    "            val_size += val_sizes[idx]\n",
    "    combined_dataset_names.append(\"_\".join([dataset_names[idx] for idx in combo]))\n",
    "    combined_training_datasets.append(training_dataset)\n",
    "    combined_val_datasets.append(val_dataset)\n",
    "    steps_per_epoch_list.append(train_size // batch_size)\n",
    "    validation_steps_list.append(val_size // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute Force Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidation Combos [(0,), (1,), (2,), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (0, 1, 2), (0, 1, 3), (0, 1, 4), (0, 1, 5), (0, 2, 3), (0, 2, 4), (0, 2, 5), (0, 3, 4), (0, 3, 5), (0, 4, 5), (1, 2, 3), (1, 2, 4), (1, 2, 5), (1, 3, 4), (1, 3, 5), (1, 4, 5), (2, 3, 4), (2, 3, 5), (2, 4, 5), (0, 1, 2, 3), (0, 1, 2, 4), (0, 1, 2, 5), (0, 1, 3, 4), (0, 1, 3, 5), (0, 1, 4, 5), (0, 2, 3, 4), (0, 2, 3, 5), (0, 2, 4, 5), (0, 3, 4, 5), (1, 2, 3, 4), (1, 2, 3, 5), (1, 2, 4, 5), (1, 3, 4, 5), (2, 3, 4, 5), (0, 1, 2, 3, 4), (0, 1, 2, 3, 5), (0, 1, 2, 4, 5), (0, 1, 3, 4, 5), (0, 2, 3, 4, 5), (1, 2, 3, 4, 5), (0, 1, 2, 3, 4, 5)]\n",
      "Mapping Dict {0: {'train': ['The Wildfire Dataset'], 'val': []}, 1: {'train': ['DeepFire'], 'val': []}, 2: {'train': ['FIRE'], 'val': []}, 3: {'train': ['The Wildfire Dataset', 'DeepFire'], 'val': []}, 4: {'train': ['The Wildfire Dataset', 'FIRE'], 'val': []}, 5: {'train': ['The Wildfire Dataset'], 'val': ['The Wildfire Dataset']}, 6: {'train': ['The Wildfire Dataset'], 'val': ['DeepFire']}, 7: {'train': ['The Wildfire Dataset'], 'val': ['FIRE']}, 8: {'train': ['DeepFire', 'FIRE'], 'val': []}, 9: {'train': ['DeepFire'], 'val': ['The Wildfire Dataset']}, 10: {'train': ['DeepFire'], 'val': ['DeepFire']}, 11: {'train': ['DeepFire'], 'val': ['FIRE']}, 12: {'train': ['FIRE'], 'val': ['The Wildfire Dataset']}, 13: {'train': ['FIRE'], 'val': ['DeepFire']}, 14: {'train': ['FIRE'], 'val': ['FIRE']}, 15: {'train': ['The Wildfire Dataset', 'DeepFire', 'FIRE'], 'val': []}, 16: {'train': ['The Wildfire Dataset', 'DeepFire'], 'val': ['The Wildfire Dataset']}, 17: {'train': ['The Wildfire Dataset', 'DeepFire'], 'val': ['DeepFire']}, 18: {'train': ['The Wildfire Dataset', 'DeepFire'], 'val': ['FIRE']}, 19: {'train': ['The Wildfire Dataset', 'FIRE'], 'val': ['The Wildfire Dataset']}, 20: {'train': ['The Wildfire Dataset', 'FIRE'], 'val': ['DeepFire']}, 21: {'train': ['The Wildfire Dataset', 'FIRE'], 'val': ['FIRE']}, 22: {'train': ['The Wildfire Dataset'], 'val': ['The Wildfire Dataset', 'DeepFire']}, 23: {'train': ['The Wildfire Dataset'], 'val': ['The Wildfire Dataset', 'FIRE']}, 24: {'train': ['The Wildfire Dataset'], 'val': ['DeepFire', 'FIRE']}, 25: {'train': ['DeepFire', 'FIRE'], 'val': ['The Wildfire Dataset']}, 26: {'train': ['DeepFire', 'FIRE'], 'val': ['DeepFire']}, 27: {'train': ['DeepFire', 'FIRE'], 'val': ['FIRE']}, 28: {'train': ['DeepFire'], 'val': ['The Wildfire Dataset', 'DeepFire']}, 29: {'train': ['DeepFire'], 'val': ['The Wildfire Dataset', 'FIRE']}, 30: {'train': ['DeepFire'], 'val': ['DeepFire', 'FIRE']}, 31: {'train': ['FIRE'], 'val': ['The Wildfire Dataset', 'DeepFire']}, 32: {'train': ['FIRE'], 'val': ['The Wildfire Dataset', 'FIRE']}, 33: {'train': ['FIRE'], 'val': ['DeepFire', 'FIRE']}, 34: {'train': ['The Wildfire Dataset', 'DeepFire', 'FIRE'], 'val': ['The Wildfire Dataset']}, 35: {'train': ['The Wildfire Dataset', 'DeepFire', 'FIRE'], 'val': ['DeepFire']}, 36: {'train': ['The Wildfire Dataset', 'DeepFire', 'FIRE'], 'val': ['FIRE']}, 37: {'train': ['The Wildfire Dataset', 'DeepFire'], 'val': ['The Wildfire Dataset', 'DeepFire']}, 38: {'train': ['The Wildfire Dataset', 'DeepFire'], 'val': ['The Wildfire Dataset', 'FIRE']}, 39: {'train': ['The Wildfire Dataset', 'DeepFire'], 'val': ['DeepFire', 'FIRE']}, 40: {'train': ['The Wildfire Dataset', 'FIRE'], 'val': ['The Wildfire Dataset', 'DeepFire']}, 41: {'train': ['The Wildfire Dataset', 'FIRE'], 'val': ['The Wildfire Dataset', 'FIRE']}, 42: {'train': ['The Wildfire Dataset', 'FIRE'], 'val': ['DeepFire', 'FIRE']}, 43: {'train': ['The Wildfire Dataset'], 'val': ['The Wildfire Dataset', 'DeepFire', 'FIRE']}, 44: {'train': ['DeepFire', 'FIRE'], 'val': ['The Wildfire Dataset', 'DeepFire']}, 45: {'train': ['DeepFire', 'FIRE'], 'val': ['The Wildfire Dataset', 'FIRE']}, 46: {'train': ['DeepFire', 'FIRE'], 'val': ['DeepFire', 'FIRE']}, 47: {'train': ['DeepFire'], 'val': ['The Wildfire Dataset', 'DeepFire', 'FIRE']}, 48: {'train': ['FIRE'], 'val': ['The Wildfire Dataset', 'DeepFire', 'FIRE']}, 49: {'train': ['The Wildfire Dataset', 'DeepFire', 'FIRE'], 'val': ['The Wildfire Dataset', 'DeepFire']}, 50: {'train': ['The Wildfire Dataset', 'DeepFire', 'FIRE'], 'val': ['The Wildfire Dataset', 'FIRE']}, 51: {'train': ['The Wildfire Dataset', 'DeepFire', 'FIRE'], 'val': ['DeepFire', 'FIRE']}, 52: {'train': ['The Wildfire Dataset', 'DeepFire'], 'val': ['The Wildfire Dataset', 'DeepFire', 'FIRE']}, 53: {'train': ['The Wildfire Dataset', 'FIRE'], 'val': ['The Wildfire Dataset', 'DeepFire', 'FIRE']}, 54: {'train': ['DeepFire', 'FIRE'], 'val': ['The Wildfire Dataset', 'DeepFire', 'FIRE']}, 'all': {'train': 'all', 'val': 'all'}}\n"
     ]
    }
   ],
   "source": [
    "consolidation_combos = []\n",
    "consolidated_datasets = train_datasets + val_datasets\n",
    "consolidated_sizes = train_sizes + val_sizes\n",
    "# Generate combinations ensuring at least one training dataset\n",
    "for n in range(1, len(consolidated_datasets) + 1):\n",
    "    for combo in combinations(range(len(consolidated_datasets)), n):\n",
    "        if any(idx < len(train_datasets) for idx in combo): # Ensure at least one element from train_datasets\n",
    "            consolidation_combos.append(combo)\n",
    "\n",
    "print(\"Consolidation Combos\", consolidation_combos)\n",
    "\n",
    "# Generate consolidated datasets\n",
    "consolidated_training_datasets = []\n",
    "consolidated_steps_per_epoch_list = []\n",
    "consolidated_ids = []\n",
    "mapping_dict = {}\n",
    "\n",
    "for id, combo in enumerate(consolidation_combos):\n",
    "    train_dataset_names = []\n",
    "    val_dataset_names = []\n",
    "    consolidated_training_datasets.append(consolidate_to_train([consolidated_datasets[idx] for idx in combo]))\n",
    "    consolidated_steps_per_epoch_list.append(sum([consolidated_sizes[idx] for idx in combo]) // batch_size)\n",
    "    if len(combo) == len(consolidated_datasets):\n",
    "        mapping_dict[\"all\"] = {\"train\": \"all\", \"val\": \"all\"}\n",
    "    else:\n",
    "        for idx in combo:\n",
    "            normalized_idx = idx % len(dataset_names) # Normalize the index to the dataset_names list to avoid out of bound access\n",
    "            if idx < len(dataset_names):\n",
    "                train_dataset_names.append(dataset_names[normalized_idx])\n",
    "            else:\n",
    "                val_dataset_names.append(dataset_names[normalized_idx])\n",
    "        mapping_dict[id] = {\"train\": train_dataset_names, \"val\": val_dataset_names}\n",
    "    consolidated_ids.append(id) # Keeping this to guarantee the order for training loops\n",
    "# print(\"Consolidated Training Datasets\", consolidated_training_datasets)\n",
    "# print(\"Consolidated Steps Per Epoch\", consolidated_steps_per_epoch_list)\n",
    "\n",
    "print(\"Mapping Dict\", mapping_dict)\n",
    "\n",
    "# Ensure that the lengths are consistent across the board\n",
    "assert(len(consolidated_training_datasets) == len(consolidated_steps_per_epoch_list) and len(consolidated_steps_per_epoch_list) == len(consolidated_ids) and len(mapping_dict.keys()) == len(consolidated_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 858 images belonging to 2 classes.\n",
      "Class indices: {'fire': 0, 'nofire': 1}\n",
      "\n",
      "\n",
      "Test Dataset Class Counts:\n",
      "fire: 371\n",
      "nofire: 487\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dir = \"test_combined\"\n",
    "test_generator, augmented_test_generator = create_generators(test_dir, augment=False, shuffle=False) # No augmentation for testing\n",
    "test_dataset = create_dataset(test_generator)\n",
    "\n",
    "print(\"Class indices:\", test_generator.class_indices)\n",
    "print(\"\\n\")\n",
    "print(\"Test Dataset Class Counts:\")\n",
    "for class_name, class_index in test_generator.class_indices.items():\n",
    "    print(f\"{class_name}: {sum(test_generator.classes == class_index)}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_model(bm, custom=False):\n",
    "    if custom:\n",
    "        model = bm\n",
    "        model.compile(optimizer=optimizer_fn, loss=loss_fn, metrics=metrics_list)\n",
    "        model.save_weights(os.path.join(checkpoint_path, f\"{model.name}_initial.weights.h5\"))    \n",
    "        return model\n",
    "    \n",
    "    base_model = bm(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(img_height, img_width, 3)\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create the model\n",
    "    inputs = Input(shape=(img_height, img_width, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name=bm.__name__)\n",
    "    model.compile(optimizer=optimizer_fn, loss=loss_fn, metrics=metrics_list)\n",
    "    model.save_weights(os.path.join(checkpoint_path, f\"{model.name}_initial.weights.h5\"))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the models and combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_results = {}\n",
    "run_number = len([d for d in os.listdir(\"runs\") if os.path.isdir(os.path.join(\"runs\", d)) and d.startswith('run_')]) + 1\n",
    "run_dir = os.path.join(\"runs\", f\"run_{run_number}\")\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(run_dir, 'brute_force_mapping_dict.json'), 'w') as f:\n",
    "    json.dump(mapping_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ResNet50V2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"ResNet50V2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet50v2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,564,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet50v2 (\u001b[38;5;33mFunctional\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,564,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m524,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,098,817</span> (91.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,098,817\u001b[0m (91.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">529,409</span> (2.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m529,409\u001b[0m (2.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,569,408</span> (89.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,569,408\u001b[0m (89.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: ResNet50V2 on dataset: The Wildfire Dataset\n",
      "Epoch 1/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 616ms/step - accuracy: 0.7220 - auc: 0.8094 - f1_score: 0.7533 - loss: 0.6589 - precision: 0.8079 - recall: 0.7146 - val_accuracy: 0.9212 - val_auc: 0.9749 - val_f1_score: 0.9335 - val_loss: 0.2360 - val_precision: 0.9308 - val_recall: 0.9403 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 683ms/step - accuracy: 0.8673 - auc: 0.9331 - f1_score: 0.8882 - loss: 0.3300 - precision: 0.8913 - recall: 0.8901 - val_accuracy: 0.9525 - val_auc: 0.9873 - val_f1_score: 0.9611 - val_loss: 0.1782 - val_precision: 0.9639 - val_recall: 0.9600 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 696ms/step - accuracy: 0.8761 - auc: 0.9400 - f1_score: 0.8977 - loss: 0.3148 - precision: 0.8883 - recall: 0.9108 - val_accuracy: 0.9688 - val_auc: 0.9953 - val_f1_score: 0.9740 - val_loss: 0.1341 - val_precision: 0.9794 - val_recall: 0.9695 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 709ms/step - accuracy: 0.9075 - auc: 0.9682 - f1_score: 0.9235 - loss: 0.2236 - precision: 0.9221 - recall: 0.9274 - val_accuracy: 0.9725 - val_auc: 0.9961 - val_f1_score: 0.9771 - val_loss: 0.1129 - val_precision: 0.9818 - val_recall: 0.9739 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 695ms/step - accuracy: 0.9124 - auc: 0.9713 - f1_score: 0.9263 - loss: 0.2115 - precision: 0.9212 - recall: 0.9348 - val_accuracy: 0.9762 - val_auc: 0.9977 - val_f1_score: 0.9809 - val_loss: 0.0956 - val_precision: 0.9827 - val_recall: 0.9808 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 703ms/step - accuracy: 0.9274 - auc: 0.9820 - f1_score: 0.9399 - loss: 0.1708 - precision: 0.9367 - recall: 0.9437 - val_accuracy: 0.9912 - val_auc: 0.9989 - val_f1_score: 0.9929 - val_loss: 0.0733 - val_precision: 0.9898 - val_recall: 0.9959 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 700ms/step - accuracy: 0.9183 - auc: 0.9788 - f1_score: 0.9320 - loss: 0.1826 - precision: 0.9229 - recall: 0.9440 - val_accuracy: 0.9812 - val_auc: 0.9987 - val_f1_score: 0.9844 - val_loss: 0.0751 - val_precision: 0.9817 - val_recall: 0.9877 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 704ms/step - accuracy: 0.9352 - auc: 0.9856 - f1_score: 0.9469 - loss: 0.1515 - precision: 0.9475 - recall: 0.9490 - val_accuracy: 0.9937 - val_auc: 0.9999 - val_f1_score: 0.9948 - val_loss: 0.0566 - val_precision: 0.9979 - val_recall: 0.9917 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 709ms/step - accuracy: 0.9324 - auc: 0.9847 - f1_score: 0.9440 - loss: 0.1562 - precision: 0.9516 - recall: 0.9389 - val_accuracy: 0.9937 - val_auc: 0.9999 - val_f1_score: 0.9947 - val_loss: 0.0492 - val_precision: 0.9939 - val_recall: 0.9960 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 713ms/step - accuracy: 0.9390 - auc: 0.9855 - f1_score: 0.9488 - loss: 0.1496 - precision: 0.9467 - recall: 0.9541 - val_accuracy: 0.9962 - val_auc: 1.0000 - val_f1_score: 0.9970 - val_loss: 0.0450 - val_precision: 0.9959 - val_recall: 0.9979 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 712ms/step - accuracy: 0.9488 - auc: 0.9903 - f1_score: 0.9580 - loss: 0.1238 - precision: 0.9525 - recall: 0.9652 - val_accuracy: 0.9962 - val_auc: 1.0000 - val_f1_score: 0.9968 - val_loss: 0.0449 - val_precision: 1.0000 - val_recall: 0.9939 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 731ms/step - accuracy: 0.9487 - auc: 0.9896 - f1_score: 0.9563 - loss: 0.1260 - precision: 0.9606 - recall: 0.9556 - val_accuracy: 0.9975 - val_auc: 1.0000 - val_f1_score: 0.9979 - val_loss: 0.0346 - val_precision: 1.0000 - val_recall: 0.9958 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 705ms/step - accuracy: 0.9605 - auc: 0.9918 - f1_score: 0.9674 - loss: 0.1115 - precision: 0.9704 - recall: 0.9656 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0339 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 724ms/step - accuracy: 0.9496 - auc: 0.9894 - f1_score: 0.9568 - loss: 0.1245 - precision: 0.9592 - recall: 0.9601 - val_accuracy: 0.9987 - val_auc: 1.0000 - val_f1_score: 0.9988 - val_loss: 0.0321 - val_precision: 1.0000 - val_recall: 0.9980 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 738ms/step - accuracy: 0.9649 - auc: 0.9937 - f1_score: 0.9703 - loss: 0.1009 - precision: 0.9698 - recall: 0.9714 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0238 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 712ms/step - accuracy: 0.9564 - auc: 0.9933 - f1_score: 0.9637 - loss: 0.1054 - precision: 0.9623 - recall: 0.9661 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0226 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 730ms/step - accuracy: 0.9562 - auc: 0.9912 - f1_score: 0.9643 - loss: 0.1138 - precision: 0.9601 - recall: 0.9688 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0249 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 709ms/step - accuracy: 0.9665 - auc: 0.9939 - f1_score: 0.9721 - loss: 0.0932 - precision: 0.9710 - recall: 0.9749 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0230 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 708ms/step - accuracy: 0.9760 - auc: 0.9969 - f1_score: 0.9802 - loss: 0.0710 - precision: 0.9818 - recall: 0.9793 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0179 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 20/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 741ms/step - accuracy: 0.9710 - auc: 0.9964 - f1_score: 0.9759 - loss: 0.0759 - precision: 0.9758 - recall: 0.9771 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0171 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 21/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 693ms/step - accuracy: 0.9724 - auc: 0.9950 - f1_score: 0.9768 - loss: 0.0839 - precision: 0.9732 - recall: 0.9821 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0140 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 22/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 681ms/step - accuracy: 0.9612 - auc: 0.9922 - f1_score: 0.9680 - loss: 0.0996 - precision: 0.9710 - recall: 0.9668 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0188 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 23/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 759ms/step - accuracy: 0.9670 - auc: 0.9953 - f1_score: 0.9717 - loss: 0.0821 - precision: 0.9700 - recall: 0.9772 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0154 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 24/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 680ms/step - accuracy: 0.9721 - auc: 0.9973 - f1_score: 0.9764 - loss: 0.0664 - precision: 0.9763 - recall: 0.9776 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0151 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 25/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 757ms/step - accuracy: 0.9724 - auc: 0.9945 - f1_score: 0.9775 - loss: 0.0801 - precision: 0.9780 - recall: 0.9784 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0163 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 26/80\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568ms/step - accuracy: 0.9779 - auc: 0.9980 - f1_score: 0.9818 - loss: 0.0587 - precision: 0.9847 - recall: 0.9793\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 691ms/step - accuracy: 0.9778 - auc: 0.9980 - f1_score: 0.9817 - loss: 0.0588 - precision: 0.9846 - recall: 0.9793 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0155 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Training time: 2155.33 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 591ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 600ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 578ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 587ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 592ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 593ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 583ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 569ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 613ms/step\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 569ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 586ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 583ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 589ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 575ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 587ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 578ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 585ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 594ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 593ms/step\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 572ms/step\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 614ms/step\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 637ms/step - accuracy: 0.8174 - auc: 0.5305 - f1_score: 0.2141 - loss: 0.5430 - precision: 0.3933 - recall: 0.5031\n",
      "{'ResNet50V2': {'The Wildfire Dataset': {'history': {'accuracy': [0.7895299196243286, 0.875, 0.8886218070983887, 0.9038461446762085, 0.9139957427978516, 0.9278846383094788, 0.9204059839248657, 0.9340277910232544, 0.938034176826477, 0.9383012652397156, 0.944978654384613, 0.9511218070983887, 0.9561966061592102, 0.9545940160751343, 0.961271345615387, 0.9620726704597473, 0.9583333134651184, 0.9626068472862244, 0.9748931527137756, 0.969284176826477, 0.9703525900840759, 0.9636752009391785, 0.9700854420661926, 0.973557710647583, 0.9732906222343445, 0.9748931527137756], 'auc': [0.8738661408424377, 0.9386177062988281, 0.9506331086158752, 0.966698169708252, 0.9721618294715881, 0.9797037243843079, 0.9785062074661255, 0.9831766486167908, 0.9835408926010132, 0.985834002494812, 0.9879105687141418, 0.9904929995536804, 0.9907745122909546, 0.9907065033912659, 0.99322509765625, 0.9941664934158325, 0.9921005368232727, 0.9943030476570129, 0.9966294765472412, 0.9953827857971191, 0.995461642742157, 0.9925968647003174, 0.9964103102684021, 0.9971719980239868, 0.9960605502128601, 0.9973233342170715], 'f1_score': [0.8199314475059509, 0.8951394557952881, 0.9078100919723511, 0.9204161763191223, 0.9277603030204773, 0.9400395154953003, 0.9346519112586975, 0.9449142217636108, 0.9487345218658447, 0.9486895799636841, 0.95441073179245, 0.958916425704956, 0.9630606174468994, 0.9617495536804199, 0.9674412608146667, 0.9686192274093628, 0.9658327698707581, 0.968870997428894, 0.9791178703308105, 0.9743279218673706, 0.9752426147460938, 0.9699217081069946, 0.9753080606460571, 0.9777047634124756, 0.9778468012809753, 0.979087233543396], 'loss': [0.5159294009208679, 0.31420576572418213, 0.279878705739975, 0.22751042246818542, 0.20892195403575897, 0.1788019835948944, 0.18277248740196228, 0.16136814653873444, 0.1584189385175705, 0.1480671912431717, 0.13589589297771454, 0.12155218422412872, 0.1188606470823288, 0.11742284893989563, 0.10359599441289902, 0.09650600701570511, 0.10765231400728226, 0.09423882514238358, 0.07326323539018631, 0.08224590122699738, 0.08120608329772949, 0.09616512060165405, 0.07366833835840225, 0.06800791621208191, 0.07334666699171066, 0.06560901552438736], 'precision': [0.8484988212585449, 0.8989059329032898, 0.9028375148773193, 0.9208695888519287, 0.9278804063796997, 0.9402268528938293, 0.9270166158676147, 0.9421915411949158, 0.9482535719871521, 0.9482158422470093, 0.9507984519004822, 0.9594478011131287, 0.9645203948020935, 0.9616544842720032, 0.9679543375968933, 0.9696048498153687, 0.9647672772407532, 0.9694856405258179, 0.9798068404197693, 0.97641921043396, 0.9739921689033508, 0.9705755114555359, 0.9754416346549988, 0.9785745739936829, 0.9779506921768188, 0.9809027910232544], 'recall': [0.7997387647628784, 0.8965517282485962, 0.9166302680969238, 0.9224738478660583, 0.9303351044654846, 0.9418706297874451, 0.9447585940361023, 0.9508054256439209, 0.9515361189842224, 0.9511130452156067, 0.9599128365516663, 0.9615218043327332, 0.9636761546134949, 0.9649805426597595, 0.9683794379234314, 0.968763530254364, 0.9672917723655701, 0.9694856405258179, 0.9789473414421082, 0.9734436273574829, 0.977806806564331, 0.9705755114555359, 0.9762828946113586, 0.9781468510627747, 0.9787970781326294, 0.9783549904823303], 'val_accuracy': [0.9212499856948853, 0.9524999856948853, 0.96875, 0.9725000262260437, 0.9762499928474426, 0.9912499785423279, 0.981249988079071, 0.9937499761581421, 0.9937499761581421, 0.9962499737739563, 0.9962499737739563, 0.9975000023841858, 1.0, 0.9987499713897705, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_auc': [0.9748662710189819, 0.9872766733169556, 0.9952906370162964, 0.9960936307907104, 0.9977060556411743, 0.9989347457885742, 0.9987324476242065, 0.999882698059082, 0.9998776316642761, 0.9999804496765137, 0.9999999403953552, 1.0, 0.9999999403953552, 1.0, 1.0, 0.9999999403953552, 1.0, 0.9999999403953552, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 1.0, 1.0], 'val_f1_score': [0.9334596395492554, 0.9610698819160461, 0.9740060567855835, 0.9770694971084595, 0.9808782339096069, 0.9929158091545105, 0.9843572378158569, 0.9948123693466187, 0.9947448968887329, 0.9969630241394043, 0.9967503547668457, 0.9979432821273804, 1.0, 0.9987878203392029, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.2360447645187378, 0.17821848392486572, 0.1340770125389099, 0.11287274211645126, 0.0955757424235344, 0.07333821058273315, 0.07511618733406067, 0.05658957362174988, 0.049235567450523376, 0.045009396970272064, 0.04487491771578789, 0.034565772861242294, 0.033891625702381134, 0.03210007771849632, 0.023780815303325653, 0.022603284567594528, 0.02487422525882721, 0.02296583168208599, 0.017855646088719368, 0.017067743465304375, 0.014004659838974476, 0.018849119544029236, 0.01535777561366558, 0.015106117352843285, 0.016337677836418152, 0.015475517138838768], 'val_precision': [0.9307535886764526, 0.9638554453849792, 0.9794238805770874, 0.9817813634872437, 0.9826589822769165, 0.9898374080657959, 0.981670081615448, 0.9979079365730286, 0.9939393997192383, 0.9958677887916565, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_recall': [0.9403291940689087, 0.9599999785423279, 0.9694501161575317, 0.9738956093788147, 0.9807692170143127, 0.9959100484848022, 0.9877049326896667, 0.9916840195655823, 0.9959514141082764, 0.9979296326637268, 0.9939271211624146, 0.9958246350288391, 1.0, 0.9980040192604065, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'learning_rate': [0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513]}, 'training_time': 2155.3256669044495, 'optimal_threshold': 0.7395251989364624, 'train_dataset_size': 3744, 'val_dataset_size': 800, 'evaluation': {'accuracy': 0.8377403616905212, 'auc': 0.9083821177482605, 'f1_score': 0.5308767557144165, 'loss': 0.45489978790283203, 'precision': 0.8424369692802429, 'recall': 0.8698481321334839}}}}\n",
      "Training model: ResNet50V2 on dataset: DeepFire\n",
      "Epoch 1/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 714ms/step - accuracy: 0.8752 - auc: 0.9448 - f1_score: 0.8794 - loss: 0.2985 - precision: 0.8773 - recall: 0.8887 - val_accuracy: 0.9868 - val_auc: 0.9991 - val_f1_score: 0.9867 - val_loss: 0.0496 - val_precision: 1.0000 - val_recall: 0.9732 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 716ms/step - accuracy: 0.9790 - auc: 0.9984 - f1_score: 0.9786 - loss: 0.0498 - precision: 0.9862 - recall: 0.9724 - val_accuracy: 0.9934 - val_auc: 0.9998 - val_f1_score: 0.9935 - val_loss: 0.0262 - val_precision: 1.0000 - val_recall: 0.9869 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 723ms/step - accuracy: 0.9812 - auc: 0.9988 - f1_score: 0.9819 - loss: 0.0469 - precision: 0.9846 - recall: 0.9779 - val_accuracy: 0.9967 - val_auc: 0.9999 - val_f1_score: 0.9957 - val_loss: 0.0162 - val_precision: 1.0000 - val_recall: 0.9932 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 730ms/step - accuracy: 0.9926 - auc: 0.9996 - f1_score: 0.9922 - loss: 0.0241 - precision: 0.9948 - recall: 0.9904 - val_accuracy: 0.9951 - val_auc: 0.9998 - val_f1_score: 0.9953 - val_loss: 0.0203 - val_precision: 1.0000 - val_recall: 0.9907 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 774ms/step - accuracy: 0.9920 - auc: 0.9993 - f1_score: 0.9918 - loss: 0.0256 - precision: 0.9919 - recall: 0.9920 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0043 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 791ms/step - accuracy: 0.9874 - auc: 0.9998 - f1_score: 0.9879 - loss: 0.0300 - precision: 0.9970 - recall: 0.9787 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0027 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 727ms/step - accuracy: 0.9865 - auc: 0.9994 - f1_score: 0.9856 - loss: 0.0303 - precision: 0.9835 - recall: 0.9899 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0020 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 733ms/step - accuracy: 0.9919 - auc: 0.9995 - f1_score: 0.9919 - loss: 0.0246 - precision: 0.9946 - recall: 0.9891 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0017 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 761ms/step - accuracy: 0.9938 - auc: 0.9998 - f1_score: 0.9937 - loss: 0.0179 - precision: 0.9928 - recall: 0.9946 - val_accuracy: 0.9984 - val_auc: 1.0000 - val_f1_score: 0.9982 - val_loss: 0.0027 - val_precision: 1.0000 - val_recall: 0.9967 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 807ms/step - accuracy: 0.9913 - auc: 0.9998 - f1_score: 0.9922 - loss: 0.0225 - precision: 0.9968 - recall: 0.9869 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 5.6833e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 713ms/step - accuracy: 0.9955 - auc: 0.9998 - f1_score: 0.9951 - loss: 0.0139 - precision: 0.9946 - recall: 0.9964 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 8.4142e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 740ms/step - accuracy: 0.9912 - auc: 0.9996 - f1_score: 0.9893 - loss: 0.0242 - precision: 0.9864 - recall: 0.9952 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 5.4602e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 753ms/step - accuracy: 0.9942 - auc: 0.9994 - f1_score: 0.9936 - loss: 0.0159 - precision: 0.9919 - recall: 0.9962 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 3.4507e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 811ms/step - accuracy: 0.9937 - auc: 0.9999 - f1_score: 0.9928 - loss: 0.0132 - precision: 0.9922 - recall: 0.9954 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 5.1073e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 736ms/step - accuracy: 0.9920 - auc: 0.9997 - f1_score: 0.9907 - loss: 0.0234 - precision: 0.9877 - recall: 0.9960 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 5.5895e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 724ms/step - accuracy: 0.9967 - auc: 0.9997 - f1_score: 0.9960 - loss: 0.0085 - precision: 0.9966 - recall: 0.9968 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 3.7442e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 736ms/step - accuracy: 0.9909 - auc: 0.9993 - f1_score: 0.9892 - loss: 0.0219 - precision: 0.9893 - recall: 0.9925 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 3.4414e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641ms/step - accuracy: 0.9912 - auc: 0.9997 - f1_score: 0.9898 - loss: 0.0227 - precision: 0.9901 - recall: 0.9919\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 810ms/step - accuracy: 0.9913 - auc: 0.9997 - f1_score: 0.9898 - loss: 0.0226 - precision: 0.9902 - recall: 0.9919 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 4.2836e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 758ms/step - accuracy: 0.9936 - auc: 0.9999 - f1_score: 0.9945 - loss: 0.0171 - precision: 0.9983 - recall: 0.9892 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 6.2146e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 20/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 732ms/step - accuracy: 0.9948 - auc: 0.9998 - f1_score: 0.9951 - loss: 0.0187 - precision: 0.9965 - recall: 0.9937 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 4.5551e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 21/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 748ms/step - accuracy: 0.9959 - auc: 1.0000 - f1_score: 0.9956 - loss: 0.0115 - precision: 0.9974 - recall: 0.9945 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 2.7426e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 22/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 779ms/step - accuracy: 0.9922 - auc: 0.9998 - f1_score: 0.9922 - loss: 0.0143 - precision: 0.9917 - recall: 0.9925 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 2.8771e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 23/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 779ms/step - accuracy: 0.9980 - auc: 0.9996 - f1_score: 0.9978 - loss: 0.0085 - precision: 0.9976 - recall: 0.9982 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 2.1019e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 24/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 733ms/step - accuracy: 0.9952 - auc: 0.9998 - f1_score: 0.9953 - loss: 0.0111 - precision: 0.9981 - recall: 0.9922 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 1.8363e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 25/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 731ms/step - accuracy: 0.9953 - auc: 0.9999 - f1_score: 0.9960 - loss: 0.0105 - precision: 0.9988 - recall: 0.9920 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 2.8133e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 26/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 816ms/step - accuracy: 0.9956 - auc: 0.9999 - f1_score: 0.9961 - loss: 0.0116 - precision: 0.9987 - recall: 0.9925 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 1.6162e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 27/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 759ms/step - accuracy: 0.9990 - auc: 1.0000 - f1_score: 0.9991 - loss: 0.0048 - precision: 0.9996 - recall: 0.9984 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 1.3368e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 28/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579ms/step - accuracy: 0.9965 - auc: 1.0000 - f1_score: 0.9968 - loss: 0.0090 - precision: 0.9989 - recall: 0.9943\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 726ms/step - accuracy: 0.9965 - auc: 1.0000 - f1_score: 0.9968 - loss: 0.0090 - precision: 0.9989 - recall: 0.9943 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 1.5339e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 29/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 738ms/step - accuracy: 0.9963 - auc: 0.9999 - f1_score: 0.9959 - loss: 0.0109 - precision: 0.9975 - recall: 0.9951 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 1.3977e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.5000e-04\n",
      "Epoch 30/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 815ms/step - accuracy: 0.9964 - auc: 0.9999 - f1_score: 0.9965 - loss: 0.0096 - precision: 0.9959 - recall: 0.9969 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 1.3393e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.5000e-04\n",
      "Epoch 31/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 742ms/step - accuracy: 0.9966 - auc: 1.0000 - f1_score: 0.9960 - loss: 0.0080 - precision: 0.9943 - recall: 0.9991 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 1.4192e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.5000e-04\n",
      "Epoch 32/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 735ms/step - accuracy: 0.9983 - auc: 1.0000 - f1_score: 0.9980 - loss: 0.0066 - precision: 0.9983 - recall: 0.9984 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 8.9313e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.5000e-04\n",
      "Epoch 33/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 583ms/step - accuracy: 0.9966 - auc: 1.0000 - f1_score: 0.9968 - loss: 0.0075 - precision: 0.9989 - recall: 0.9944 - learning_rate: 2.5000e-04\n",
      "Epoch 34/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n",
      "d:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,auc,f1_score,loss,precision,recall\n",
      "  current = self.get_monitor_value(logs)\n",
      "d:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:206: UserWarning: Can save best model only with val_loss available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n",
      "d:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:151: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,auc,f1_score,loss,precision,recall,learning_rate.\n",
      "  callback.on_epoch_end(epoch, logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 765ms/step - accuracy: 0.9941 - auc: 0.9996 - f1_score: 0.9936 - loss: 0.0146 - precision: 0.9922 - recall: 0.9960 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 1.0577e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.5000e-04\n",
      "Epoch 35/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 803ms/step - accuracy: 0.9959 - auc: 1.0000 - f1_score: 0.9962 - loss: 0.0070 - precision: 0.9980 - recall: 0.9939 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 1.0586e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.5000e-04\n",
      "Epoch 36/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 711ms/step - accuracy: 0.9979 - auc: 1.0000 - f1_score: 0.9976 - loss: 0.0058 - precision: 0.9960 - recall: 0.9998 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 1.0315e-04 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.5000e-04\n",
      "Epoch 37/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 722ms/step - accuracy: 0.9975 - auc: 1.0000 - f1_score: 0.9977 - loss: 0.0066 - precision: 0.9994 - recall: 0.9957 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 9.5243e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.5000e-04\n",
      "Epoch 38/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581ms/step - accuracy: 0.9978 - auc: 0.9999 - f1_score: 0.9976 - loss: 0.0101 - precision: 0.9980 - recall: 0.9978\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 736ms/step - accuracy: 0.9978 - auc: 0.9999 - f1_score: 0.9976 - loss: 0.0101 - precision: 0.9980 - recall: 0.9978 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 8.6784e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.5000e-04\n",
      "Epoch 39/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 827ms/step - accuracy: 0.9984 - auc: 1.0000 - f1_score: 0.9981 - loss: 0.0063 - precision: 0.9980 - recall: 0.9988 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 7.7949e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 1.2500e-04\n",
      "Epoch 40/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 720ms/step - accuracy: 0.9984 - auc: 1.0000 - f1_score: 0.9984 - loss: 0.0039 - precision: 0.9986 - recall: 0.9982 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 7.8057e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 1.2500e-04\n",
      "Epoch 41/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 725ms/step - accuracy: 0.9937 - auc: 0.9991 - f1_score: 0.9952 - loss: 0.0161 - precision: 0.9985 - recall: 0.9891 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 9.4847e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 1.2500e-04\n",
      "Epoch 42/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 745ms/step - accuracy: 0.9962 - auc: 0.9999 - f1_score: 0.9951 - loss: 0.0103 - precision: 0.9960 - recall: 0.9962 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 7.9889e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 1.2500e-04\n",
      "Epoch 43/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661ms/step - accuracy: 0.9978 - auc: 1.0000 - f1_score: 0.9974 - loss: 0.0065 - precision: 0.9962 - recall: 0.9994\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 831ms/step - accuracy: 0.9978 - auc: 1.0000 - f1_score: 0.9973 - loss: 0.0065 - precision: 0.9962 - recall: 0.9994 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 6.5751e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 1.2500e-04\n",
      "Epoch 44/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 719ms/step - accuracy: 0.9975 - auc: 1.0000 - f1_score: 0.9967 - loss: 0.0075 - precision: 0.9967 - recall: 0.9983 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 6.9848e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 6.2500e-05\n",
      "Epoch 45/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 729ms/step - accuracy: 0.9980 - auc: 1.0000 - f1_score: 0.9978 - loss: 0.0051 - precision: 0.9964 - recall: 0.9997 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 7.3519e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 6.2500e-05\n",
      "Epoch 46/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 743ms/step - accuracy: 0.9970 - auc: 1.0000 - f1_score: 0.9961 - loss: 0.0055 - precision: 0.9939 - recall: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 8.1594e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 6.2500e-05\n",
      "Epoch 47/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 823ms/step - accuracy: 0.9984 - auc: 1.0000 - f1_score: 0.9984 - loss: 0.0074 - precision: 0.9988 - recall: 0.9981 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 7.2625e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 6.2500e-05\n",
      "Epoch 48/80\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582ms/step - accuracy: 0.9990 - auc: 1.0000 - f1_score: 0.9985 - loss: 0.0036 - precision: 0.9981 - recall: 1.0000\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 727ms/step - accuracy: 0.9991 - auc: 1.0000 - f1_score: 0.9986 - loss: 0.0036 - precision: 0.9981 - recall: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_f1_score: 1.0000 - val_loss: 7.0245e-05 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 6.2500e-05\n",
      "Training time: 2773.26 seconds\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (48,) and (47,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_ds_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m### Evaluation stage ###\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m optimal_threshold \u001b[38;5;241m=\u001b[39m \u001b[43mfull_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ds_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m training_results[model\u001b[38;5;241m.\u001b[39mname][dataset_id] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m: history\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_time\u001b[39m\u001b[38;5;124m'\u001b[39m: training_time,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mevaluate(test_dataset, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, steps\u001b[38;5;241m=\u001b[39mtest_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size)\n\u001b[0;32m     46\u001b[0m }\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(training_results)\n",
      "Cell \u001b[1;32mIn[11], line 161\u001b[0m, in \u001b[0;36mfull_eval\u001b[1;34m(model_ds_dir, history, model, dataset_name, test_generator)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfull_eval\u001b[39m(model_ds_dir, history, model, dataset_name, test_generator):\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Plot metrics and save the history\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m     \u001b[43mplot_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ds_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# Plot the test images with predictions\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     plot_test_images(test_generator, model_ds_dir, dataset_name, model, \u001b[38;5;241m0.5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 76\u001b[0m, in \u001b[0;36mplot_history\u001b[1;34m(run_dir, history, model_name, dataset_name)\u001b[0m\n\u001b[0;32m     74\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     75\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, acc, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mValidation Accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining and Validation Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     78\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\matplotlib\\pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\matplotlib\\axes\\_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Dissertation\\Kaggle Data\\wildfire-detection\\venv\\Lib\\site-packages\\matplotlib\\axes\\_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (48,) and (47,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "results_file = os.path.join(run_dir, 'training_results.json')\n",
    "\n",
    "for base_model, custom_bool in zip(all_models, is_custom_model):\n",
    "    model = generate_model(base_model, custom=custom_bool) # To display the model summary\n",
    "    model.summary()\n",
    "    training_results[model.name] = {}\n",
    "    plot_model(model, show_shapes=True, show_layer_names=True, to_file=os.path.join(\"architectures\", f\"{model.name}_architecture.png\"))\n",
    "    for dataset_id, train_dataset, val_dataset, steps_per_epoch, validation_steps in zip(combined_dataset_names, combined_training_datasets, combined_val_datasets, steps_per_epoch_list, validation_steps_list):\n",
    "        model.load_weights(os.path.join(checkpoint_path, f\"{model.name}_initial.weights.h5\"))\n",
    "        print(f\"Training model: {model.name} on dataset: {dataset_id}\")\n",
    "        \n",
    "        # Record the start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Initial training of the model\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=val_dataset,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks_list\n",
    "        )\n",
    "\n",
    "        # Record the end time\n",
    "        end_time = time.time()\n",
    "        # Calculate the training time\n",
    "        training_time = end_time - start_time\n",
    "        print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "        model_ds_dir = os.path.join(run_dir, model.name, dataset_id)\n",
    "        os.makedirs(model_ds_dir, exist_ok=True)\n",
    "        # Save the model\n",
    "        model.save(os.path.join(model_ds_dir, f\"{model.name}_{dataset_id}.keras\"))\n",
    "\n",
    "        ### Evaluation stage ###\n",
    "        optimal_threshold = full_eval(model_ds_dir, history, model, dataset_id, test_generator)\n",
    "        \n",
    "        training_results[model.name][dataset_id] = {\n",
    "            'history': history.history,\n",
    "            'training_time': training_time,\n",
    "            'optimal_threshold': float(optimal_threshold),\n",
    "            'train_dataset_size': steps_per_epoch * batch_size,\n",
    "            'val_dataset_size': validation_steps * batch_size,\n",
    "            \"evaluation\": model.evaluate(test_dataset, return_dict=True, steps=test_generator.samples // batch_size)\n",
    "        }\n",
    "        \n",
    "        print(training_results)\n",
    "\n",
    "        # Save the training results to a file after each iteration\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(training_results, f, indent=4)\n",
    "        \n",
    "        model.compile(optimizer=optimizer_fn, loss=loss_fn, metrics=metrics_list) # Reset the model for the next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brute force loop completed!\n",
      "All models and evaluations are available at: runs\\run_6\n"
     ]
    }
   ],
   "source": [
    "print(\"Brute force loop completed!\")\n",
    "print(f\"All models and evaluations are available at: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': [0.7258230447769165,\n",
      "              0.8127571940422058,\n",
      "              0.8443930149078369,\n",
      "              0.8751286268234253,\n",
      "              0.904321014881134],\n",
      " 'auc': [0.7971794009208679,\n",
      "         0.8877936005592346,\n",
      "         0.9179147481918335,\n",
      "         0.9452250599861145,\n",
      "         0.9639362692832947],\n",
      " 'f1_score': [0.7686777114868164,\n",
      "              0.8472543358802795,\n",
      "              0.8729486465454102,\n",
      "              0.8970165252685547,\n",
      "              0.9209895133972168],\n",
      " 'learning_rate': [0.0010000000474974513,\n",
      "                   0.0010000000474974513,\n",
      "                   0.0010000000474974513,\n",
      "                   0.0010000000474974513,\n",
      "                   0.0010000000474974513],\n",
      " 'loss': [0.5846205949783325,\n",
      "          0.41623160243034363,\n",
      "          0.3561948239803314,\n",
      "          0.289875328540802,\n",
      "          0.23601625859737396],\n",
      " 'precision': [0.7888932824134827,\n",
      "               0.8418230414390564,\n",
      "               0.8687848448753357,\n",
      "               0.8913626074790955,\n",
      "               0.916355311870575],\n",
      " 'recall': [0.7661120891571045,\n",
      "            0.8555858135223389,\n",
      "            0.880041778087616,\n",
      "            0.906879186630249,\n",
      "            0.9283010959625244],\n",
      " 'val_accuracy': [0.3956473171710968,\n",
      "                  0.5641741156578064,\n",
      "                  0.87109375,\n",
      "                  0.8911830186843872,\n",
      "                  0.9397321343421936],\n",
      " 'val_auc': [0.7820625901222229,\n",
      "             0.880294680595398,\n",
      "             0.9541445374488831,\n",
      "             0.9730848073959351,\n",
      "             0.9876116514205933],\n",
      " 'val_f1_score': [0.006938501726835966,\n",
      "                  0.43793556094169617,\n",
      "                  0.8853815197944641,\n",
      "                  0.9040260314941406,\n",
      "                  0.9485512971878052],\n",
      " 'val_loss': [2.113976001739502,\n",
      "              0.9911763072013855,\n",
      "              0.29621121287345886,\n",
      "              0.23324449360370636,\n",
      "              0.16732968389987946],\n",
      " 'val_precision': [1.0,\n",
      "                   0.9909090995788574,\n",
      "                   0.9277108311653137,\n",
      "                   0.936274528503418,\n",
      "                   0.9716714024543762],\n",
      " 'val_recall': [0.0036798526998609304,\n",
      "                0.29592761397361755,\n",
      "                0.8531855940818787,\n",
      "                0.8801843523979187,\n",
      "                0.9295393228530884]}\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - accuracy: 0.7505 - auc: 0.6056 - f1_score: 0.0026 - loss: 0.9262 - precision: 1.0000 - recall: 0.0046\n",
      "{'accuracy': 0.4170673191547394, 'auc': 0.6012849807739258, 'f1_score': 0.004662004299461842, 'loss': 2.0492188930511475, 'precision': 1.0, 'recall': 0.004219409078359604}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(history.history)\n",
    "print(model.evaluate(test_dataset, return_dict=True, steps=test_generator.samples // batch_size))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1351460,
     "sourceId": 2247205,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
